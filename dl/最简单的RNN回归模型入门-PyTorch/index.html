<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  <meta name="msvalidate.01" content="E0719258D9996F545A834108E5C74A7F" />
  <meta name="baidu-site-verification" content="codeva-mge5JyeFyt" />
  <meta name="google-site-verification" content="T311rYZ3FLiMQxxjBMtkGeur0fjf0m9dLn4x7jS5HUY" />
  
  <title>最简单的RNN回归模型入门(PyTorch) - 阿振的个人主页</title>
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="最简单的RNN回归模型入门(PyTorch) - 阿振的个人主页" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://theonegis.github.io/dl/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84RNN%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8-PyTorch/index.html" />
  
  <meta property="og:image" content="/favicon.png" />
  
  <meta property="og:article:published_time" content="2019-03-02T04:46:15.000Z" />
  
  <meta property="og:article:author" content="阿振" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  

  
  <meta name="msvalidate.01" content="E0719258D9996F545A834108E5C74A7F" />
  <meta name="baidu-site-verification" content="codeva-mge5JyeFyt" />
  <meta name="google-site-verification" content="T311rYZ3FLiMQxxjBMtkGeur0fjf0m9dLn4x7jS5HUY" />
<meta name="generator" content="Hexo 7.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-rainbow-banner="true"
        data-rainbow-banner-shown="auto"
        data-rainbow-banner-month="6"
        data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="2"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">TheOneGIS</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/">Home</a>
            
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/categories">Categories</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-bilibili nav-item-icon" href="https://space.bilibili.com/1078671754/" target="_blank" aria-label="Bilibili">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/theonegis" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        
        <div class="categories text-uppercase">
        
            <a href="/categories/dl/">深度学习</a>
        
        </div>
        

        
        <div class="date" id="date">
            <span>March</span>
            <span>2,</span>
            <span>2019</span>
        </div>
        

        <h2 class="title">最简单的RNN回归模型入门(PyTorch)</h2>
    </div>

    <div class="divider"></div>

    <div class="content">
        <p>版权声明：本文为博主原创文章，转载请注明原文出处！<br>写作时间：2019-03-02 12:46:15</p>
<p>本文部分图片素材来自互联网，如有侵权，请联系作者删除！</p>
<h1 id="最简单的RNN回归模型入门（PyTorch版）"><a href="#最简单的RNN回归模型入门（PyTorch版）" class="headerlink" title="最简单的RNN回归模型入门（PyTorch版）"></a>最简单的RNN回归模型入门（PyTorch版）</h1><h2 id="RNN入门介绍"><a href="#RNN入门介绍" class="headerlink" title="RNN入门介绍"></a>RNN入门介绍</h2><p>至于RNN的能做什么，擅长什么，这里不赘述。如果不清楚，请先维基一下，那里比我说得更加清楚。</p>
<p>我们首先来来看一张经典的RNN模型示意图！</p>
<p><img src="/images/ml/Recurrent-Neural-Network.png" alt="Recurrent Neural Network"></p>
<p>图分左右两边：左边给出的RNN是一个抽象的循环结构，右边是左边RNN展开以后的形式。先来看右边的结构，从下往上依次是序列数据的输入X（图中的绿色结构，可以是时间序列，也可以是文本序列等等）。对于t时刻的x经过一个线性变换（U是变换的权重），然后与t-1时刻经过线性变换V的h相加，再经过一个 非线性激活（一般使用tanh或者relu函数）以后，形成一个t时刻的中间状态h，然后再经过一个线性变换（W）输出o ，最后再经过一个非线性激活（可以是sigmoid函数或者softmax等函数）形成最后的输出y。</p>
<p>上面的文字描述，可以形式化表示为下面的公式：</p>
<p>$$a^t &#x3D; Vh^{t-1} + Ux^t + b \ h^t&#x3D;tanh(a^t) \ o^t&#x3D;Wh^t + c\ y^t&#x3D;sigmoid(o^t)$$</p>
<p>是不是公式能比文字更加说明问题！</p>
<p>再来说左边的结构，坐标的结构表明后面地展开网络中的U，V，W参数都是在共享的，就是说不管我们的序列有多长，都是共享这一套参数的。这是RNN很重要的一个特性。</p>
<p>RNN的隐藏层可以有多层，但是RNN中我们的隐藏层一般不会设置太多，因为在横向上有很长的序列扩展形成的网络，这部分特征是我们更加关注的。最后，需要说明的是RNN可以是单向的，也可以是双向的。</p>
<h2 id="PyTorch中的RNN"><a href="#PyTorch中的RNN" class="headerlink" title="PyTorch中的RNN"></a>PyTorch中的RNN</h2><p>下面我们以一个最简单的回归问题使用正弦sin函数预测余弦cos函数，介绍如何使用PyTorch实现RNN模型。</p>
<p>先来看一下PyTorch中<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#rnn">RNN</a>类的原型：</p>
<p><img src="/images/ml/RNNClass.png" alt="torch.nn.RNN"></p>
<ul>
<li>必选参数<code>input_size</code>指定输入序列中单个样本的大小尺寸，比如在NLP中我们可能用用一个10000个长度的向量表示一个单词，则这个<code>input_size</code>就是10000。在咱们的回归案例中，一个序列中包含若干点，而每个点的所代表的函数值（Y）作为一个样本，则咱们案例中的<code>input_size</code>为1。这个参数需要根据自己的实际问题确定。</li>
<li>必选参数<code>hidden_size</code>指的是隐藏层中输出特征的大小，这个是自定义的超参数。</li>
<li>必选参数<code>num_layers</code>指的是纵向的隐藏层的个数，根据实际问题我们一般可以选择1~10层。</li>
<li>可选参数<code>batch_first</code>指定是否将<code>batch_size</code>作为输入输出张量的第一个维度，如果是，则输入的尺寸为（<code>batch_size</code>， <code>seq_length</code>，<code>input_size</code>），否则，默认的顺序是（<code>seq_length</code>，<code>batch_size</code>， <code>input_size</code>）。</li>
<li>可选参数<code>bidirectional</code>指定是否使用双向RNN。</li>
</ul>
<p>下面再来说说RNN输入输出尺寸的问题，了解了这个可以让我们我们调试代码的时候更加清晰。下面是PyTorch官方的说明：</p>
<p><img src="/images/ml/RNNInOut.png" alt="RNN的输入输出"></p>
<p>对于RNN的输入包括输入序列和一个初始化的隐藏状态$h_0$。输入序列尺寸默认是（<code>sequence_length</code>，<code>batch_size</code>， <code>input_size</code>），所以如果我们的数据形式不是这样的，则需要手动调整为这种类型的格式。</p>
<p>隐藏状态$h_i$的尺寸是（<code>num_layers * num_directions</code>， <code>batch_size</code>，<code> hidden_size</code>）。单向RNN的<code>num_directions</code>为1，双向RNN的<code>num_directions</code>为2。</p>
<p>他们的尺寸为什么是这样的呢？这得根据本文开头的那个公式计算，即就是矩阵的相乘需要满足矩阵尺寸的关系，聪明的你想明白了吗？</p>
<p>输出的尺寸为 （<code>sequence_length</code>， <code>batch_size</code>， <code>num_directions * hidden_size</code>）</p>
<p>每一次RNN运行结果输出中还会附带输出中间隐藏状态$h_i$，当然这个尺寸和初始的隐藏状态相同。</p>
<p>下面以一个简单的例子说明怎么在程序中查看他们的尺寸：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">rnn = nn.RNN(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">inputs = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)  <span class="comment"># (time_step, batch_size, input_size)</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)  <span class="comment"># (num_layers, batch_size, hidden_size)</span></span><br><span class="line">output, hn = rnn(inputs, h0)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># (time_step, batch_size, hidden_size)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> rnn.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">        <span class="built_in">print</span>(name, param.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其输出结果如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.Size</span>(<span class="selector-attr">[5, 3, 20]</span>)</span><br><span class="line">weight_ih_l0 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20, 10]</span>)</span><br><span class="line">weight_hh_l0 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20, 20]</span>)</span><br><span class="line">bias_ih_l0 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20]</span>)</span><br><span class="line">bias_hh_l0 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20]</span>)</span><br><span class="line">weight_ih_l1 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20, 20]</span>)</span><br><span class="line">weight_hh_l1 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20, 20]</span>)</span><br><span class="line">bias_ih_l1 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20]</span>)</span><br><span class="line">bias_hh_l1 torch<span class="selector-class">.Size</span>(<span class="selector-attr">[20]</span>)</span><br></pre></td></tr></table></figure>

<p>这里的<code>weight_ih_l0</code>表示的是RNN隐藏层第一层的权重U，<code>weight_hh_l0</code>表示的隐藏层第一层的权重V，类似的<code>bias</code>开头的表示偏置或者叫增益（我不知道中文如何翻译），以<code>l数字</code>结尾的表示第几层的权重或者偏置。</p>
<h2 id="代码实现与结果分析"><a href="#代码实现与结果分析" class="headerlink" title="代码实现与结果分析"></a>代码实现与结果分析</h2><p>好了，搞清楚了RNN的基本原理以及PyTorch中RNN类的输入输出参数要求，我们下面实现我们的回归案例。</p>
<p>比较重要的几个超参数是：<code>TIME_STEP</code>指定输入序列的长度（一个序列中包含的函数值的个数），<code>INPUT_SIZE</code>是1，表示一个序列中的每个样本包含一个函数值。</p>
<p>我们自定义的RNN类包含两个模型：一个<code>nn.RNN</code>层，一个<code>nn.Linear</code>层，注意<code>forward</code>函数的实现，观察每个变量的尺寸（注释中给出了答案）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2019</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参设置</span></span><br><span class="line">TIME_STEP = <span class="number">10</span>  <span class="comment"># RNN时间步长</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span>  <span class="comment"># RNN输入尺寸</span></span><br><span class="line">INIT_LR = <span class="number">0.02</span>  <span class="comment"># 初始学习率</span></span><br><span class="line">N_EPOCHS = <span class="number">100</span>  <span class="comment"># 训练回数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(</span><br><span class="line">            input_size=INPUT_SIZE,</span><br><span class="line">            hidden_size=<span class="number">32</span>,  <span class="comment"># RNN隐藏神经元个数</span></span><br><span class="line">            num_layers=<span class="number">1</span>,  <span class="comment"># RNN隐藏层个数</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, h</span>):</span><br><span class="line">        <span class="comment"># x (time_step, batch_size, input_size)</span></span><br><span class="line">        <span class="comment"># h (n_layers, batch, hidden_size)</span></span><br><span class="line">        <span class="comment"># out (time_step, batch_size, hidden_size)</span></span><br><span class="line">        out, h = <span class="variable language_">self</span>.rnn(x, h)</span><br><span class="line">        prediction = <span class="variable language_">self</span>.out(out)</span><br><span class="line">        <span class="keyword">return</span> prediction, h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line"><span class="built_in">print</span>(rnn)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=INIT_LR)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line">h_state = <span class="literal">None</span>  <span class="comment"># 初始化隐藏层</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.ion()</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(N_EPOCHS):</span><br><span class="line">    start, end = step * np.pi, (step + <span class="number">1</span>) * np.pi  <span class="comment"># 时间跨度</span></span><br><span class="line">    <span class="comment"># 使用Sin函数预测Cos函数</span></span><br><span class="line">    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=<span class="literal">False</span>)</span><br><span class="line">    x_np = np.sin(steps)</span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line">    x = torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  <span class="comment"># 尺寸大小为(time_step, batch, input_size)</span></span><br><span class="line">    y = torch.from_numpy(y_np[:, np.newaxis, np.newaxis])</span><br><span class="line"></span><br><span class="line">    prediction, h_state = rnn(x, h_state)  <span class="comment"># RNN输出（预测结果，隐藏状态）</span></span><br><span class="line">    h_state = h_state.detach()  <span class="comment"># 这一行很重要，将每一次输出的中间状态传递下去(不带梯度)</span></span><br><span class="line">    loss = loss_func(prediction, y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制中间结果</span></span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.plot(steps, y_np, <span class="string">&#x27;r-&#x27;</span>)</span><br><span class="line">    plt.plot(steps, prediction.data.numpy().flatten(), <span class="string">&#x27;b-&#x27;</span>)</span><br><span class="line">    plt.draw()</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>最后的结果如下：</p>
<p><img src="/images/ml/RNNSinCos.gif" alt="RNN使用Sin预测Cos"></p>
<p>最后放一个当<code>TIME_STEP</code>分别等于10和20的最终预测结果的对比图：</p>
<p><img src="/images/ml/Time-Step-10.png" alt="RNN TIME_STEP等于10"></p>
<p><img src="/images/ml/Time-Step-20.png" alt="RNN TIME_STEP=20"></p>
<p>第一张是<code>TIME_STEP</code>&#x3D;10的预测结果，第二张是<code>TIME_STEP</code>&#x3D;20的预测结果。为什么当<code>TIME_STEP</code>&#x3D;20的预测结果差得十万八千里呢？</p>
<p>这是因为经典的RNN存在梯度爆炸和梯度弥散问题（我尝试修剪了梯度可是结果还是很差，不知道是不是其它原因），对长时序的预测表现很不好，所以才有了后来的LSTM和GRU等RNN变种。实际现在已经很少使用经典RNN了。有时间在说说LSTM吧，欢迎关注！</p>

    </div>

    <!-- 
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by 阿振, licensed under <a href="undefined">undefined</a>.</p>
        </div>
        
        <p class="tags">
            
            <i class="icon"></i>
            <a href="/tags/PyTorch/" class="tag">#PyTorch</a><a href="/tags/RNN/" class="tag">#RNN</a>
        </p>
        
    </div>
     -->

    <div class="container post-prev-next">
        
        <a href="/dl/%E9%80%9A%E4%BF%97LSTM%E9%95%BF%E7%9F%AD%E6%97%B6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/" class="next">
            <div>
                <div class="text">
                    <p class="label">Next</p>
                    <h3 class="title">通俗LSTM长短时循环神经网络介绍</h3>
                </div>
            </div>
        </a>
        
        
        <a href="/algorithm/LeetCode-Longest-Palindromic-Subsequence/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">LeetCode-Longest Palindromic Subsequence</>
                </div>
            </div>
        </a>
        
    </div>

    
        
        
    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
        </div>
        <div><h3>这个世界并不在乎你的自尊，只希望你在自我感觉良好之前有所作为！</h3></div>
        <span>&copy; 2024 阿振<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>