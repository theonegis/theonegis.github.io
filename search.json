[{"title":"关于C++函数返回值的拷贝优化问题","url":"/cxx/关于C-函数返回值的拷贝优化问题/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：{{ date }}\n\n---\n\n\n在传统C++程序中，如果函数的返回值是一个对象的话，可能需要对函数中的局部对象进行拷贝。如果该对象很大的话，则程序的效率会降低。\n\n在C++ 11以后，出现的移动语义（Move Semantic）及拷贝优化（Copy Elision）都是解决这个问题的方法。\n\n本文试图以一个最简单的例子来说明这个问题。\n\n# 案例\n\n下面来看一个简单的例子（这里的BigObj类的实例假设是一个需要很大存储空间的大对象）：\n\n```cpp\n#include <iostream>\n\nusing std::cout;\nusing std::endl;\n\n\nclass BigObj\n{\npublic:\nBigObj()\n{\n    cout << \"这是默认构造函数\" << endl;\n}\n\nBigObj(const BigObj& that)\n{\n    cout << \"这是拷贝构造函数\" << endl;\n}\n\nBigObj(BigObj&& that)\n{\n    cout << \"这是移动构造函数\" << endl;\n}\n\n~BigObj()\n{\n    cout << \"这是析构函数\" << endl;\n}\n};\n\n\nBigObj fun()\n{\n    BigObj obj = BigObj();\n    return obj;\n}\n\nint main()\n{\n    BigObj obj = fun();\n    return EXIT_SUCCESS;\n}\n```\n\n# 拷贝优化\n\n运行该程序，我们会得到如下输出：\n\n```plain\n这是默认构造函数\n这是析构函数\n```\n\n可以发现fun()函数在返回BigObj对象的时候没有进行拷贝，这是由于编译期帮我们做了拷贝优化。\n\n# 移动语义\n\n但是编译器堆函数返回值的拷贝优化并不是能完全实现的，有一些特殊情况下会失效。所以比较保险的做法是定义移动构造函数，当没有拷贝优化的时候可以通过移动语义避免低效的拷贝。\n\n我们可以通过-fno-elide-constructors关闭编译器的拷贝优化，下面是对应的cmake文件：\n\n```cmake\ncmake_minimum_required(VERSION 3.26)\nproject(CxxTutorial)\n\nset(CMAKE_CXX_STANDARD 23)\n#SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-elide-constructors\")\n\nadd_executable(CxxTutorial main.cpp)\n```\n\n通过配置关闭拷贝优化以后，我们执行上面的程序，输出结果如下：\n\n```plain\n这是默认构造函数\n这是移动构造函数\n这是析构函数\n这是析构函数\n```\n\n可以看到关闭拷贝优化以后，在定义了移动构造函数的时候，函数返回零时对象的时候会调用移动构造函数，转义所有权，减少数据拷贝。但是移动构造也会生成一个新的对象，所以输出结果中会调用两次析构函数，第一次析构函数是析构了函数中定义的零时对象，第二次是析构了函数返回值返回后的对象。\n\n那如果我们没有定义移动构造函数，而且编译期也没有进行拷贝优化程序的运行会是怎么样的呢？\n\n注释掉上面的移动构造函数，我们可以看到输出结果如下：\n\n```plain\n这是默认构造函数\n这是拷贝构造函数\n这是析构函数\n这是析构函数\n```\n\n这个结果是在预料之中的，没有拷贝优化，没有移动构造函数的情况下，程序会调用拷贝构造函数。假设这个对象是一个大对象，则拷贝过程会花费一些时间，降低了程序的执行效率。而使用移动语义的话，直接转义对象的所有权，效率会高一些。\n\n# 结论\n\n对于C++函数返回一个大对象的时候，在编译器能进行拷贝优化的时候，会优先进行返回值的拷贝优化。如果不能进行拷贝优化，在有定义移动构造函数的时候，则会调用移动构造函数进行返回值对象所有权转义，减少不必要的拷贝。最后，这两种情况失效的时候，才会调用拷贝构造函数进行对象的深拷贝。\n\n有了上述结论，我们在写程序的时候最佳实践是函数返回值可以直接返回函数体内定义的零时对象，但是我们需要在定义该对象的时候实现移动构造函数。这样就可以保证函数的返回值要么有编译器拷贝优化，要么会调用移动构造函数减少拷贝开销。\n","tags":["C++","移动语义","函数返回值","拷贝优化"],"categories":["C++"]},{"title":"R包安装过程中开启C++11支持","url":"/tools/R包安装过程中开启C-11支持/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：{{ 2021年12月8日晚 }}\n\n---\n\n# R包安装过程中开启C++11支持\n\n自打工作以后，就很少有闲余时间写写博客，分享自己学习的点点滴滴了。也许这就是上学和工作的区别吧。\n\n## 问题描述\n\n打算利用R读取ASD光谱文件，需要安装`prospectr`包，使用常规命令`install.packages(\"prospectr\")`死活安装不上，提示需要编译器支持C++11。\n\n报错信息如下：\n\"*** C++11 compiler required; enable C++11 mode in your compiler, or use an earlier version of Armadillo\"\n\n可是我的编译器是支持C++11的呀，这就是如何开启支持的问题了，经过千方百计搜索，得到了如下圆满的解决方案。\n\n## 问题解决\n\n使用R的`withr`包设置编译环境：\n\n```\nlibrary(withr)\nwith_makevars(c(PKG_CFLAGS = \"-std=c++11\"), install.packages(\"prospectr\"), assignment = \"+=\")\n```\n\n其中，`PKG_CFLAGS`指示了编译器的附加参数。","tags":["C++","R"],"categories":["工具"]},{"title":"QGIS中WKT转为可视化图层","url":"/geos/QGIS中WKT转为可视化图层/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2021-01-21 19:00:05\n\n---\n\n# QGIS中WKT转为可视化涂层\n\n## 常见的几种用于互操作的空间数据标准格式\n\n - WKT（Well-Known Text）是开放地理空间联盟OGC（[Open Geospatial Consortium](https://www.ogc.org/) ）制定的一种文本标记语言，用于表示矢量几何对象及其空间参照系统等。\n- WKB（Well-Known Binary） 是OGC制订的WKT的二进制表示形式，解决了WKT表达方式冗余的问题，便于传输和在数据库中存储信息。\n- GeoJSON是JSON格式的空间要素信息输出格式，它便于被JavaScript等脚本语言处理，OpenLayers等GIS库是采用GeoJSON格式进行数据互操作的。\n\n这三种格式是在我们进行GIS系统开发和设计时经常会遇到的数据交换格式。\n\n## 使用QGIS的Python接口将WKT转为可视化涂层\n\n我们现在有一个WKT格式的要素，我想看一下这个要素到底是什么形状，有没有什么方便的方法呢？\n\n在QGIS的菜单栏中选择`Plugin`->`Python Console`打开Python控制台面板，输入Python代码。\n\n下面的代码很简单，将WKT转为Geometry，通过Geometry生成Feature，然后将Feature添加到图层Layer中。\n\n```Python\n\nwkt = 'POLYGON((99.60 37.24, 100.77 37.24, 100.77 36.54, 99.60 36.54, 99.60 37.2)'\n# 定义一个矢量图层，第一个参数是URL表示的参数路径，例如：Point?crs=epsg:4326&field=id:integer&field=name:string(20)&index=yes\n# 第二个参数是图层名称，第三个是数据源，\"memory\"表示来自内存，即在程序中动态生成的数据\ntemp = QgsVectorLayer('Polygon?crs=epsg:4326', 'result', 'memory')\n# 给QGIS工程的实例添加该图层\nQgsProject.instance().addMapLayer(temp)\n# 开始编辑图层添加数据，先新建一个Feature，设置Feature的Geometry，然后给图层添加该Feature\ntemp.startEditing()\ngeom = QgsGeometry().fromWkt(wkt)\nfeature = QgsFeature()\nfeature.setGeometry(geom)\ntemp.dataProvider().addFeatures([feature])\n# 提交修改\ntemp.commitChanges()\n```\n\n结果如下：\n\n![截屏2021-01-21 下午7.11.07](/images/QGIS/截屏2021-01-21下午7.11.07.png)","tags":["Python","QGIS"],"categories":["空间数据处理"]},{"title":"Dijkstra算法及其C++实现","url":"/algorithm/Dijkstra算法及其C-实现/","content":"\n# Dijkstra算法及其C++实现\n\n## 什么是最短路径问题\n\n如果从图中某一顶点（称为源点）到达另一顶点（称为终点）的路径可能不止一条，如何找到一条路径使得沿此路径上各边上的权值总和达到最小。\n\n单源最短路径问题是指对于给定的图$G=(V, E)$，求源点$v_0$到其它顶点$v_t$的最短路径。\n\n## Dijkstra算法\n\nDijkstra算法用于计算一个节点到其他节点的最短路径。Dijkstra是一种按路径长度递增的顺序逐步产生最短路径的方法，是一种贪婪算法。\n\nDijkstra算法的核心思想是首先求出长度最短的一条最短路径，再参照它求出长度次短的一条最短路径，依次类推，直到从源点$v_0$到其它各顶点的最短路径全部求出为止。\n\n具体来说：图中所有顶点分成两组，第一组是已确定最短路径的顶点，初始只包含一个源点，记为集合$S$；第二组是尚未确定最短路径的顶点，记为集合$U$。\n\n按最短路径长度递增的顺序逐个把$U$中的顶点加到$S$中去，同时动态更新$U$集合中源点到各个顶点的最短距离，直至所有顶点都包括到$S$中。\n\n## 实现思路\n\n1. 初始时，$S$集合只包含起点$v_0$；$U$集合包含除$v_0$外的其他顶点$v_t$，且$U$中顶点的距离为起点$v_0$到该顶点的距离。（$U$中顶点$v_t$的距离为$(v_0, v_t)$的长度，如果$v_0$和$v_t$不相邻，则$v_t$的最短距离为$\\infty$）\n2. 从$U$中选出距离最短的顶点$v_{t'}$，并将顶点$v_{t'}$加入到$S$中；同时，从$U$中移除顶点$v_{t'}$。\n3. 更新$U$中各个顶点$v_t$到起点$v_0$的距离以及最短路径中当前顶点的前驱顶点。之所以更新$U$中顶点的距离以及前驱顶点是由于上一步中确定了$v_{t'}$是求出最短路径的顶点，从而可以利用$v_{t'}$来更新$U$中其它顶点$v_t$的距离，因为存在$(v_0, v_t)$的距离可能大于$(v_0, v_{t'}) + (v_{t'}, v_t)$距离的情况，从而也需要更新其前驱顶点\n4. 重复步骤(2)和(3)，直到遍历完所有顶点\n\n## 案例分析\n\n![Dijkstra](/images/algorithm/Dijkstra.png)\n\n## 代码实现\n\n使用了部分C++11特性，注释丰富，读起来应该不会太困难！\n\n```C++\n#include <cstdio>\n#include <iostream>\n#include <vector>\n#include <list>\n#include <stack>\n\nusing namespace std;\nusing Matrix = vector<vector<uint>>;                // 连接矩阵（使用嵌套的vector表示）\nusing SNodes = vector<tuple<uint, uint, uint>>;     // 已计算出最短路径的顶点集合S（类似一个动态数组）\nusing UNodes = list<tuple<uint, uint, uint>>;       // 未进行遍历的顶点集合U（使用list主要是方便元素删除操作）\nusing ENode = tuple<uint, uint, uint>;              // 每个节点包含（顶点编号，当前顶点到起始点最短距离，最短路径中当前顶点的上一个顶点）信息\n\n\n/***\n * 从未遍历的U顶点集合中找到下一个离起始顶点距离最短的顶点\n * @param unvisitedNodes 未遍历的U顶点集合\n * 每个元素是（顶点编号，当前顶点到起始点最短距离，最短路径中当前顶点的上一个顶点）的tuple\n * @return 下一个离起始顶点距离最短的顶点\n */\nENode searchNearest(const UNodes &unvisitedNodes) {\n    uint minDistance = UINT_MAX;\n    ENode nearest;\n    for (const auto &node: unvisitedNodes) {\n        if (get<1>(node) <= minDistance) {\n            minDistance = get<1>(node);\n            nearest = node;\n        }\n    }\n    return nearest;\n}\n\n\n/***\n * 迪克斯特拉算法的实现\n * @param graph 连接矩阵（使用嵌套的vector表示）\n * @param startNodeIndex 起始点编码（从0开始）\n * @return 返回一个vector，每个元素是到起始顶点的距离排列的包含（顶点编号，当前顶点到起始点最短距离，最短路径中当前顶点的上一个顶点）的tuple\n */\nSNodes dijkstra(const Matrix &graph, uint startNodeIndex) {\n    const uint numOfNodes = graph.size();   // 图中顶点的个数\n    // S是已计算出最短路径的顶点的集合（顶点编号，当前顶点到起始点最短距离，最短路径中当前顶点的上一个顶点）\n    SNodes visitedNodes;\n    // U是未计算出最短路径的顶点的集合（其中的key为顶点编号，value为到起始顶点最短距离和最短路径中上一个节点编号组成的pair）\n    UNodes unvisitedNodes;\n\n    // 对S和U集合进行初始化，起始顶点的距离为0，其他顶点的距离为无穷大\n    // 最短路径中当前顶点的上一个顶点初始化为起始顶点，后面会逐步进行修正\n    for (auto i = 0; i < numOfNodes; ++i) {\n        if (i == startNodeIndex) visitedNodes.emplace_back(i, 0, startNodeIndex);\n        else unvisitedNodes.emplace_back(i, graph[startNodeIndex][i], startNodeIndex);\n    }\n\n    while (!unvisitedNodes.empty()) {\n        // 从U中找到距离起始顶点距离最短的顶点，加入S，同时从U中删除\n        auto nextNode = searchNearest(unvisitedNodes);\n        unvisitedNodes.erase(find(unvisitedNodes.begin(), unvisitedNodes.end(), nextNode));\n        visitedNodes.emplace_back(nextNode);\n        // 更新U集合中各个顶点的最短距离以及最短路径中的上一个顶点\n        for (auto &node: unvisitedNodes) {\n            // 更新的判断依据就是起始顶点到当前顶点（nextNode）距离加上当前顶点到U集合中顶点的距离小于原来起始顶点到U集合中顶点的距离\n            // 更新最短距离的时候同时需要更新最短路径中的上一个顶点为nextNode\n            if (graph[get<0>(nextNode)][get<0>(node)] != UINT_MAX &&\n                graph[get<0>(nextNode)][get<0>(node)] + get<1>(nextNode) < get<1>(node)) {\n                get<1>(node) = graph[get<0>(nextNode)][get<0>(node)] + get<1>(nextNode);\n                get<2>(node) = get<0>(nextNode);\n            }\n        }\n    }\n\n    return visitedNodes;\n}\n\n\n/***\n * 对使用迪克斯特拉算法求解的最短路径进行打印输出\n * @param paths vector表示的最短路径集合\n * 每个元素是到起始顶点的距离排列的包含（顶点编号，当前顶点到起始点最短距离，最短路径中当前顶点的上一个顶点）的tuple\n */\nvoid print(const SNodes &paths) {\n    stack<int> tracks;  //从尾部出发，使用stack将每个顶点的最短路径中的前一个顶点入栈，然后出栈的顺序就是最短路径顺序\n    // 第一个元素是起始点，从第二个元素进行打印输出\n    for (auto it = ++paths.begin(); it != paths.end(); ++it) {\n        // 打印头部信息\n        printf(\"%c -> %c:\\t Length: %d\\t Paths: %c\",\n               char(get<0>(paths[0]) + 65),\n               char(get<0>(*it) + 65),\n               get<1>(*it),\n               char(get<0>(paths[0]) + 65));\n        auto pointer = *it;\n        // 如果当前指针pointer指向的节点有中途节点（判断的条件是最短路径中的前一个节点不是起始点）\n        while (get<2>(pointer) != get<0>(paths[0])) {\n            tracks.push(get<0>(pointer));\n            // Lambda表达式，使用find_if函数把当前顶点的前一个顶点从paths中找出来继续进行循环直到前一个节点就是起始点\n            auto condition = [pointer](tuple<uint, uint, uint> x) { return get<0>(x) == get<2>(pointer); };\n            pointer = *find_if(paths.begin(), paths.end(), condition);\n        }\n        tracks.push(get<0>(pointer));\n\n        // 以出栈的顺序进行打印输出\n        while (!tracks.empty()) {\n            printf(\" -> %c\", char(tracks.top() + 65));\n            tracks.pop();\n        }\n        printf(\"\\n\");\n    }\n}\n\nint main() {\n    Matrix graph = {\n            {0,        12,       UINT_MAX, UINT_MAX, UINT_MAX, 16, 14},\n            {12,       0,        10,       UINT_MAX, UINT_MAX, 7, UINT_MAX},\n            {UINT_MAX, 10,       0, 3,               5,        6, UINT_MAX},\n            {UINT_MAX, UINT_MAX, 3, 0,               4, UINT_MAX, UINT_MAX},\n            {UINT_MAX, UINT_MAX, 5, 4,               0,        2,  8},\n            {16,       7,        6,        UINT_MAX, 2,        9,  9},\n            {14,       UINT_MAX, UINT_MAX, UINT_MAX, 8,        9,  0}\n    };  // 图对应的连接矩阵\n    auto results = dijkstra(graph, uint('D' - 65));          // 选取顶点C（大写字母A的ASCII编码是65）\n    print(results);     // 打印输出结果\n    return 0;\n}\n```\n\n运行结果：\n\n```\nD -> C:\t Length: 3\t Paths: D -> C\nD -> E:\t Length: 4\t Paths: D -> E\nD -> F:\t Length: 6\t Paths: D -> E -> F\nD -> G:\t Length: 12\t Paths: D -> E -> G\nD -> B:\t Length: 13\t Paths: D -> C -> B\nD -> A:\t Length: 22\t Paths: D -> E -> F -> A\n```\n\n\n\n\n\n## \n\n\n\n","tags":["最短路径","Dijkstra","迪克斯特拉"],"categories":["算法"]},{"title":"GIS空间分析之Clip","url":"/geos/GIS空间分析之Clip/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-16 周一 早安\n\n---\n\n# 开篇\n\n接着上篇《[GIS空间分析之Buffer](https://blog.csdn.net/theonegis/article/details/106753745)》之后，这篇来介绍矢量数据的裁剪（Clip）。裁剪其实算不上是一种空间分析功能，就是一种简单矢量数据处理操作。\n\n一般，我们会使用一个代表研究区域或者感兴趣的区域（AOI，Area of Interest）的多边形数据去裁剪一个比较大的全区域的数据，得到的结果就是感兴趣区域中包含的数据。矢量数据的裁剪在平时工作中用得不是特别多。\n\n# 案例介绍\n\n本文使用一个AOI区域（黄河流域区）区裁剪中国的县界（线要素），从而得到黄河流域区中的县界。\n\n在上篇《[GIS空间分析之Buffer](https://blog.csdn.net/theonegis/article/details/106753745)》中我们是使用了Geometry类的`Buffer()`方法建立缓冲区的，而本篇中我们需要使用Layer图层类的`Clip()`方法进行矢量数据裁切。\n\n对于矢量数据的操作，有的是通过Geometry类的方法实现的，有的是通过Layer类的方法实现的。\n\n下面来看一下我们的数据，红色区块是黄河流域，红色的线表示的是各个县的边界。\n\n![](/images/geos/屏幕快照2020-06-16上午8.34.13.png)\n\n\n\n# 代码展示\n\n```Python\nfrom pathlib import Path\nimport ogr\n\nogr.UseExceptions()\n\n# 读取被裁剪的数据\nin_ds: ogr.DataSource = ogr.Open('../data/County.shp')\nin_lyr: ogr.Layer = in_ds.GetLayer()\n\n# 读取裁剪范围数据\nmethod_ds: ogr.DataSource = ogr.Open('../data/YellowRiver.shp')\nmethod_lyr: ogr.Layer = method_ds.GetLayer()\n\nfname: str = 'Clipped.shp'\n# 创建被裁剪以后的输出文件\ndriver: ogr.Driver = ogr.GetDriverByName('ESRI Shapefile')\nif Path(fname).exists():\n    driver.DeleteDataSource(fname)\n# 新建DataSource，Layer\nout_ds: ogr.DataSource = driver.CreateDataSource(fname)\nout_lyr: ogr.Layer = out_ds.CreateLayer(fname,\n                                        in_lyr.GetSpatialRef(),\n                                        in_lyr.GetGeomType())\n# 开始进行裁剪\nin_lyr.Clip(method_lyr, out_lyr)\nout_ds.FlushCache()\ndel in_ds, method_ds, out_ds\n\n```\n\n裁剪的结果如下：\n\n![](/images/geos/屏幕快照2020-06-16上午8.34.55.png)\n\n# 方法总结\n\n1. 首先我们使用`ogr.Open()`函数分别读取被裁剪的数据以及裁剪范围数据；使用`GetLayer()`方法获取数据的图层。裁剪范围数据在GDAL中被称为Method Layer。\n2. 使用Driver类的`CreateDataSource()`方法创建裁剪输出结果；使用`CreateLayer()`函数创建输出图层。`CreateLayer()`函数传入三个参数，分别是图层名，空间参考以及空间要素类型。后两个参数可以通过读取被裁剪数据获取。\n3. 使用读取被裁剪数据图层的`Clip()`函数进行裁剪，改函数传入两个参数，分别是方法图层（Method Layer）以及输出结果图层（Result Layer）。","tags":["GDAL","GIS","空间分析","裁剪"],"categories":["空间数据处理"]},{"title":"GIS空间分析之Buffer","url":"/geos/GIS空间分析之Buffer/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-14 周末 夏夜\n\n---\n\n# 开篇\n\nGIS空间分析是通过对GIS系统中的空间地物的空间位置以及分布形态等空间特性进行分析推理等得到额外有用信息的过程。GIS空间分析包含广泛的内容，是GIS系统的核心功能。\n\n从这篇博文开始，我们会简单介绍几种GIS系统中最常见最简单的空间分析功能，并使用Python的GDAL API进行实现。\n\n首先，我们来看一下开源GIS软件QGIS中提供的几种简单的空间分析工具，我们接下来的几篇博文会介绍如何使用Python脚本实现这些功能。\n\n![QGIS空间分析功能](/images/geos/屏幕快照2020-06-14下午10.07.57.png)\n\n虽然，这些都是最基础的空间分析操作，但是很多复杂的分析功能就是这些简单操作的组合，所以掌握这些基础操作很重要。\n\n# 案例介绍\n\n本文我们将使用缓冲区分析工具Buffer制作中国地图的晕线。中国地图边界的晕线就是我们经常看到的国界外面的突出色浅色部分。\n\n那么我们如何制作晕线呢？一个解决方案是我们给国界做一个缓冲区，然后将缓冲区图层放置在边界图层的上面就可以形成这样的效果。\n\n# 代码展示\n\n下面的代码展示了如何制作面状地物的缓冲区。\n\n```Python\nfrom pathlib import Path\nimport ogr\n\nogr.UseExceptions()\n\nin_ds: ogr.DataSource = ogr.Open('../data/China.shp')\nin_lyr: ogr.Layer = in_ds.GetLayer()\n\nfname: str = 'Buffer.shp'\n# 创建输出Buffer文件\ndriver: ogr.Driver = ogr.GetDriverByName('ESRI Shapefile')\nif Path(fname).exists():\n    driver.DeleteDataSource(fname)\n# 新建DataSource，Layer\nout_ds: ogr.DataSource = driver.CreateDataSource(fname)\nout_lyr: ogr.Layer = out_ds.CreateLayer(fname,\n                                        in_lyr.GetSpatialRef(), ogr.wkbPolygon)\ndef_feature: ogr.FeatureDefn = out_lyr.GetLayerDefn()\n\n# 遍历原始的Shapefile文件给每个Geometry做Buffer操作\nfor feature in in_lyr:\n    geometry = feature.GetGeometryRef()\n    buffer = geometry.Buffer(20000.0)\n    out_feature = ogr.Feature(def_feature)\n    out_feature.SetGeometry(buffer)\n    out_lyr.CreateFeature(out_feature)\n    out_feature = None\nout_ds.FlushCache()\ndel in_ds, out_ds\n```\n\n结果展示（紫色部分就是我们的缓冲区）：\n\n![缓冲区分析](/images/geos/屏幕快照2020-06-14下午10.07.31.png)\n\n# 方法总结\n\n1. 首先我们使用`Driver`类的`CreateDataSource()`方法创建输出缓冲区文件。\n2. 然后使用`DataSource`类的`CreateLayer()`方法创建一个图层，该方法有三个参数，分别是图层名，空间投影以及空间几何体类型。这里我们的空间投影直接从原始的Shapefile中进行读取，输入的空间几何体类型设置为`ogr.wkbPolygon`。\n3. 加下来我们对原始数据图层中的每个Feature要素进行遍历，取出其中的Geometry，然后利用`Geometry`类的`Buffer()`方法生成新的缓冲区Feature。`Buffer`的参数是缓冲区的距离，距离的单位是投影坐标系中的默认单位。\n4. 生成新的缓冲区Geometry以后，我们新建一个Feature并使用`SetGeometry()`将缓冲区Geometry设置为该Feature的Geometry。最后使用`CreateFeature()`方法将该Feature添加到图层中。\n5. 如此循环，直到遍历完所有的Feature为止。","tags":["GDAL","GIS","空间分析","缓冲区分析"],"categories":["空间数据处理"]},{"title":"矢量数据空间查询","url":"/geos/矢量数据空间查询/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-14 周天\n\n---\n\n# 开篇\n\n在前面四篇博客中我们主要讲了对于空间矢量数据的属性数据的增删改查，在这篇博文中我们要讲解空间查询--GIS系统很重要的一项功能。空间查询就是根据地物的空间位置进行查询的一种数据检索方式。比如，我们要查询一条河流经的城市；一个公园内的所有路灯；离当前位置最近的公共卫生间等等都属于常用的空间查询。\n\nOGC[简单要素规范](https://www.ogc.org/standards/sfa)定义了空间几何体之间的空间关系，包括Equals，Disjoint，Intersects，Touches，Crosses，Within，Contains，Overlaps，Relate，LocateAlong，LocateBetween。感兴趣的同学可以从OGC官网下载下来看看。\n\n现有的空间数据库例如Oracle Spatial，PostGIS，SQL Server都根据OGC简单要素规范提供了对空间查询的支持，他们有差异地在标准SQL语句中添加了空间关系查询的功能。\n\n本文主要介绍如何使用GDAL库对空间数据进行空间查询，常用的方法可以概括为三大类：\n\n1. 第一类就是使用支持空间查询的SQL语句进行查询，但是这种方式只对某些特定种类的数据源可以使用，有些数据源不一定支持。\n2. 第二类是使用GDAL提供的`SetSpatialFilter()`方法增加过滤条件。但是这种方式只能是选择给定范围的空间地位，类似于Within或者Contains的功能，不能实现其他类型的空间关系查询。\n3. 第三类就是读取每个Feature要素包含的Geometry对象，根据Geometry的空间关系手动进行筛选。因为GDAL中的Geometry对象基本上实现了OGC简单要素规范定义的空间关系，所以这种方式最灵活，本文主要介绍如何使用这种方式进行空间查询。\n\n# 案例一\n\n## 案例说明\n\n我们现在有省的面状数据以及每个城市的点数据，我们需要找到湖北省内的所有城市。\n\n实现思路是先从省的面状数据中找出湖北省，然后遍历城市的点数据看是否落在湖北省境内。\n\n## 代码演示\n\n```Python\nimport ogr\nogr.UseExceptions()\n\nds_province: ogr.DataSource = ogr.Open('../data/Provinces.shp')\nl_province: ogr.Layer = ds_province.GetLayer()\n# 使用filter()方法找出湖北省\nf_hubei: ogr.Feature = next(filter(lambda f: '湖北' in f.GetField('NAME'), l_province))\n\nds_city: ogr.DataSource = ogr.Open('../data/Cities.shp')\nl_city: ogr.Layer = ds_city.GetLayer()\n# 使用filter()方法过滤出落在湖北省境内的所有市\nselected = filter(lambda f: f.GetGeometryRef().Within(f_hubei.GetGeometryRef()), l_city)\nfor city in selected:\n    print(city.GetField('name'))\ndel ds_province\ndel ds_city\n```\n\n\n\n## 方法总结\n\n1. 使用`ogr.Open()`函数读取Shapefile数据，使用`GetLayer()`获取当前图层，图层中包含了所有的Feature要素。\n2. 使用Python的内置`filter()`函数对省进行过滤，通过`NAME`字段找出湖北省。`filter()`函数的第一个参数是一个自定义函数，第二个参数是一个可迭代对象iterable。该函数会遍历可迭代对象将满足第一个自定义函数的值过滤出来。通过`next()`方法拿到迭代器的当前值，即湖北省的Feature对象。\n3. 继续使用`filter()`函数对城市的点数据进行筛选，这里通过Feature的`GetGeometryRef()`方法获得要素代表的几何体，然后调用Geometry的`Within()`方法判断该城市是否落在湖北省对应的Geometry中。\n\n# 案例二\n\n## 案例说明\n\n我们将使用城市的点数据获取离武汉市最近的三座城市。\n\n实现的思路是首先从数据中找到武汉市，然后计算每个城市到武汉市的距离并排序，对排好序的Feature选择前三即可。\n\n## 代码演示\n\n```Python\nimport ogr\nogr.UseExceptions()\n\nds: ogr.DataSource = ogr.Open('../data/Cities.shp')\ncities: ogr.Layer = ds.GetLayer()\n# 使用filter()方法找出武汉市\ncity: ogr.Feature = next(filter(lambda f: '武汉' in f.GetField('name'), cities))\n# 调用ResetReading()方法特别重要，如果不ResetReading的话后面的对Feature的遍历会出错\ncities.ResetReading()\n# 根据每个市到武汉市的距离进行排序\nselected = sorted(cities, key=lambda f: f.GetGeometryRef().Distance(city.GetGeometryRef()))\nfor i in range(1, 4):\n    print(selected[i].GetField('name'))\ndel ds\n```\n\n\n\n## 方法总结\n\n1. 跟案例一一样，我们使用Python的内置`filter()`函数对市进行过滤，通过`NAME`字段找出武汉市。\n2. 需要特别注意了，当我们遍历完一遍Layer的Feature以后需要调用`ResetReading()`对迭代器重新归位，否则后面要继续进行要素遍历的话会出错。\n3. 接着我们使用Python内置函数`sorted()`根据每个城市到武汉市的距离进行排序。`sorted()`函数包含三个参数（后两个可选），第一个参数是一个可迭代对象iterable，第二个参数是用于自定义排序的函数，第三个参数指定是否逆序。`sorted()`函数的返回值是一个`list`对象。\n4. 对于距离的计算，我们首先使用`GetGeometryRef()`函数获得要素对应的空间几何体，然后再使用Geometry对象的`Distance()`函数进行。\n5. 计算完以后我们从第二个元素进行输出，因为第一个元素肯定是武汉市，武汉市到武汉市的距离为0，为最小距离。","tags":["GDAL","矢量数据","空间查询","简单要素模型"],"categories":["空间数据处理"]},{"title":"Shapefile属性操作之查","url":"/geos/Shapefile属性操作之查/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-13 夜\n\n---\n\n# 开篇\n\n在前面几篇博文中，我们分别介绍了矢量空间数据的属性数据的增（CREATE）删（DELETE）改（UPDATE）操作，这篇博文我们来聊聊属性数据的查询（Retrieve）操作。\n\n# 案例介绍\n\n我们还是使用之前的分省Shapefile数据，主要包含每个省的一些统计信息。下面以两个案例进行介绍：\n\n1. 从给定数据中查询中学数量（HighSchool字段）大于1万所的省份\n2. 从给定数据中查询中学数量最多的省份\n\n下面简单说一下使用GDAL进行属性数据查询的两种思路：\n\n1. 因为矢量数据的属性一般都是以关系表进行保存的，所以我们可以使用关系数据库查询语言SQL进行数据查询。GDAL支持部分SQL查询功能。\n2. 我们可以遍历图层Layer中包含的所有Feature要素，然后读取要素的属性数据进行筛选过滤得到我们想要的结果。\n\n# SQL查询方式\n\n##  代码展示\n\nTalk is cheap. Show me the code.\n\n首先，我们来看看使用SQL进行查询的代码。\n\n```Python\nfrom osgeo import ogr\nogr.UseExceptions()\n\nds: ogr.DataSource = ogr.Open('../data/Provinces.shp')\n# 注意Layer的名称不能包含中文\nlayer: ogr.Layer = ds.GetLayer()\n# 选择出中学数量大于1万所的省份\nquery: str = f'SELECT NAME, HighSchool FROM {layer.GetName()} WHERE HighSchool > 10000'\nselected: ogr.Layer = ds.ExecuteSQL(query)\n# 这里的Feature中只包含两个属性NAME和HighSchool\nfor feature in selected:\n    print(feature.GetField('NAME'))\n\n# 选择出中学数量最多的省份\n# 我尝试使用MAX函数和嵌套的SELECT语句进行实现，但是执行报错，应该是OGR不支持嵌套的SQL查询\nquery: str = f'SELECT NAME, HighSchool FROM {layer.GetName()} ORDER BY HighSchool DESC'\nselected: ogr.Layer = ds.ExecuteSQL(query)\nprint(selected.GetFeature(0).GetField('NAME'))\nprint(selected.GetFeature(0).GetField('HighSchool'))\n\ndel ds\n```\n\n##  方法总结\n\n1. 首先我们使用`ogr.Open()`函数读取数据，使用`GetLayer()`方法获取图层\n2. 然后构造SQL查询语句，图层名称对应的是SQL语句中的表名。注意如果图层名称为中文，查询会失败。\n3. 然后使用`ExecuteSQL()`方法执行查询，得到的查询结果仍然是一个`ogr.Layer`图层类，但是图层中的要素属性只包含我们查询语句中指定的字段\n4. 对于第二个案例，我们本来可以使用一个嵌套的SELECT查询语句以及MAX聚合函数得到最大的中学数量，但是使用嵌套的SQL查询执行会失败。所以我在第二个案例中使用了ORDER BY子句进行排序，然后查询结果的第一个要素就是我们寻找的最大值。\n\n# 遍历Feature要素方式\n\n## 代码展示\n\n我们再来看使用遍历Feature要素的方法。\n\n```Python\nfrom osgeo import ogr\nogr.UseExceptions()\n\nds: ogr.DataSource = ogr.Open('../data/Provinces.shp')\n# 注意Layer的名称不能包含中文\nlayer: ogr.Layer = ds.GetLayer()\n# 使用filter函数对要素属性进行过滤\nselected = list(filter(lambda f: f.GetField('HighSchool') > 10000, layer))\nfor feature in selected:\n    print(feature.GetField('NAME'))\n\n# 使用sorted方法对要素进行自定义排序，这里使用逆序\nselected = sorted(layer, key=lambda f: f.GetField('HighSchool'), reverse=True)\nprint(selected[0].GetField('NAME'))\nprint(selected[0].GetField('HighSchool'))\n\ndel ds\n```\n\n## 方法总结\n\n1. 使用遍历Feature要素的方法进行查询是我们在获取了图层包含的所有Feature要素集合以后，使用Python内置的函数对该集合进行过滤，排序等操作得到我们想要的查询结果。\n2. `filter()`函数的第一个参数是一个自定义函数，第二个参数是一个可迭代对象iterable。该函数会遍历可迭代对象将满足第一个自定义函数的值过滤出来。\n3. `sorted()`函数包含三个参数（后两个可选），第一个参数是一个可迭代对象iterable，第二个参数是用于自定义排序的函数，第三个参数指定是否逆序。`sorted()`函数的返回值是一个`list`对象。\n4. 我更喜欢使用第二种遍历的方式，因为更方便调试一些。当然如果对SQL语言熟悉的同学，可能更喜欢SQL这种声明式编程的方式。","tags":["Shapefile","GDAL","矢量数据","属性操作","增加字段"],"categories":["空间数据处理"]},{"title":"Shapefile属性操作之改","url":"/geos/Shapefile属性操作之改/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-13\n\n---\n\n# 开篇\n\n在上篇的《[Shapfile属性操作之增](https://theonegis.blog.csdn.net/article/details/106735196)》和《[Shapefile属性操作之删](https://blog.csdn.net/theonegis/article/details/106735451)》中我们分别介绍了对于空间矢量数据属性的增加（CREATE）和删除（DELETE）操作，这篇我们聊聊数据的更新操作（UPDATE）。\n\n# 案例介绍\n\n这里我们要处理的数据是中国地图分省的矢量Shapefile，是一个面状数据。该数据有一个`NAME`字段，给出了每个省的名称。这里我们要更新`NAME`字段，给直辖市名称后面添加“市”字，给自治区后面添加“自治区”字样，给特别行政区后面添加“特别行政区”字样，剩下的省名称后面添加“省”字。\n\n思路是我们遍历图层中的每一个Feature要素，然后通过`ogr.Feature`的`SetField()`方法更新属性值。\n\n# 代码展示\n\nTalk is cheap. Show me the code.\n\n```Python\nfrom osgeo import ogr\nogr.UseExceptions()\n\n\n# 打开一个Shapefile\nds: ogr.DataSource = ogr.Open('../data/省级行政区.shp', update=True)\nlayer: ogr.Layer = ds.GetLayer()\n\n# 填充属性值\nfor feature in layer:\n    name: str = feature.GetField('NAME')\n    if name in ('北京', '天津', '重庆', '上海'):\n        name += '市'\n    elif name in ('内蒙古', '广西', '宁夏', '新疆', '西藏'):\n        name += '自治区'\n    elif name in ('香港', '澳门'):\n        name += '特别行政区'\n    else:\n        name += '省'\n    feature.SetField('NAME', name)\n    # 修改完了记得Set一下\n    layer.SetFeature(feature)\n\n# 关闭数据集\nds = None\n\n```\n\n\n\n# 方法总结\n\n1. 首先，我们使用`ogr.Open()`函数打开Shapefile数据，注意我们要设置`update`参数为`True`，即允许GDAL更新我们的原始数据。\n2. 使用`GetLayer()`方法获取图层，然后使用for循环遍历图层中的要素。通过`GetField()`方法获取需要修改的字段，然后通过`SetField()`方法修改字段。\n3. 记得添加完属性值以后，需要使用`SetFeature()`方法将当前`feature`更新到涂层`layer`中去。\n\n","tags":["Shapefile","GDAL","矢量数据","属性操作","增加字段"],"categories":["空间数据处理"]},{"title":"Shapefile属性操作之删","url":"/geos/Shapefile属性操作之删/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-13 又是一个周末\n\n---\n\n# 开篇\n\n延续上篇的《[Shapfile属性操作之增](https://theonegis.blog.csdn.net/article/details/106735196)》，这篇我们来聊聊如何进行属性删除。使用的工具依旧是我们的GDAL库。\n\n# 案例介绍\n\n这里我们要处理的数据是中国地图分省的矢量Shapefile，是一个面状数据。在上篇中我们给该数据添加了一个属性字段`Abbr`用以表示省的简称。这篇我们再把该字段给删除掉。\n\n删除的方法包括两部：首先，从属性表中找到该字段，然后删除该字段。\n\n# 代码展示\n\nTalk is cheap. Show me the code.\n\n```Python\nfrom osgeo import ogr\nogr.UseExceptions()\n\n\n# 从给定图层中读取字段的定义，根据给定字段名称找到该字段的索引编号\ndef get_field_index_by_name(layer: ogr.Layer, name: str):\n    defs: ogr.FeatureDefn = layer.GetLayerDefn()\n    for i in range(defs.GetFieldCount()):\n        if name == defs.GetFieldDefn(i).GetName():\n            return i\n    raise ValueError(f'{name} not found')\n\n\n# 打开一个Shapefile文件\nds: ogr.DataSource = ogr.Open('../data/省级行政区.shp', update=True)\nlayer: ogr.Layer = ds.GetLayer()\n# 删除Abbr字段\nindex = get_field_index_by_name(layer, 'Abbr_1')\nlayer.DeleteField(index)\nds = None\n\n```\n\n\n\n# 方法总结\n\n1. GDAL的图层`ogr.Layer`类提供了`DeleteField()`方法用于删除字段，但是该方法传入的参数必须是要删除字段的索引编号。\n2. 我们一般的业务需求是根据字段名称去进行删除操作，所以我自定义了一个`get_field_index_by_name()`函数用于从给定图层中查找给定字段名称对应的索引编号。该函数接受两个参数，第一个是图层变量，第二个是字段名称。该函数实现的逻辑是遍历传入的图层中的字段的定义，找到和给定名称相同的字段并返回其索引。有一种特殊情况是传入的字段名称在当前图层中并不存在，对于这种情况，我们直接抛出一个`ValueError`错误。\n3. 这样我们就可以采用`layer`的`DeleteField()`方法进行删除了。","tags":["Shapefile","GDAL","矢量数据","属性操作","增加字段"],"categories":["空间数据处理"]},{"title":"Shapefile属性操作之增","url":"/geos/Shapefile属性操作之增/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-06-13 又是一个周末\n\n---\n\n# 开篇\n\n《[Python空间数据处理实战](https://blog.csdn.net/theonegis/article/details/80089375)》系列的博文好久都没有更新了，今天乘周末有点时间，补了个觉，然后写几篇博文。\n\n关于Python空间数据处理，如果大家有什么想看到的内容，欢迎评论区留言，我会增加一些大家需要的内容！\n\n今天我打算用四篇博客简要介绍一下如何对空间矢量数据的属性数据进行操作。对于属性数据，我们可以简单将其看作一个二维表格，学过数据库的朋友，可能会想到关系数据库的概念。是的，我们现在的空间矢量数据的属性数据大部分就是以关系表的形式进行存储的。而对于关系数据库的操作，常用的就是增删改查（CRUD，即Create，Retrieve，Update，Delete）操作。\n\n这篇博文单讲Create增加操作，使用的工具还是我们的GDAL库。\n\n# 案例介绍\n\n我还是喜欢用案例的方式进行知识的讲解。这里我们要处理的数据是中国地图分省的矢量Shapefile，是一个面状数据。该数据有一个`NAME`字段，给出了每个省的名称。然后我们需要给给数据增加一个属性字段`Abbr`，用以表示每个省的简称。\n\n要完成这项工作，我们首先需要建立一个省份名称到简称的字典，方便程序查询。然后读取原始数据，新建一个属性字段`Abbr`，然后遍历数据中的每个Feature要素，取出`NAME`，再根据字典查询到当前`NAME`对应的`Abbr`填充进对应的字段即可。\n\n# 代码展示\n\nTalk is cheap. Show me the code.\n\n```Python\nfrom osgeo import ogr\nogr.UseExceptions()\n\n\n# 首先定义每个省全称到简称的映射字典\nnames = {\n    '北京': '京',\n    '天津': '津',\n    '重庆': '渝',\n    '上海': '沪',\n    '河北': '冀',\n    '山西': '晋',\n    '辽宁': '辽',\n    '吉林': '吉',\n    '黑龙江': '黑',\n    '江苏': '苏',\n    '浙江': '浙',\n    '安徽': '皖',\n    '福建': '闽',\n    '江西': '赣',\n    '山东': '鲁',\n    '河南': '豫',\n    '湖北': '鄂',\n    '湖南': '湘',\n    '广东': '粤',\n    '海南': '琼',\n    '四川': '川/蜀',\n    '贵州': '黔/贵',\n    '云南': '云/滇',\n    '陕西': '陕/秦',\n    '甘肃': '甘/陇',\n    '青海': '青',\n    '台湾': '台',\n    '内蒙古': '蒙',\n    '广西': '桂',\n    '宁夏': '宁',\n    '新疆': '新',\n    '西藏': '藏',\n    '香港': '港',\n    '澳门': '澳'\n}\n\n# 打开一个Shapefile文件获取属性定义\nds: ogr.DataSource = ogr.Open('../data/省级行政区.shp', update=True)\nlayer: ogr.Layer = ds.GetLayer()\ndefs: ogr.FeatureDefn = layer.GetLayerDefn()\nfor i in range(defs.GetFieldCount()):\n    defn: ogr.FieldDefn = defs.GetFieldDefn(i)\n    print(f'{defn.GetName()} ->  {defn.GetType()} -> {defn.GetWidth()}')\n\n# 添加一个省简称的字段\nfield: ogr.FieldDefn = ogr.FieldDefn('Abbr', ogr.OFTString)\nfield.SetWidth(5)\nlayer.CreateField(field)\n\n# 填充属性值\nfor feature in layer:\n    name: str = feature.GetField('NAME')\n    feature.SetField('Abbr', names.get(name, ''))\n    # 修改完了记得Set一下\n    layer.SetFeature(feature)\n\n# 关闭数据集\nds = None\n\n```\n\n\n\n# 方法总结\n\n下面我们来对上面的代码进行一个方法的总结：\n\n1. 首先，我们使用`ogr.Open()`函数打开Shapefile数据，注意我们要设置`update`参数为`True`，即允许GDAL更新我们的原始数据。\n2. 使用`ogr.FieldDefn()`函数新建一个字段，然后添加到`layer`图层中。注意我们这里新建的字段的类型是字符串类型`ogr.OFTString`，当然我们还可以新建其他类型的字段，例如整形`ogr.OFTInteger`，实数形`ogr.OFTReal`，日期型`ogr.OFTDate`等。一般对于字符串类型，我们还需要设置字符串的宽度。\n3. 接下来我们遍历`layer`中的`feature`，使用`SetField()`方法设置属性值。记得添加完属性值以后，需要使用`SetFeature()`方法将当前`feature`更新到涂层`layer`中去。","tags":["Shapefile","GDAL","矢量数据","属性操作","增加字段"],"categories":["空间数据处理"]},{"title":"QGIS制图中面积小的区域不显示注记","url":"/geos/QGIS制图中面积小的区域不显示注记/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-05-24 周天\n\n---\n\n# QGIS制图中面积太小的区域不显示注记\n\n在使用QGIS进行制图的过程中，对于面积太小的区域有可能存在注记显示不出来的情况。比如在中国地图中，香港和澳门区域面积较小，就存在显示不了注记的情况。\n\n如下图，就会发现澳门没有显示出来。\n\n![屏幕快照2020-05-24上午7.36.07](/images/QGIS/屏幕快照2020-05-24上午7.36.07.png)\n\n这时候，我们需要进行一些额外的设置，右键相应的图层，选择属性，在注记选项卡中进行设置（`Show all labels for this layer`），如下图。\n\n![屏幕快照2020-05-24上午7.36.30](/images/QGIS/屏幕快照2020-05-24上午7.36.30.png)\n\n设置完成以后应该就可以显示了，但是有时候仍然不能正确显示，我们还需要进行如下设置（`whole polygon`）：\n\n![屏幕快照2020-05-24上午7.37.18](/images/QGIS/屏幕快照2020-05-24上午7.37.18.png)\n\n最终结果可能存在标签注记压盖的情况，这时候我们可以手动对标签的位置进行调整，达到视觉上清晰的状态。\n\n![屏幕快照2020-05-24上午7.38.47](/images/QGIS/屏幕快照2020-05-24上午7.38.47.png)\n\n","tags":["QGIS","制图","技巧"],"categories":["空间数据处理"]},{"title":"QGIS面数据融合以后有小的线段或者洞存在如何解决","url":"/geos/QGIS面数据融合以后有小的线段或者洞存在如何解决/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-05-24 周末\n\n---\n\n# QGIS面数据融合以后有小的线段或者洞存在如何解决\n\n在QGIS中我们使用`Dissolve`工具进行面数据的融合，如下图对中国分省的矢量面数据进行融合得到国界\n\n![屏幕快照2020-05-24上午7.15.23](/images/QGIS/屏幕快照2020-05-24上午7.15.23.png)\n\n![屏幕快照2020-05-24 上午7.13.58](/images/QGIS/屏幕快照2020-05-24 上午7.13.58.png)\n\n![屏幕快照2020-05-24上午7.15.57](/images/QGIS/屏幕快照2020-05-24上午7.15.57.png)\n\n可以看到融合结果中存在明显的细小线段存在，那么融合处理这些问题呢？\n\n我们可以使用QGIS提供的`Delete holes`工具进行处理（从菜单栏Processing -> Toolbox进行打开，然后根据名称搜索相关工具）\n\n![屏幕快照2020-05-24上午7.18.08](/images/QGIS/屏幕快照2020-05-24上午7.18.08.png)\n\n下面是处理以后的结果图，可以看到讨厌的线段被清除了。\n\n![屏幕快照2020-05-24上午7.19.12](/images/QGIS/屏幕快照2020-05-24上午7.19.12.png)\n\n总结一下，QGIS面数据融合的步骤是首先使用`Dissolve`工具进行面融合，然后使用`Delete holes`工具进行离散的线段或者小洞的删除。","tags":["QGIS","制图","技巧"],"categories":["空间数据处理"]},{"title":"Scala和Kotlin脚本编程","url":"/java/Scala和Kotlin脚本编程/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n作者：阿振\n\n写作时间：2020-05-05 五一假期末\n\n---\n\n# Scala和Kotlin脚本编程\n\nScala和Kotlin作为运行在JVM上的编程语言，解决了Java的很多痛点。今天我们来聊聊如何将Scala和Kotlin作为脚本语言使用（Java不支持以脚本形式运行哦）。\n\n## Kotlin脚本编程\n\nKotlin脚本的扩展名为`kts`，运行命令为`kotlinc -script <脚本文件名.kts>`\n\n### HelloWorld示例\n\n下面我们来看一个HelloWorld示例：\n\n新建名称为`Main.kts`的Kotlin脚本文件，内容如下：\n\n```Kotlin\nprintln(\"你好 ${if (args.isNotEmpty()) args[0] else \"\"}!\")\n```\n\n可以看到脚本程序不需要主函数，Kotlin脚本以`args`参数接收用户输入（args是一个`Array<String>`类型的数组）\n\n在命令行执行：`kotlinc -script Main.kts 高寒`\n\n得到脚本运行结果为：`你好 高寒!`\n\n### 调用外部命令\n\n如果我们想在Kotlin脚本中调用外部的命令或者程序需要怎么做呢？\n\n使用Java API中提供的`Runtime.getRuntime().exec()`函数或者`ProcessBuilder`类创建一个`Process`对象调用外部命令。\n\n下面以调用系统`ls`命令作为示例程序进行演示说明。\n\n```Kotlin\nimport java.lang.Runtime\n\nval process: Process = Runtime.getRuntime().exec(\"ls /Users/TheOneGIS/Desktop\")\nprocess.waitFor()\n\nprocess.inputStream.reader().use {\n    println(it.readText())\n}\n```\n\n或者\n\n```Kotlin\nimport java.lang.ProcessBuilder\n\nval process: Process = ProcessBuilder(\"ls\", \"/Users/TheOneGIS/Desktop\").start()\nprocess.waitFor()\n\nprocess.inputStream.reader().use {\n    println(it.readText())\n}\n```\n\n在命令行中执行` kotlinc -script Main.kts`，输出结果如下：\n\n```Kotlin\n11\ndata\nraw\n[这里有一个空行]\n```\n\n\n\n注意：\n\n1. `Runtime.getRuntime().exec()`函数中直接输入命令名称加参数组成的字符串\n2. `ProcessBuilder`使用多个参数进行命令名称和参数进行类的初始化\n3. 命令中的路径名称不能使用`~`特殊字符，否则会出错。建议使用全路径。\n4. `Process.waitFor()`等待调用的外部程序执行完毕再接着执行脚本后续代码。\n5. 从输出结果的InputStream中得到文本输出，可以看到最终的文本输出每一行之后都会增加一个换行符。\n\n## Scala脚本编程\n\nScala脚本的扩展名仍为`scala`，运行命令为`scala <脚本文件名.scala>`，和普通的Scala类一样。\n\n### HelloWorld示例\n\n新建名称为`Main.scala`的Scala脚本文件，内容如下：\n\n```Scala\nprintln(s\"你好 ${if (args.nonEmpty) args(0) else \"\"}!\")\n```\n\n可以看到和Kotlin脚本程序一样不需要主函数，Scala脚本同样以`args`参数接收用户输入（args是一个`Array[String]`类型的数组）\n\n在命令行执行：`scala Main.scala 高寒`\n\n得到脚本运行结果为：`你好 高寒!`\n\n### 调用外部命令\n\n在Scala中可以调用Java方法，我们可以直接使用上面类似Kotlin的方式，但是Scala中为我们提供了更加便捷的方法来调用外部命令或程序，下面我们来看一下吧！\n\n1. 使用`Process`的`!`方法，得到执行结果的状态码，一般0表示成功\n2. 使用`Process`的`!!`方法，得到执行结果的文本输出\n3. 使用`Process`的`lazyLines`方法，将得到保存在`LazyList[String]`结构的输出中（延迟执行）\n\n注意：这里的`Process`类是Scala类库中提供的，前面在Kotlin中使用的`Process`类是Java类库中提供的。\n\n我们先来看第一种方式：\n\n```Scala\nimport sys.process._\nProcess(\"ls /Users/TheOneGIS/Desktop\").!\n```\n\n在命令行中运行`scala Main.scala`，得到执行结果：\n\n```Scala\n11\ndata\nraw\n```\n\n`Process`的`!`方法是有返回值的，这里我们直接舍弃了，并不关心`ls`的返回值，我们只关心`ls`命令的输出结果。\n\n再看第二种方式：\n\n```Scala\nimport sys.process._\nval results = Process(\"ls /Users/TheOneGIS/Desktop\").!!\nprintln(results)\n```\n\n在命令行中运行`scala Main.scala`，得到执行结果：\n\n```Scala\n11\ndata\nraw\n[这里有一个空行]\n```\n\n可以看到`results`字符串是给`ls`命令的每个输出字符后面都加了换行符。\n\n再看第三种方式：\n\n```Scala\nimport sys.process._\nval results = Process(\"ls /Users/TheOneGIS/Desktop\").lazyLines\nresults.foreach(println)\n```\n\n输出结果和第一种方式一样，不过当需要获取最后输出并且需要对输出进行操作的时候我最喜欢使用第三种方式，不需要对输出进行进一步处理的时候我喜欢使用第一种方式。\n\n## Scala和Kotlin脚本编程的异同\n\n1. Scala脚本的扩展名和执行方式和普通Scala类一样；Kotlin脚本的扩展名为`kts`，执行的时候需要加`-script`参数\n2. Scala提供了对于外部命令调用的快捷方法；Kotlin主要依靠Java类库进行外部命令调用\n3. 在IntelliJ IDEA中可以直接运行Kotlin脚本，但是不支持直接运行Scala脚本\n4. IntelliJ IDEA提供了对[Ammonite](https://ammonite.io)项目的支持，可以运行Ammonite Scala脚本\n\n注：Ammonite项目扩展了Scala的脚本功能，添加了很多额外的特性，甚至提供了一个基于Scala的Shell（类似于Bash Shell）。用Ammonite写的Scala脚本扩展名为`sc`，使用`amm`命令进行执行。感兴趣的童鞋可以去围观该项目。","tags":["Scala","Kotlin","Script"],"categories":["Java"]},{"title":"SNAP Java API处理Sentinel-1数据","url":"/geos/SNAP-Java-API处理Sentinel-1数据/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2020年4月14日 周二\n\n---\n\n# 使用SNAP JAVA API处理Sentinel-1数据\n\nSNAP软件使用Java语言开发，提供了Python接口snappy，官方教程中也多以Python接口进行示范。但是我在使用Python接口过程中，发现并不是很好用，你必须要同时懂Java语言才能很好地使用Python接口，在IDEA中使用Python接口的代码基本上没有提示，报错了也是Java的错误提示。而且，Java本来是运行在虚拟机上的语言，效率不高，再用Python包一层，更加降低了运行效率。\n\n所以，对于我来说，SNAP的Python接口就是鸡肋，没有使用的必要。直接使用Java接口，方便程序直接进行调试，有问题可以直接去看源代码，解决了官方资料不足的问题。\n\n## SNAP GPF的使用范式\n\nSNAP推荐使用GPF（Graph Processing Framework）进行数据处理。GPF的使用也很简单，首先基于Operation创建Product，然后写入Product。在写入过程中会自动执行Operation完成你想要的数据处理流程，处理的算法是封装在Operation中的。\n\n使用GPF的好处是你可以进行多个Operation的流程处理，前一个处理结果直接进入后一个处理，不需要写入磁盘，可以减少磁盘IO带来的时间消耗。\n\n## 对Sentinel-1 GRD数据的处理案例\n\n下面以对Sentinel-1 GRD数据进行一系列预处理流程演示SNAP Java接口的使用。\n\n预处理的流程包括：首先对GRD各个极化波进行段辐射校正输出后向散射系数（Sigma nought），然后进行数据裁剪，最后进行地形校正的。\n\n下面是使用Better Java （Kotlin）进行entinel-1 GRD数据处理的源码。其中，`GPF.createProduct`方法需要传入Operation的名称和参数，这些信息都可以通过查看Java源代码的方式找到。\n\n代码运行过程中需要下载DEM数据，可能会比较耗时。如果不对数据裁剪，由于原始数据太大，可能会造成内存溢出。我的笔记本根本跑不动。\n\n```Kotlin\npackage cn.demo\n\nimport java.nio.file.Paths\nimport org.apache.commons.io.FilenameUtils\n\nimport kotlin.collections.HashMap\n\nimport org.esa.snap.core.dataio.ProductIO\nimport org.esa.snap.core.gpf.GPF\nimport com.bc.ceres.core.PrintWriterConciseProgressMonitor\n\nimport org.locationtech.jts.io.WKTReader\n\n\nfun main() {\n    val srcPath =\n    Paths.get(\"/Users/Demo/Desktop/S1A_IW_GRDH_1SDV_20200301T104455_20200301T104520_031481_03A00B_0A9F.zip\")\n    val srcProduct = ProductIO.readProduct(srcPath.toFile())\n\n    val outDir = \"/Users/Demo/Desktop\"\n    val baseName = FilenameUtils.getBaseName(srcPath.toString())\n    GPF.getDefaultInstance().operatorSpiRegistry.loadOperatorSpis()\n    for (polar in arrayOf(\"VV\", \"VH\")) {\n        // 首先进行辐射校正（CALIBRATION）\n        // 对应是的org.esa.s1tbx.calibration.gpf.CalibrationOp类\n        var parameters = HashMap<String, Any>()\n        parameters[\"outputSigmaBand\"] = true\n        parameters[\"selectedPolarisations\"] = polar\n        val caliProduct = GPF.createProduct(\"Calibration\", parameters, srcProduct)\n\n        // 然后进行裁剪，如果不裁剪，图像太大，容易OutOfMemory\n        // 对应的是org.esa.snap.core.gpf.common.SubsetOp类\n        val wktRect = \"POLYGON((108.175 33.873,108.782 33.873,108.782 33.129,108.175 33.129,108.175 33.873))\"\n        parameters[\"geoRegion\"] = WKTReader().read(wktRect)\n        parameters[\"bandNames\"] = \"Sigma0_${polar}\"\n        val subsetProduct = GPF.createProduct(\"Subset\", parameters, caliProduct)\n\n\n        // 然后进行地形校正（TERRAIN CORRECTION）\n        // 对应的是org.esa.s1tbx.sar.gpf.geometric.RangeDopplerGeocodingOp\n        val corrPath = \"${outDir}/${baseName}_Corrected_${polar}\"\n        parameters.clear()\n        parameters[\"pixelSpacingInMeter\"] = 10.0\n        parameters[\"sourceBands\"] = \"Sigma0_${polar}\"\n        val corrProduct = GPF.createProduct(\"Terrain-Correction\", parameters, subsetProduct)\n        ProductIO.writeProduct(\n            corrProduct, corrPath, \"GeoTIFF\",\n            PrintWriterConciseProgressMonitor(System.out)\n        )\n\t\t\t\t\n      \t// 最后进行对象销毁，释放内存空间\n        caliProduct.dispose()\n        subsetProduct.dispose()\n        corrProduct.dispose()\n    }\n    srcProduct.dispose()\n}\n```\n\n","tags":["Sentinel","Java","SNAP"],"categories":["空间数据处理"]},{"title":"Sentinel数据处理工具包SNAP Python开发环境搭建","url":"/geos/Sentinel数据处理工具包SNAP-Python开发环境搭建/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2020年12月4日周末\n\n---\n\n# Sentinel数据处理工具包SNAP Python开发环境搭建\n\n这篇博文主要探索安装SNAP工具包并且使用Python接口进行开发过程中搭建开发环境所踩的坑。不得不说欧空局SANP官方提供的资料太少，而且不全面。当然有问题你可以去Forum提问，可是回不回答就是另外一回事了！\n\n下面言归正传说说如何搭建SNAP Python开发环境用于处理Sentinel卫星数据吧！（这篇文章主要谈开发环境搭建，具体数据处理可以关注后续博文。）\n\n安装思路是：首先从[SNAP](https://step.esa.int/main/download/snap-download/)官网安装提供的二进制包（截至目前最新版本是7.0.0），然后再进行相应的配置即可。对于二进制包的安装没什么可说的，可是环境的配置到处是坑，按照官网的[教程](https://senbox.atlassian.net/wiki/spaces/SNAP/pages/24051781/Using+SNAP+in+your+Python+programs)根本不可能走通的。\n\n下面主要谈一下具体如何处理这些问题。\n\n## 安装问题\n\n官网提供了两种形式的Python环境配置（参见[Configure Python to use the SNAP-Python (snappy) interface](https://senbox.atlassian.net/wiki/spaces/SNAP/pages/50855941/Configure+Python+to+use+the+SNAP-Python+snappy+interface)）：第一，在安装二进制包过程中可以选择Python安装目录，SANP安装包自动帮你设置；第二，安装好以后，自己手动进行设置。\n\n我刚开始是选择第二种方式进行的，第二种方式需要手动编译安装Python接口，但是我从来没有编译成功过。（错误提示：`AttributeError: ‘list’ object has no attribute ‘join’`，然而根本不知道该怎么解决，[论坛](https://forum.step.esa.int/t/opening-img-disk-image-file-in-python/18388/10)中也有人遇到这样的错误）\n\n所以我就删掉了装好的SNAP包，重新安装。在安装过程中选择自己提前安装好的Python路径。我是使用Conda提前安装了一个3.6版本的Python环境（`conda create -n snap python=3.6`）。在安装SNAP过程中根据安装向导进行设置即可。\n\n注意：SNAP安装过程中安装向导提示要求Python是2.7，3.3或者3.4版本（官方教程中说是支持2.7，3.3或者3.6版本，不知道哪个是对的）。\n\n安装即算完成了。然后根据官方的教程进行测试，首先切换目录到SNAP Python接口snappy所在目录，macOS下在`~/.snap/snap-python`，不同操作系统在用户目录下的`.snap`文件夹中找即可。然后在控制台调用我们刚才安装SNAP过程中设置的Python命令。如果我们是使用Conda安装的Python，直接`conda activate snap`即可。\n\n```Shell\ncd <cd <snappy-dir>\n<python-exe>\n```\n在`snap-python`目录，我们可以使用如下代码进行测试Python开发接口是否正常工作。\n\n```Python\nfrom snappy import ProductIO\np = ProductIO.readProduct('snappy/testdata/MER_FRS_L1B_SUBSET.dim')\nlist(p.getBandNames())\n```\n\n然而，测试过程中你会发现有很多问题，而官方教程中根本没提供解决方案。下面细细说一下我遇到的问题以及解决方案。\n\n## 缺包问题\n\n运行以后的错误提示如下：`ImportError: No module named jpyutil`，这显然是Python找不到包的缘故。那没有的话，我们安装一个即可（需要安装JPY包，一个用于Java和Python语言直接相互调用的桥接库）。\n\n安装过程中，我直接使用`pip install jpy`进行安装，结果又有新的错误。\n\n后来，我发现[JPY](https://github.com/bcdev/jpy/releases)的GitHub网站上提供了编译好的二进制安装包，根据自己的的平台和Python版本选择合适的WHL二进制文件下载，下载以后直接使用`pip install <jpy.whl>`命令安装即可（`jpy.whl`换成你自己下载好的文件路径）。\n\n## JDK版本问题\n\n缺包问题解决以后，继续进行测试，结果提示找不到JDK的动态链接库，根据错误提示，我发现snappy查找的Python版本跟我系统设置的`JAVA_HOME`路径中的JDK不一致。可能程序中使用了自己定义好JDK版本(1.8.0_112)，而没有选择使用环境变量中提供的版本。\n\n所以我根据错误提示我下载了1.8.0_112版本的JDK二进制包进行安装，JDK的问题就算解决了。\n\nJava JDK官方网站提供的一般是最新的版本，要下载老旧的JDK可以通过网站[Oracle Java Archive](https://www.oracle.com/java/technologies/oracle-java-archive-downloads.html)选择自己的操作系统平台进行二进制包的下载安装，不要忘了提前申请一个Oracle的账号。\n\n## 环境变量问题\n\nJDK版本问题解决以后，继续进行测试。结果又提示找不到`SANP_HOME`路径，我们在环境变量中设置`SANP_HOME`变量指向到SNAP的安装目录即可。Windows下可以通过`This PC`属性进行设置，Linux可以在`~/.bashrc`文件中进行设置，macOS在`~/.bash_profile`进行设置。我的设置如下：\n\n```Shell\nexport SNAP_HOME=/Applications/snap\n```\n\n最后，我们再继续进行测试，终于看到输出结果了：\n\n```\n['radiance_1', 'radiance_2', 'radiance_3', 'radiance_4', 'radiance_5', 'radiance_6', 'radiance_7', 'radiance_8', 'radiance_9', 'radiance_10', 'radiance_11', 'radiance_12', 'radiance_13', 'radiance_14', 'radiance_15', 'l1_flags', 'detector_index']\n```\n\n关于snappy接口的具体使用，欢迎关注我的后续文章！","tags":["Python","SNAP","Sentinel影像","snappy"],"categories":["空间数据处理"]},{"title":"ESA SNAP工具包Java接口的使用","url":"/geos/ESA-SNAP工具包Java接口的使用/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2010年4月10日晚\n\n---\n\n欧盟的SNAP工具包提供了对Sentinel卫星数据的处理功能。\n\nSANP使用Java编写，UI界面使用了NetBeans框架。\n\n如果要进行批处理的话，还提供了基于图的**G**raph **P**rocessing **F**ramework (GPF)处理模式。\n\n此外，SNAP还贴心的提供了Python接口snappy（对Java API的封装）。\n\n参考资料：https://senbox.atlassian.net/wiki/spaces/SNAP/overview\n\n但是我使用官方提供的安装包安装好以后，Python接口用不了，所以我想着还不如就用原生的Java接口。\n\n首先，从[GitHub](https://github.com/senbox-org)下载SNAP的源码进行编译，主要下载了[snap-engine](https://github.com/senbox-org/snap-engine)，[snap-desktop](https://github.com/senbox-org/snap-desktop)，[s1tbx](https://github.com/senbox-org/s1tbx)和[s2tbx](https://github.com/senbox-org/s2tbx)这四个工程，依次进行编译（最新版本为8.0.0-SNAPSHOT）。\n\n例如对snap-engine的编译命令如下：\n\n```Shell\ncd snap\ngit clone https://github.com/senbox-org/snap-engine.git\ncd snap-engine\nmvn clean install -DskipTests=true\n```\n\n编译中主要问题是一些第三方库下载特别慢或者根本下载不下来，这时候你需要科学上网手动下载需要的依赖包，再次进行编译。\n\n编译完这四个包以后，我们新建Maven工程进行代码测试。下面代码使用Better Java （Kotlin）进行示范，我把注释加进了代码中方便理解。\n\n```\npackage cn.demo\n\nimport java.nio.file.Paths\nimport org.esa.snap.core.dataio.ProductIO\n\n\nfun main(args: Array<String>) {\n    val path = Paths.get(\"S2B_MSIL1C_20200304T032629_N0209_R018_T48SYC_20200304T075000.zip\")\n    // 通过文件获得读取数据的Reader\n    val reader = ProductIO.getProductReaderForInput(path.toFile())\n    // 通过这个Reader对象来读取Sentinel数据产品Product\n    val product = reader.readProductNodes(path.toFile(), null)\n    // 通过这个Product对象来读取波段中存储的数据\n    val band = product.getBand(\"B1\")\n    // 数据不是自动加载的，需要手动加载一下\n    if (!band.hasRasterData()) {\n        band.loadRasterData()\n    }\n    // 获取波段数据，数据是以一维数组的形式存储的，主要使用PixelInterleavedSampleModel这类进行控制像素交替方式存储\n    // 有兴趣的可以参考java.awt.image.PixelInterleavedSampleModel\n    val data = band.rasterData\n    assert(band.rasterHeight * band.rasterWidth == data.numElems)\n}\n\n```\n\n代码运行过程中，提示找不到OpenJPEG的库（Sentinel-2 SAFE格式的文件中是以JPG图像格式保存各个波段的观测数据的），最后我通过代码调试发现，SNAP中对OpenJPEG库的路径定义在`~/.snap/auxdata/openjpeg/8.0.0-SNAPSHOT/`目录下，而我的本地没有该目录（虽然`/usr/local/bin`\t目录中有OpenJPEG库，但是SNAP不是在这个目录中寻找的），所以程序会报错。\n\n解决的方案是我由于本来安装的SNAP桌面版（7.0.0），该版本在`~/.snap/auxdata/openjpeg/7.0.0/`目录下存放了OpenJPEG库，所以拷贝该版本的库修改名称为`8.0.0-SNAPSHOT`即可，程序正常运行。\n\n总得来说，SNAP提供的参考资料不多，有问题只能取Forum中提问。所以我觉得还是使用GDAL读取Sentinel数据，然后再进行进一步操作方便些。","tags":["SANP","Sentinel","Java"],"categories":["空间数据处理"]},{"title":"macOS下GDAL Java开发环境搭建","url":"/geos/macOS下GDAL-Java开发环境搭建/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2020年4月9日下午\n\n---\n\n# macOS下GDAL Java开发环境搭建\n\n今天在笔记本（macOS平台）上折腾了一下如何搭建GDAL的Java开发环境。虽然GDAL的Python接口更好用，但是有时候需要协同使用一些Java库的时候，也必须使用Java版本的GDAL。\n\n在macOS上安装GDAL一般有三种方式：\n\n1. 使用HomeBrew\n2. 使用[Kyng-Chaos](https://www.kyngchaos.com/software/frameworks/)提供的编译好的二进制包（Framework）\n3. 从源码编译\n\n由于前两种方式不自带有Java相关的接口，所以这里选择从源码编译。\n\n## GDAL源码编译安装\n\n首先，下载GDAL源码，可以从[GitHub](https://github.com/OSGeo/gdal)上下载，也可以从[GDAL](https://gdal.org/download.html)官网下载。\n\n我这里从官网下载了最新的2.4.4版本（3.X版本也发布了，但是还是先用稳定一点的吧）的源码进行编译安装。GDAL依赖的库都可以使用HomeBrew安装，我的方法是直接使用Brew安装GDAL，然后再卸载掉GDAL，这样GDAL依赖的第三方库都就自动安装了。\n\n此外，还需要系统中安装了Java JDK和SWIG以及Ant编译工具。\n\n下面进行GDAL源码编译并安装到`/usr/local`目录中。注意配置系统中Java的路径，我这里使用的是JDK11。\n\n```\ncd gdal-2.4.4\n./configure --with-threads --disable-static --without-grass --with-jasper=/usr/local/lib --with-libtiff=/usr/local/lib --with-jpeg=/usr/local/lib --with-gif=/usr/local/lib --with-png=/usr/local/lib --with-geotiff=/usr/local/lib --with-pcraster=internal --with-geos=/usr/local/lib --with-static-proj4=/usr/local/lib --with-expat=/usr/local/lib --with-curl=/usr/local/lib --with-netcdf=/usr/local/lib --with-hdf5=/usr/local/lib --with-opencl --with-libz=internal --without-python --with-java --with-jvm-lib=/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home \nmake\nsudo make install\n```\n\n然后接下来我们编译Java接口需要的相关文件。`make install`命令会将动态链接库` libgdalalljni.20.dylib`和`libgdalalljni.dylib`拷贝到`/usr/local/lib`目录中。\n\n```\ncd swig/java\nmake CFLAGS=\"-I/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include -I//Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include/darwin\"\nsudo make install\n```\n\n`make`命令会生成我们需要的`gdal.jar`包，是我们做Java开发需要的依赖包，必须添加到工程中。\n\n我以为这样就可以了，但是当我在工程中调用GDAL的时候，提示找不到Native Library，然后我就把` libgdalalljni.20.dylib`和`libgdalalljni.dylib`又拷贝到了`/Library/Java/Extensions`目录中，问题才得以解决。\n\n## Maven安装本地JAR\n\n我习惯使用Maven管理依赖，但是Maven中央仓库中没有提供GDAL 2.4.4版本的JAR包。所以需要把刚才生成的`gdal.jar`文件安装到本地Maven仓库中，这样才能在Maven工程中使用。\n\n使用如下命令进行安装以后，我们就可以在工程中开心的使用了！\n\n```\nmvn install:install-file -Dfile=/Users/TheOneGIS/Development/gdal-2.4.4/swig/java/gdal.jar -DgroupId=org.gdal -DartifactId=gdal -Dversion=2.4.4 -Dpackaging=jar\n```\n\n## 使用Java版GDAL示例\n\n这里使用Better Java（Kotlin）语言进行一个简单的Shapefile文件读取的示例。首先，新建Maven Kotlin工程；然后在POM文件中添加GDAL依赖，最后书写我们的HelloWorld示例。\n\n```XML\n<dependency>\n  <groupId>org.gdal</groupId>\n  <artifactId>gdal</artifactId>\n  <version>2.4.4</version>\n</dependency>\n```\n\n示例代码如下：\n\n```Kotlin\npackage cn.demo\n\nimport org.gdal.gdal.*\n\nfun main(args: Array<String>) {\n    // 初始化GDAL环境\n    gdal.UseExceptions()\n    gdal.AllRegister()\n\t\n\t\t// 读取数据，输出数据信息\n    val path = \"China.shp\"\n    val ds = gdal.OpenEx(path)\n    val layer = ds.GetLayer(0)\n    println(layer.GetName())\n    for (i in 0 until layer.GetFeatureCount()) {\n        print(layer.GetNextFeature().GetFieldAsString(\"NAME\"))\n    }\n    ds.delete()\n}\n\n```\n\n","tags":["Java","GDAL","Maven","macOS"],"categories":["空间数据处理"]},{"title":"UNIX系统下删除老旧Maven依赖包的方法","url":"/tools/UNIX系统下删除老旧Maven依赖包的方法/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2020年4月8日晚\n\n---\n\n# UNIX系统下删除老旧Maven依赖包的方法\n\n笔记本是macOS，磁盘只有256，Maven下载的依赖包，特别是老旧的JAR很占空间，如何使用一行命令删除这些依赖包呢？\n\n基本思想是使用`find`命令找出来给定时间段没有使用过的JAR包（通过POM文件的时间戳判断），然后使用`rm`命令进行删除。\n\n下面的命令通过遍历`~/.m2`文件夹下修改时间大于5天的以`.pom`为扩展名的文件，然后删除该POM文件所在的父文件夹。\n\n```\nfind ~/.m2 -ctime +5d -iname '*.pom' | while read pom; do parent=`dirname \"$pom\"`; rm -rf \"$parent\"\n```\n\n下面稍微看一下`find`命令，我们可以通过`-atime`和`-ctime`分别给出查找文件的最后访问时间获最后修改时间到当前的时间间隔（时间单位可以是s，m，h，d，w，分别代表秒，分钟，小时，天和周）。此外，我们可以直接通过`-amin`和`-cmin`分别指定以分钟为单位的时常。具体命令的使用可以使用`man find`进行查看。\n\n最后要说的是，在进行正式删除之前建议通过Dry Run查看一下那些文件要删除，不要删错了以后后悔莫及！\n所以在正式删除之前，先试试下面的命令吧！\n```\nfind ~/.m2 -ctime +5d -iname '*.pom' | while read pom; do parent=`dirname \"$pom\"`; echo \"$parent\"; done > output.txt\n```\n\nWindows下的删除没有试过，不过应该可以使用对应的BAT批处理相关命令进行操作。如果对于Windows 10，我们可以采用Linux子系统（WSL）进行操作。","tags":["Maven","JAR包","Linux","MacOS","find"],"categories":["工具"]},{"title":"手把手教你用QGIS制作地图","url":"/geos/手把手教你用QGIS制作地图/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2020年7月4日晚\n\n---\n\n# 手把手教你使用QGIS制作地图\n\nQGIS是一款开源免费的地理信息系统软件，虽然比不上商业的ArcGIS软件，但是QGIS免费而且跨平台，值得学习！\n\n今天我们聊聊如何使用QGIS进行地图制作并输出。对任意一幅地图的制作下面介绍的步骤并不是都要用得到，我会分知识点进行介绍，学习一些常用地图制作技巧。\n\n下面我们一步一步进行吧！（我是在macOS平台下进行操作的，Windows平台界面可能稍有差异）\n\n## 加载矢量数据\n\n打开QGIS，从文件管理面板Browser加载所要的数据，如下图所示（以陕西省为例）。\n\n![屏幕快照2020-04-07下午8.56.03](/images/geos/making-a-map/屏幕快照2020-04-07下午8.56.03.png)\n\n## 加载背景底图\n\n底图的加载我们可以有很多选择，比如使用OpenStreetMap或者谷歌地图。当然，我们也可以选择不使用底图。\n\n下面给出加载底图的步骤：\n\n在文件管理面板Browser的XYZ Tiles节点上右键，选择New Connection...，然后在弹出的对话框中输出Name和URL。下图给出了OpenStreetMap的添加界面。\n\n![屏幕快照2020-04-07下午8.56.24](/images/geos/making-a-map/屏幕快照2020-04-07下午8.56.24.png)\n\n添加完Connection以后，直接点击添加的地图服务节点将底图添加到我们的工程。\n\n鼠标在图层Layers面板中拖动数据层的顺序，将刚添加的底图移动到最下方的位置。如下图所示。\n\n![屏幕快照2020-04-07下午8.56.41](/images/geos/making-a-map/屏幕快照2020-04-07下午8.56.41.png)\n\n此外，这里附上谷歌地图服务的地址，方便有需要的朋友使用：\n\n**Google Maps**: https://mt1.google.com/vt/lyrs=r&x={x}&y={y}&z={z}\n\n**Google Satellite:** http://www.google.cn/maps/vt?lyrs=s@189&gl=cn&x={x}&y={y}&z={z}\n\n**Google Satellite Hybrid:** https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\n\n**Google Terrain:** https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}\n\n**Google Roads:** https://mt1.google.com/vt/lyrs=h&x={x}&y={y}&z={z}\n\n拿走不谢！\n\n## 美化矢量数据\n\n在Layers面板中选中数据层，右键选择Properties...，在弹出的对话框中选择左侧列表中的Symbology，然后设置矢量数据的填充（Fill），边线（Stroke）等。\n\n![屏幕快照2020-04-07下午9.43.53](/images/geos/making-a-map/屏幕快照2020-04-07下午9.43.53.png)\n\n## 添加晕线\n\n地图制作中有时候需要给行政边界添加晕线，制作方法很简单。思路是这样的：首先，给原始行政区做缓冲区，然后添加缓冲区到原始行政区图层下面，设置缓冲区的边线的颜色粗细。\n\n注意：我在使用QGIS的过程中，通过菜单栏Vector->Geoprocessing Tools->Buffer...工具进行缓冲区制作的时候，发现制作的缓冲区地理坐标不对（和原始的行政区地理间隔很大），我也不找到出错的原因。\n\n我通过菜单栏Processing->Toolbox打开QGIS工具箱，使用GDAL提供的Buffer工具，则不会出现错误，如下图（QGIS中集成了GDAL，GRASS等开源GIS工具，所以经常在处理一个任务的时候，我们有多个工具可以选择）。\n\n![屏幕快照2020-04-07下午9.00.53](/images/geos/making-a-map/屏幕快照2020-04-07下午9.00.53.png)\n\n做完缓冲区之后，我们需要对缓冲区进行美化（你自己认为漂亮即可），效果如下图！\n\n![屏幕快照2020-04-07下午9.02.43](/images/geos/making-a-map/屏幕快照2020-04-07下午9.02.43.png)\n\n## 切换到排版视图\n\n在ArcGIS中我们一般在进行地图输出的时候一般会切换到布局视图（好像是叫Layotu View，如果我没记错的话）进行地图整饰和出图。\n\n在QGIS中也是类似的，我们需要点击工具栏的New Print Layout（我的在保存Save Project按钮旁边，我的节目自己调整过，所以可能和标准界面不一样）。这时候会出现一个新的Tab面板（对应ArcGIS的布局视图），我们在该选项卡面板中进行操作，如下图所示。\n\n![屏幕快照2020-04-07下午9.59.15](/images/geos/making-a-map/屏幕快照2020-04-07下午9.59.15.png)\n\n在布局视图面板的左侧有一系列工具，我们首先点击Add Map按钮，在空白画布上拖动一个地图范围，这样我们刚才制作的地图就会显示在该画布上面。\n\n![屏幕快照2020-04-07下午9.04.17](/images/geos/making-a-map/屏幕快照2020-04-07下午9.04.17.png)\n\n## 添加经纬度格网\n\n下面我们添加经纬度格网，在该视图的右边Items选项卡中选择我们的地图对象，然后在Item Properties选项卡中，选择Grids节点进行展开，点击➕按钮添加一个Grid对象，然后点击Modify Grid按钮编辑格网的属性。\n\n我们可以设置格网显示的坐标系，格网显示的间隔，格网显示的样式等等。根据自己的需求自由发挥吧！\n\n![屏幕快照2020-04-07下午9.04.38](/images/geos/making-a-map/屏幕快照2020-04-07下午9.04.38.png)\n\n## 添加其他修饰元素\n\n此外，我们还可以点击面板右边的按钮添加比例尺、图例、图名、指北针等等修饰元素。这里不做详细介绍，自己慢慢探索吧！添加完以后，如下图。\n\n![屏幕快照2020-04-07下午9.14.00](/images/geos/making-a-map/屏幕快照2020-04-07下午9.14.00.png)\n\n## 地图输出\n\n最后我们要将地图输出为PDF或者图片格式进行保存，在工具栏提供了相应的按钮进行操作。\n\n我这里想说的是在QGIS地图制作过程中如果添加了地图服务（Web-Service-Based Map），则有可能在输出保存的时候，底图的显示不太对（会有缩放），我们的矢量地图不存在问题。","tags":["QGIS","地图制作","地图输出"],"categories":["空间数据处理"]},{"title":"从傅立叶级数到傅立叶变化","url":"/math/从傅立叶级数到傅立叶变化/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-10-31\n\n---\n\n写这篇博文的初衷是在翻阅数字图像处理相关教科书的时候，发现大部分对傅立叶变换的讲解直接给出了变换公式，而对于公式从何而来并没有给出说明。所以，本文在假设已经了解傅立叶级数的背景下，从傅立叶级数推导出傅立叶变换的一般公式。\n\n# 傅立叶级数\n\n学过高数的童鞋都听过傅立叶级数，下面直接给出定义，具体证明可以参考高等数学教材。\n\n设周期为$T$的周期函数$f(x)$的傅立叶级数为\n\n$$f(x) = \\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left(a_{n} \\cos \\frac{2\\pi n x}{T}+b_{n} \\sin \\frac{2\\pi n x}{T}\\right) \\tag{1}$$\n\n其中，系数$a_n$和$b_n$分别为：\n\n$$\\left.\\begin{array}{ll}{a_{n}=\\frac{2}{T} \\int_{\\frac{T}{2}}^{\\frac{T}{2}} f(x) \\cos \\frac{2\\pi n x}{T} \\mathrm{d} x} & {(n=0,1,2, \\cdots)} \\\\ {b_{n}=\\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(x) \\sin \\frac{2\\pi n x}{T} \\mathrm{d} x} & {(n=1,2,3, \\cdots)}\\end{array}\\right\\} \\tag{2}$$\n\n利用欧拉公式$$\\cos t=\\frac{\\mathrm{e}^{t \\mathrm{i}}+\\mathrm{e}^{-t i}}{2}, \\quad \\sin t=\\frac{\\mathrm{e}^{t i}-\\mathrm{e}^{-t i}}{2 \\mathrm{i}}$$\n\n可以将公式（1）转化为傅立叶级数的复数形式\n\n$$f(x) = \\sum\\limits_{n=-\\infty}^{\\infty} c_{n} e^{\\frac{2\\pi n x}{T} \\mathrm{i}} \\tag{3}$$\n\n系数$c_n$为\n\n$$c_{n}=\\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(x) \\mathrm{e}^{-\\frac{2\\pi n x}{T} \\mathrm{i}} \\mathrm{d} x \\quad(n=0, \\pm 1, \\pm 2, \\cdots) \\tag{4}$$\n\n傅立叶级数的两种形式本质上是一样的，但是复数形式比较简洁，而且只用一个算式计算系数。\n\n# 傅立叶变换\n\n傅立叶级数是针对周期函数的，为了可以处理非周期函数，需要傅立叶变换。\n\n傅立叶变换将周期函数在一个周期内的部分无限延拓，即让周期趋紧于无穷，然后就得到了傅立叶变换，如下图所示。\n\n![非周期函数延拓](/images/math/FourierTransform.jpeg)\n\n图片来源：[Fourier Transform 101 — Part 3: Fourier Transform](https://medium.com/sho-jp/fourier-transform-101-part-3-fourier-transform-6def0bd2ca9b)\n\n下面我们看一下，当周期$T$趋于$\\infty$的时候，我们看一下公式（3）和（4）的变化。\n\n令$\\frac{1}{T} = \\Delta \\omega$，则\n\n$$\\begin{align}f(x) &= \\sum\\limits_{n=-\\infty}^{\\infty} c_{n} e^{\\frac{2 \\pi n x}{T} \\mathrm{i}} \\\\ &= \\sum\\limits_{n=-\\infty}^{\\infty} c_{n} e^{2 \\pi n \\Delta \\omega x \\mathrm{i}} \\\\ &= \\sum\\limits_{n=-\\infty}^{\\infty} \\frac{1}{T} [\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(x) \\mathrm{e}^{-2\\pi n \\Delta \\omega x \\mathrm{i}} \\mathrm{d} x] e^{2 \\pi n \\Delta \\omega x \\mathrm{i}} \\\\ \\end{align}$$\n\n当$T \\to \\infty$时，$\\Delta \\omega \\to 0$，$\\Delta \\omega \\to \\mathrm{d}\\omega$ ，$\\mathrm{d}\\omega$和$n \\mathrm{d}\\omega$都成为连续的变量，记为$\\omega$。\n\n$$\\begin{align}f(x) &= \\lim_{T\\to \\infty}{\\sum\\limits_{n=-\\infty}^{\\infty} \\frac{1}{T} [\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} f(x) \\mathrm{e}^{-n \\pi xl \\mathrm{i}} \\mathrm{d} x] e^{2 \\pi n \\Delta \\omega x \\mathrm{i}}} \\\\ &= \\int_{-\\infty}^{\\infty}[\\int_{-\\infty}^{\\infty}f(x)e^{-2\\pi\\omega x \\mathrm{i}} \\mathrm{d}x]e^{2\\pi\\omega x \\mathrm{i}}\\mathrm{d}\\omega \\\\ \\end{align}$$\n\n对应于傅立叶级数，傅立叶变换可以表示为\n\n$$F(\\omega) = \\int_{-\\infty}^{\\infty}f(x)e^{-2\\pi\\omega x \\mathrm{i}} \\mathrm{d}x \\tag{5}$$\n\n而相应地傅立叶逆变换可以表示为\n\n$$f(x) = \\int_{-\\infty}^{\\infty}F(\\omega) e^{2\\pi\\omega x \\mathrm{i}}\\mathrm{d}\\omega \\tag{6}$$","tags":["傅立叶变换","傅立叶级数"],"categories":["数学"]},{"title":"Morton码","url":"/algorithm/Morton码/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-07-08 17:11:27\n\n---\n\n# Morton码的计算\n\nMorton码是对栅格格网进行编码的一种算法，在Google中搜索Morton，搜索结果第一位是Wikipedia的Z-order Curve，这是因为Morton码编码结果展现为一种Z形的填充曲线。下面简要说一下如何计算四进制和十进制的Morton码。\n\n![Morton码二进制](/images/geos/z-curve.png)\n\n## 四进制Morton码计算\n\n四进制编码对左上，右上，左下，右下的顺序对四个格网单元分布编码为0，1，2，3。\n\n其计算方式为：二进制的行列号$r$、$l$（从第0行0列开始），四进制编码$M=2*l+ r$；那么这里就是：第5行（101）第7列（111）：$M=2*101+111=313$（313对应的十进制是55）\n\n## 十进制Morton码计算\n\n十进制的编码规则：首先，行列号转为二进制（从第0行0列开始）；然后行列号交叉排列；最后将二进制结果转为十进制。十进制Morton编码是按左上，右上，左下，右下的顺序从0开始对每个格网进行自然编码的。\n\n对于第5行（101）第7列（111），交叉排列得到110111，然后转为十进制就是55。和四进制的编码结果是一样的。\n\n下面给出十进制Morton码的C++实现：\n\n```C++\n#include <iostream>\n\nusing std::cout;\n\nint main() {\n    uint32_t row = 5;\n    uint32_t col = 7;\n    uint64_t morton = 0;\n\n    for (int i = 0; i < sizeof(row) * 8; i++) {\n        morton |= (row & (uint64_t)1 << i) << i | (col & (uint64_t)1 << i) << (i + 1);\n    }\n    cout << morton << '\\n';\n    return 0;\n}\n```\n\n\n\n","tags":["Morton"],"categories":["算法"]},{"title":"NumPy中的维度Axis","url":"/python/NumPy中的维度Axis/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-04-16 14:56:53\n\n---\n\n# 浅谈NumPy中的维度Axis\n\nNumPy中的维度是一个很重要的概念，很多函数的参数都需要给定维度Axis，如何直观的理解维度呢？我们首先以二维数组为例进行说明，然后推广到多维数组。\n\n(有人将`ndim`属性叫维度，将`axis`叫轴，我还是习惯将`axis`称之为维度，`axis=0`称为第一个维度)\n\n## 二维数组的列子\n\n下面是一个二维数组的列子：\n\n```Python\nIn [1]: import numpy as np\n\nIn [2]: x = np.random.randint(0, 9, (2, 3))\n\nIn [3]: x\nOut[3]:\narray([[0, 8, 6],\n       [1, 2, 1]])\n\nIn [4]: x.ndim\nOut[4]: 2\n\nIn [5]: x.shape\nOut[5]: (2, 3)\n\nIn [6]: x[0]\nOut[6]: array([0, 8, 6])\n\nIn [7]: x[:, 0]\nOut[7]: array([0, 1])\n\nIn [8]: x.sum(axis=0)\nOut[8]: array([ 1, 10,  7])\n\nIn [9]: x.sum(axis=1)\nOut[9]: array([14,  4])\n\nIn [10]: x[0] + x[1]\nOut[10]: array([ 1, 10,  7])\n\nIn [11]: x[:, 0] + x[:, 1] + x[:, 2]\nOut[11]: array([14,  4])\n```\n\n看上面这个例子，`x`是一个2行3列的数组，所以`x`是一个二维数组。\n\n从第6和第7个输入输出，我们可以肯定地说\"对于二维数组，第一维指的是行，第二维指的是列\"。\n\n我们通过`sum`求和函数，探究一下`x`的第一维和第二维的意义？从第8个和第9个输入输出，我们可以看到对于参数`axis=0`，其结果是数组列的和；而对于参数`axis=1`，其参数是数组行的和。\n\n对于`axis=0`第一个维度求和，不是将第一维度（行）中的所有元素相加，而是沿着第一个维度，将对应其他维度（列）的数据相加，分解开来就是第10个输入输出。同理，对于`axis=1`，是沿着列，将行中的元素相加。\n\nNumPy中对于维度的操作都是以类似这样的逻辑操作的。\n\n## 多维数组\n\n对于多维数组我们如何准确区分维度呢？下面以图示进行说明：\n\n![NumPy中的维度](/images/python/NumPy中的维度.png)\n\n所以，我的结论就是：在概念上维度是从整体到局部看的，最外围的是第一个维度，然后依次往里，最内部的就是最后一维。\n\n下面我们用代码验证一下上面的结论：\n\n```Python\nIn [19]: x = np.random.randint(0, 9, (2, 3, 4))\n\nIn [20]: x\nOut[20]:\narray([[[0, 7, 5, 5],\n        [6, 3, 1, 3],\n        [7, 5, 3, 4]],\n\n       [[8, 1, 4, 6],\n        [8, 1, 4, 8],\n        [3, 0, 8, 2]]])\n\nIn [21]: x[0]\nOut[21]:\narray([[0, 7, 5, 5],\n       [6, 3, 1, 3],\n       [7, 5, 3, 4]])\n\nIn [22]: x[:, 0, :]\nOut[22]:\narray([[0, 7, 5, 5],\n       [8, 1, 4, 6]])\n```\n\n可以看到，第21个输入输出取到的是第一维的第一个元素，第22个输入输出取到的是第二维的第一个元素。大家可以细细体味一下！","tags":["NumPy"],"categories":["Python"]},{"title":"栅格数据裁剪","url":"/geos/栅格数据裁剪/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n写作时间：2019-03-22\n\n在进行遥感影像处理的时候，我们经常需要进行裁剪的工作，来看看如何使用GDAL工具进行这项操作吧！\n\n参考资料：\n\n1. [GDAL: gdalwarp](https://www.gdal.org/gdalwarp.html)\n2. [GDAL: gdal_translate](https://www.gdal.org/gdal_translate.html)\n3. [GDAL/OGR Python API](https://gdal.org/python/)\n\n## 使用GDAL命令\n\nGDAL提供了两个命令可以用于影像的裁剪：`gdalwarp`和`gdal_translate`，两个命令中我更推荐使用后者。\n\n`gdalwarp`命令可以使用`-te`制定裁剪范围。默认是在原数据的坐标系下的`xmin ymin xmax ymax`，当然我们也可以使用`-te_srs`参数指定`-te`参数所在的坐标系。\n\n为什么不推荐`gdalwarp`命令呢？这是因为`gdalwarp`命令只提供了根据坐标系的范围进行裁剪，而不支持根据行列号的裁剪。这时候我们可以求助于`gdal_translate`命令。\n\n`gdal_transalte`命令即支持使用`-srcwin`参数指定行列号范围`xoff yoff xsize ysize`，也支持使用`-projwin`参数指定原数据坐标系下的范围`ulx uly lrx lry`。同时提供参数`-projwin_srs`可以用于指定`-projwin`参数所在的坐标系，即跟`gdalwarp`命令中的`-te_srs`参数类似。\n\n下面给出一个示例：\n\n`gdal_translate -of \"GTiff\" -srcwin 10 10 256 256 -a_scale 1 HDF4_EOS:EOS_GRID:\"MOD09GA.A2018349.h26v05.006.2018351030314.hdf\":MODIS_Grid_500m_2D:sur_refl_b01_1 sr_1.tif`\n\n这行命令将MODIS数据中的反射率的第一波段进行裁剪，起点为第10行第10列，输出大小为256$\\times$255，输出格式为TIFF。\n\n注意这行命令有一个`-a_scale 1`参数，这个参数指定了裁剪过程不要对DN值进行缩放。如果不加这个值得话，输出图像的DN值会被根据原数据的`Scale=10000`放大10000倍。\n\n## 使用Python代码\n\n对于使用Python代码进行裁剪，我们有两种方法：\n\n- 第一就是对命令行对应的借口直接进行调用。这个最直接最简单。\n- 第二就是首先自己选择出需要裁剪的区域，然后计算裁剪区域的GeoTransform的系数，最后将投影和GeoTransform系数赋值给裁剪子区域，写入输出文件。\n\n我们知道[GDAL中使用了六参数模型存储GeoTransform参数](https://blog.csdn.net/theonegis/article/details/80304873)，如果进行矩形裁剪的话，只有`GT(0)`和`GT(3)`参数会有变化，即需要重新计算裁剪以后的左上角坐标即可。\n\n下面给出使用Python对MODIS反射率的第一波段进行裁剪的代码：\n\n```Python\nfrom osgeo import gdal\nimport numpy as np\n\n# API参考：https://gdal.org/python/\n# GDAL命令行参考：https://www.gdal.org/gdal_translate.html\nimage_name = ('HDF4_EOS:EOS_GRID:'\n              '\"MOD09GA.A2018349.h26v05.006.2018351030314.hdf\":'\n              'MODIS_Grid_500m_2D:sur_refl_b01_1')\n\n# 第一种方式，也是最简单的方法：直接使用GDAL命令行对应的Python方法\nsrc: gdal.Dataset = gdal.Open(image_name)\nsrc = gdal.Translate('cropped_with_translate.tif', src, srcWin=[10, 10, 256, 256],\n                     options=['-a_scale', '1'])\ndel src\n\n# 第二种方式，自己选择出需要的像素，然后自己确定裁剪以后的空间参考关系，并写入到输出文件\nsrc: gdal.Dataset = gdal.Open(image_name)\nband: gdal.Band = src.GetRasterBand(1)\nsubset: np.ndarray = band.ReadAsArray(10, 10, 256, 256)\n\ndriver: gdal.Driver = gdal.GetDriverByName('GTiff')\ndst: gdal.Dataset = driver.Create('cropped_from_scratch.tif', 256, 256, 1, gdal.GDT_Int16)\ndst.SetProjection(src.GetProjection())\ntrans = list(src.GetGeoTransform())\ntrans[0] -= -10 * trans[1]\ntrans[3] -= -10 * trans[5]\ndst.SetGeoTransform(tuple(trans))\n\nband: gdal.Band = dst.GetRasterBand(1)\nband.WriteArray(subset)\nband.FlushCache()\ndel src\ndel dst\n```\n\n","tags":["Python","GDAL","裁剪"],"categories":["空间数据处理"]},{"title":"Python中如何优雅地使用switch语句","url":"/python/Python中如何优雅地使用switch语句/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-03-07 13:49:45\n\n# Python中如何优雅地使用switch语句\n\n我们知道Python中没有类似C++或者Java中的`switch...case`语句，我们可以使用多个`if...elif...else`进行模拟，但是这样的写法让代码看起来很凌乱，个人不是很推荐在代码中大量使用`if`语句。\n\n那么解决的办法是什么呢？答曰：字典（`dict`）。下面我们以两个典型案例进行说明。\n\n## 案例一（简单情况）\n\n第一种简单情况就是一对一，给定一个值，返回一个值，这是C++和Java中的`switch`语句支持的情况。\n\n下面的案例是将英文日期翻译为中文日期：\n\n```Python\ndates = {\n    'Sun': '星期天', 'Mon': '星期一', 'Tues': '星期二', 'Wed': '星期三',\n    'Thurs': '星期四', 'Fri': '星期五', 'Sat': '星期六'}\n\nday = dates.get('Fri', '未知')\nprint(day)  # 输出结果为星期五\n```\n\n## 案例二（带条件判断）\n\n第二种情况是多对一，反映在编程上就是`case`语句中带条件判断，这个是诸如Scala中的`switch`和Kotlin中的`when`支持的情况。\n\n下面给出的案例是给定一个数字，如果该数字在某个范围之类，则返回一个指定的数字。\n\n```Python\n# 这里的conditions是一个函数\nconditions = lambda x: {\n    x < -1: 0, -1 <= x <= 1: 0.5, x > 1: 1\n}\n\nnum = conditions(0.25)[True]\nprint(num)\nnum = conditions(10)[True]\nprint(num)\n```\n\n这里我们的`dict`不是一个普通的字典，其`key`是一个`lambda`表达式（一个函数）。如果我们调用该函数，则会返回一个字典，该字典中有两个元素：一个元素的键是`True`，另一个是`False`。`True`元素包含的值是对应`lambda`函数中满足条件的给定值，`False`元素包含的值是对应`lambda`函数中最后一个不满足条件的给定值（这句话写得比较拗口，不好理解。动手实践一下，可以加深理解）。\n\n经过上面的介绍，我们以后可以大大减少对`if...else`语句的使用了，让我们的代码更加干净一些！\n\n","tags":["Python","Switch"],"categories":["Python"]},{"title":"使用卷积网络做手写数字识别","url":"/dl/使用卷积网络做手写数字识别/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-03-02 22:24:22\n\n# 使用卷积网络做手写数字识别\n\n## 思路分析\n\n上篇博文《[使用循环神经网络做手写数字识别](https://theonegis.blog.csdn.net/article/details/88086423)》介绍了利用LSTM做手写数字的识别，想着好事成双，也写一个姊妹篇卷积网络实现手写数字的识别。\n\n博文主要通过最简单的代码量展示一个入门级别的识别案例。需要注意的几点：\n\n- 卷积网络的输入大小为（`batch_size`，`num_channels`，`image_width`，`image_height`）\n- 本文中的模型使用了卷积层和线性连接层。Linear层的输入大小为（`*`，`num_input_feature`），所以在卷积层输出流入线性层的时候，需要转化一下张量的尺寸大小\n- 综合使用`MaxPooling`层和`Dropout`层可以提高识别准确率\n\n## PyTorch实现\n\n```Python\nimport torch\nfrom torch import nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ntorch.manual_seed(2019)\n\n# 超参设置\nEPOCH = 1  # 训练EPOCH次，这里为了测试方便只跑一次\nBATCH_SIZE = 32\nINIT_LR = 1e-3  # 初始学习率\nDOWNLOAD_MNIST = True  # 设置是否需要下载数据集\n\n# 使用DataLoader加载训练数据，为了演示方便，对于测试数据只取出2000个样本进行测试\ntrain_data = datasets.MNIST(root='mnist', train=True, transform=transforms.ToTensor(), download=DOWNLOAD_MNIST)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_data = datasets.MNIST(root='mnist', train=False)\ntest_x = test_data.test_data.type(torch.FloatTensor)[:2000] / 255.\ntest_x.unsqueeze_(1)  # 调整test_x的尺寸为四维，添加了一个channel维度\ntest_y = test_data.test_labels.numpy()[:2000]\n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, 5),  # 图像输出大小为24*24\n            nn.MaxPool2d(2),  # 图像输出大小为12*12\n            nn.ReLU(True),\n            nn.Conv2d(32, 64, 5),  # 图像输出大小为8*8\n            nn.Dropout2d(),\n            nn.MaxPool2d(2),  # 图像输出大小为4*4\n            nn.ReLU(True)\n        )\n\n        self.linear = nn.Sequential(\n            nn.Linear(4 * 4 * 64, 128),\n            nn.ReLU(True),\n            nn.Dropout2d(),\n            nn.Linear(128, 10),\n            nn.Softmax(1)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1, 4 * 4 * 64)\n        out = self.linear(x)\n        return out\n\n\nmodel = ConvNet()\nprint(model)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=INIT_LR)\nloss_func = nn.CrossEntropyLoss()\n\n# RNN训练\nfor epoch in range(EPOCH):\n    for index, (b_x, b_y) in enumerate(train_loader):\n        model.train()\n        # 输入尺寸为(batch_size, channels, height, width)\n        output = model(b_x)  # (64, 1, 28, 28)\n        loss = loss_func(output, b_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if index % 50 == 0:\n            model.eval()\n            prediction = model(test_x)  # 输出为(2000, 10)\n            pred_y = torch.max(prediction, 1)[1].data.numpy()\n            accuracy = (pred_y == test_y).sum() / float(test_y.size)\n            print(f'Epoch: [{index}/{epoch}]', f'| train loss: {loss.item()}', f'| test accuracy: {accuracy}')\n\n# 打印测试数据集中的后20个结果\nmodel.eval()\nprediction = model(test_x[:20])\npred_y = torch.max(prediction, 1)[1].data.numpy()\nprint(pred_y, 'prediction number')\nprint(test_y[:20], 'real number')\n\n```\n\n训练结果如下，可以看到对于这种不太复杂的问题，CNN和RNN都可以得到比较高的精度。\n\n![使用卷积网络做手写数字识别](/images/ml/CNN-MNIST.png)","tags":["PyTorch","卷积神经网络","CNN","手写识别"],"categories":["深度学习"]},{"title":"使用循环神经网络做手写数字识别","url":"/dl/使用循环神经网络做手写数字识别/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-03-02 21:36:12\n\n# 使用循环神经网络做手写数字识别\n\n\n\n## 思路分析\n\n做图像识别的使用卷积神经网络CNN是最好的选择，但是其实我们也可以使用循环神经网络RNN做，只是大部分时候没有卷积网络效果好！下面分析一下如何使用RNN做手写数字的识别。\n\n1. 数据的下载我们可以直接使用PyTorch中的`torchvision.datasets`提供的数据接口\n2. 对于每一张图像（28$\\times$28）我们可以将图像的每一行看做一个样本，然后所有行排列起来做成一个有序序列。对于这个序列，我们就可以使用RNN做识别训练了。\n3. 下面的实现中使用一个LSTM+Linear层组合实现（不要使用经典RNN，效果不好），损失函数使用CrossEntropyLoss。\n4. 在实践中设置`batch_first=True`可以减少一些额外的维度变换和尺寸转换的代码，推荐使用\n\n## PyTorch实现\n\n```Python\nimport torch\nfrom torch import nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ntorch.manual_seed(2019)\n\n# 超参设置\nEPOCH = 1  # 训练EPOCH次，这里为了测试方便只跑一次\nBATCH_SIZE = 32\nTIME_STEP = 28  # RNN时间跨度（图片高度）\nINPUT_SIZE = 28  # RNN输入尺寸（图片宽度）\nINIT_LR = 0.01  # 初始学习率\nDOWNLOAD_MNIST = True  # 设置是否需要下载数据集\n\n# 使用DataLoader加载训练数据，为了演示方便，对于测试数据只取出2000个样本进行测试\ntrain_data = datasets.MNIST(root='mnist', train=True, transform=transforms.ToTensor(), download=DOWNLOAD_MNIST)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_data = datasets.MNIST(root='mnist', train=False)\ntest_x = test_data.test_data.type(torch.FloatTensor)[:2000] / 255.\ntest_y = test_data.test_labels.numpy()[:2000]\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n        self.rnn = nn.LSTM(\n            input_size=INPUT_SIZE,\n            hidden_size=64,\n            num_layers=1,\n            batch_first=True\n        )\n        self.out = nn.Linear(64, 10)\n\n    def forward(self, x):\n        # x shape (batch_size, time_step, input_size)\n        # r_out shape (batch_size, time_step, output_size)\n        # h_n shape (n_layers, batch_size, hidden_size)\n        # h_c shape (n_layers, batch_size, hidden_size)\n        r_out, (h_n, h_c) = self.rnn(x)\n        # 取出最后一次循环的r_out传递到全连接层\n        out = self.out(r_out[:, -1, :])\n        return out\n\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=INIT_LR)\nloss_func = nn.CrossEntropyLoss()\n\n# RNN训练\nfor epoch in range(EPOCH):\n    for step, (b_x, b_y) in enumerate(train_loader):\n        # 数据的输入为(batch_size, time_step, input_size)\n        b_x = b_x.view(-1, TIME_STEP, INPUT_SIZE)\n        output = rnn(b_x)\n        loss = loss_func(output, b_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if step % 50 == 0:\n            prediction = rnn(test_x)  # 输出为(2000, 10)\n            pred_y = torch.max(prediction, 1)[1].data.numpy()\n            accuracy = (pred_y == test_y).sum() / float(test_y.size)\n            print(f'Epoch: [{step}/{epoch}]', f'| train loss: {loss.item()}', f'| test accuracy: {accuracy}')\n\n# 打印测试数据集中的后20个结果\nprediction = rnn(test_x[:20].view(-1, 28, 28))\npred_y = torch.max(prediction, 1)[1].data.numpy()\nprint(pred_y, 'prediction number')\nprint(test_y[:20], 'real number')\n```\n\n下面是训练结果的截图，可以看到效果还不错！\n\n![使用循环神经网络做手写数字识别](/images/ml/LSTM-MNIST.png)","tags":["PyTorch","手写识别","循环神经网络","RNN"],"categories":["深度学习"]},{"title":"通俗LSTM长短时循环神经网络介绍","url":"/dl/通俗LSTM长短时循环神经网络介绍/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n写作时间：2019-03-02 18:20:11\n本文部分图片素材来自互联网，如有侵权，请联系作者删除！\n\n# 通俗LSTM长短时记忆循环神经网络介绍\n\n## LSTM图解\n\n### 处理流程\n\n在上一篇[文章](https://blog.csdn.net/theonegis/article/details/88084305)中简单介绍了经典RNN模型，并提到了RNN的一些缺点。LSTM（Long Short-Term Memory）解决了经典RNN不能很好地保存长时序信息的缺点，得到了更加广泛地应用。下面简单说说LSTM的流程。\n\n![Long Short-Term Memory](/images/ml/Long_Short-Term_Memory.png)\n\n通过对比我们可以发现，LSTM和经典RNN有如下的区别：\n\n- 除了中间状态H，还多了一个C\n- 每个循环网络的单元（Cell）变得复杂了（多了所谓的三道门“遗忘门”（forget gate），“输入门”（input gate）和“输出门”（output gate））\n\n这里所谓的“门”其实就是选择性地对信息进行过滤，在实践中用`sigmoid`函数（在图中用$\\sigma$表示）实现。\n\n首先，$t-1$时刻的输入$h_{t-1}$和$x_t$经过一个线性变换+`sigmoid`激活以后（这就是所谓的遗忘门），输出$f_t$。$f_t$再与$c_{t-1}$进行相乘（element-wise multiplication）得到一个中间结果。\n\n然后，$t-1$时刻的输入$h_{t-1}$和$x_t$经过另外一个线性变换+`sigmoid`激活以后（这就是所谓的输入门），输出$l_t$。同时，$h_{t-1}$和$x_t$经过再另外一个线性变换+`tanh`激活以后），与$l_t$相乘得到一个中间结果。这个中间结果和上一步的中间结果相加（element-wise addition）得到$c_t$。\n\n最后，$t-1$时刻的输入$h_{t-1}$和$x_t$经过另外一个线性变换+`sigmoid`激活以后（这就是所谓的输出门），输出$o_t$。$o_t$与经过`tanh`的$c_t$相乘得到$h_t$。\n\n至此，所有的状态更新完毕。\n\n### 流程图解\n\n下面给出上面文字描述的步骤所对应的数学公式：\n\n![LSTM第一步遗忘门](/images/ml/LSTM3-focus-f.png)\n\n![LSTM第二步输入门](/images/ml/LSTM3-focus-i.png)\n\n![LSTM得到中间状态C](/images/ml/LSTM3-focus-C.png)\n\n![LSTM第三步输出门](/images/ml/LSTM3-focus-o.png)\n\n### 总结说明\n\n![LSTM数据管道](/images/ml/LSTM-Pipeline.png)\n\n上图的左子图给出了对于每个门的输入和输出，右子图说明了每个门的作用。\n\n## PyTorch实战\n\n我们还是以《[最简单的RNN回归模型入门](https://blog.csdn.net/theonegis)》中的使用Sin预测Cos的例子进行演示，代码跟之间的没有太大的区别，唯一的不同就是在中间状态更新的时候，现在有C和H两种中间状态需要更新。\n\n```Python\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(2019)\n\n# 超参设置\nTIME_STEP = 20  # RNN时间步长\nINPUT_SIZE = 1  # RNN输入尺寸\nINIT_LR = 0.02  # 初始学习率\nN_EPOCHS = 100  # 训练回数\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n        self.rnn = nn.LSTM(\n            input_size=INPUT_SIZE,\n            hidden_size=32,  # RNN隐藏神经元个数\n            num_layers=1,  # RNN隐藏层个数\n        )\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x, h):\n        # x (time_step, batch_size, input_size)\n        # h (n_layers, batch, hidden_size)\n        # out (time_step, batch_size, hidden_size)\n        out, h = self.rnn(x, h)\n        prediction = self.out(out)\n        return prediction, h\n\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=INIT_LR)\nloss_func = nn.MSELoss()\nh_state = None  # 初始化隐藏层\n\nplt.figure()\nplt.ion()\nfor step in range(N_EPOCHS):\n    start, end = step * np.pi, (step + 1) * np.pi  # 时间跨度\n    # 使用Sin函数预测Cos函数\n    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)\n    x_np = np.sin(steps)\n    y_np = np.cos(steps)\n    x = torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  # 尺寸大小为(time_step, batch, input_size)\n    y = torch.from_numpy(y_np[:, np.newaxis, np.newaxis])\n\n    prediction, h_state = rnn(x, h_state)  # RNN输出（预测结果，隐藏状态）\n    h_state = (h_state[0].detach(), h_state[1].detach())  # 注意这里和原来的RNN的不同\n    loss = loss_func(prediction, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # 绘制中间结果\n    plt.cla()\n    plt.plot(steps, y_np, 'r-')\n    plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n    plt.draw()\n    plt.pause(0.1)\nplt.ioff()\nplt.show()\n```\n\n当`TIME_STEP`设置为20的时候，输出结果如下：\n\n![LSTM Sin预测Cos](/images/ml/LSTM-Sin-20.png)\n\n## 参考资料\n\n1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n2. [Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)","tags":["循环神经网络","长短时"],"categories":["深度学习"]},{"title":"最简单的RNN回归模型入门(PyTorch)","url":"/dl/最简单的RNN回归模型入门-PyTorch/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n写作时间：2019-03-02 12:46:15\n\n本文部分图片素材来自互联网，如有侵权，请联系作者删除！\n\n# 最简单的RNN回归模型入门（PyTorch版）\n\n## RNN入门介绍\n\n至于RNN的能做什么，擅长什么，这里不赘述。如果不清楚，请先维基一下，那里比我说得更加清楚。\n\n我们首先来来看一张经典的RNN模型示意图！\n\n![Recurrent Neural Network](/images/ml/Recurrent-Neural-Network.png)\n\n图分左右两边：左边给出的RNN是一个抽象的循环结构，右边是左边RNN展开以后的形式。先来看右边的结构，从下往上依次是序列数据的输入X（图中的绿色结构，可以是时间序列，也可以是文本序列等等）。对于t时刻的x经过一个线性变换（U是变换的权重），然后与t-1时刻经过线性变换V的h相加，再经过一个 非线性激活（一般使用tanh或者relu函数）以后，形成一个t时刻的中间状态h，然后再经过一个线性变换（W）输出o ，最后再经过一个非线性激活（可以是sigmoid函数或者softmax等函数）形成最后的输出y。\n\n上面的文字描述，可以形式化表示为下面的公式：\n\n$$a^t = Vh^{t-1} + Ux^t + b \\\\ h^t=tanh(a^t) \\\\ o^t=Wh^t + c\\\\ y^t=sigmoid(o^t)$$\n\n是不是公式能比文字更加说明问题！\n\n再来说左边的结构，坐标的结构表明后面地展开网络中的U，V，W参数都是在共享的，就是说不管我们的序列有多长，都是共享这一套参数的。这是RNN很重要的一个特性。\n\nRNN的隐藏层可以有多层，但是RNN中我们的隐藏层一般不会设置太多，因为在横向上有很长的序列扩展形成的网络，这部分特征是我们更加关注的。最后，需要说明的是RNN可以是单向的，也可以是双向的。\n\n\n## PyTorch中的RNN\n\n下面我们以一个最简单的回归问题使用正弦sin函数预测余弦cos函数，介绍如何使用PyTorch实现RNN模型。\n\n先来看一下PyTorch中[RNN](https://pytorch.org/docs/stable/nn.html#rnn)类的原型：\n\n![torch.nn.RNN](/images/ml/RNNClass.png)\n\n- 必选参数`input_size`指定输入序列中单个样本的大小尺寸，比如在NLP中我们可能用用一个10000个长度的向量表示一个单词，则这个`input_size`就是10000。在咱们的回归案例中，一个序列中包含若干点，而每个点的所代表的函数值（Y）作为一个样本，则咱们案例中的`input_size`为1。这个参数需要根据自己的实际问题确定。\n- 必选参数`hidden_size`指的是隐藏层中输出特征的大小，这个是自定义的超参数。\n- 必选参数`num_layers`指的是纵向的隐藏层的个数，根据实际问题我们一般可以选择1~10层。\n- 可选参数`batch_first`指定是否将`batch_size`作为输入输出张量的第一个维度，如果是，则输入的尺寸为（`batch_size`， `seq_length`，`input_size`），否则，默认的顺序是（`seq_length`，`batch_size`， `input_size`）。\n- 可选参数`bidirectional`指定是否使用双向RNN。\n\n下面再来说说RNN输入输出尺寸的问题，了解了这个可以让我们我们调试代码的时候更加清晰。下面是PyTorch官方的说明：\n\n![RNN的输入输出](/images/ml/RNNInOut.png)\n\n对于RNN的输入包括输入序列和一个初始化的隐藏状态$h_0$。输入序列尺寸默认是（`sequence_length`，`batch_size`， `input_size`），所以如果我们的数据形式不是这样的，则需要手动调整为这种类型的格式。\n\n隐藏状态$h_i$的尺寸是（`num_layers * num_directions`， `batch_size`，` hidden_size`）。单向RNN的`num_directions`为1，双向RNN的`num_directions`为2。\n\n他们的尺寸为什么是这样的呢？这得根据本文开头的那个公式计算，即就是矩阵的相乘需要满足矩阵尺寸的关系，聪明的你想明白了吗？\n\n输出的尺寸为 （`sequence_length`， `batch_size`， `num_directions * hidden_size`）\n\n每一次RNN运行结果输出中还会附带输出中间隐藏状态$h_i$，当然这个尺寸和初始的隐藏状态相同。\n\n\n下面以一个简单的例子说明怎么在程序中查看他们的尺寸：\n\n```Python\nimport torch\nimport torch.nn as nn\n\nrnn = nn.RNN(10, 20, 2)\ninputs = torch.randn(5, 3, 10)  # (time_step, batch_size, input_size)\nh0 = torch.randn(2, 3, 20)  # (num_layers, batch_size, hidden_size)\noutput, hn = rnn(inputs, h0)\nprint(output.shape)  # (time_step, batch_size, hidden_size)\n\nfor name, param in rnn.named_parameters():\n    if param.requires_grad:\n        print(name, param.size())\n\n```\n\n其输出结果如下：\n\n```\ntorch.Size([5, 3, 20])\nweight_ih_l0 torch.Size([20, 10])\nweight_hh_l0 torch.Size([20, 20])\nbias_ih_l0 torch.Size([20])\nbias_hh_l0 torch.Size([20])\nweight_ih_l1 torch.Size([20, 20])\nweight_hh_l1 torch.Size([20, 20])\nbias_ih_l1 torch.Size([20])\nbias_hh_l1 torch.Size([20])\n```\n\n这里的`weight_ih_l0`表示的是RNN隐藏层第一层的权重U，`weight_hh_l0`表示的隐藏层第一层的权重V，类似的`bias`开头的表示偏置或者叫增益（我不知道中文如何翻译），以`l数字`结尾的表示第几层的权重或者偏置。\n\n## 代码实现与结果分析\n\n好了，搞清楚了RNN的基本原理以及PyTorch中RNN类的输入输出参数要求，我们下面实现我们的回归案例。\n\n比较重要的几个超参数是：`TIME_STEP`指定输入序列的长度（一个序列中包含的函数值的个数），`INPUT_SIZE`是1，表示一个序列中的每个样本包含一个函数值。\n\n我们自定义的RNN类包含两个模型：一个`nn.RNN`层，一个`nn.Linear`层，注意`forward`函数的实现，观察每个变量的尺寸（注释中给出了答案）。\n\n```Python\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(2019)\n\n# 超参设置\nTIME_STEP = 10  # RNN时间步长\nINPUT_SIZE = 1  # RNN输入尺寸\nINIT_LR = 0.02  # 初始学习率\nN_EPOCHS = 100  # 训练回数\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n        self.rnn = nn.RNN(\n            input_size=INPUT_SIZE,\n            hidden_size=32,  # RNN隐藏神经元个数\n            num_layers=1,  # RNN隐藏层个数\n        )\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x, h):\n        # x (time_step, batch_size, input_size)\n        # h (n_layers, batch, hidden_size)\n        # out (time_step, batch_size, hidden_size)\n        out, h = self.rnn(x, h)\n        prediction = self.out(out)\n        return prediction, h\n\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=INIT_LR)\nloss_func = nn.MSELoss()\nh_state = None  # 初始化隐藏层\n\nplt.figure()\nplt.ion()\nfor step in range(N_EPOCHS):\n    start, end = step * np.pi, (step + 1) * np.pi  # 时间跨度\n    # 使用Sin函数预测Cos函数\n    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)\n    x_np = np.sin(steps)\n    y_np = np.cos(steps)\n    x = torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  # 尺寸大小为(time_step, batch, input_size)\n    y = torch.from_numpy(y_np[:, np.newaxis, np.newaxis])\n\n    prediction, h_state = rnn(x, h_state)  # RNN输出（预测结果，隐藏状态）\n    h_state = h_state.detach()  # 这一行很重要，将每一次输出的中间状态传递下去(不带梯度)\n    loss = loss_func(prediction, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # 绘制中间结果\n    plt.cla()\n    plt.plot(steps, y_np, 'r-')\n    plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n    plt.draw()\n    plt.pause(0.1)\nplt.ioff()\nplt.show()\n```\n\n最后的结果如下：\n\n![RNN使用Sin预测Cos](/images/ml/RNNSinCos.gif)\n\n最后放一个当`TIME_STEP`分别等于10和20的最终预测结果的对比图：\n\n![RNN TIME_STEP等于10](/images/ml/Time-Step-10.png)\n\n![RNN TIME_STEP=20](/images/ml/Time-Step-20.png)\n\n第一张是`TIME_STEP`=10的预测结果，第二张是`TIME_STEP`=20的预测结果。为什么当`TIME_STEP`=20的预测结果差得十万八千里呢？\n\n这是因为经典的RNN存在梯度爆炸和梯度弥散问题（我尝试修剪了梯度可是结果还是很差，不知道是不是其它原因），对长时序的预测表现很不好，所以才有了后来的LSTM和GRU等RNN变种。实际现在已经很少使用经典RNN了。有时间在说说LSTM吧，欢迎关注！","tags":["PyTorch","RNN"],"categories":["深度学习"]},{"title":"LeetCode-Longest Palindromic Subsequence","url":"/algorithm/LeetCode-Longest-Palindromic-Subsequence/","content":"版权声明：本文为博主原创文章，转载请注明原文出处！\n写作时间：2019-02-10 11:44:52\n\n# Longest Palindromic Subsequence\n\n## 题目描述\n\n这是LeetCode的第516道题目：[516. Longest Palindromic Subsequence](https://leetcode.com/problems/longest-palindromic-subsequence/)。\n\nGiven a string s, find the longest palindromic subsequence's length in s. You may assume that the maximum length of s is 1000.\n\n**Example 1:**\nInput:\n\n```\n\"bbbab\"\n```\n\nOutput:\n\n```\n4\n```\n\nOne possible longest palindromic subsequence is \"bbbb\".\n\n**Example 2:**\nInput:\n\n```\n\"cbbd\"\n```\n\nOutput:\n\n```\n2\n```\n\nOne possible longest palindromic subsequence is \"bb\".\n\n题目要求我们计算出给定字符串中的最长回文序列（这里的序列不是一定要在给定字符串中连续排列的，就是挑出的单个字符按其在给定字符串中的顺序排列以后是回文即可）\n\n## 思路分析\n\n其实，思路跟第647道题目[Palindromic Substrings](https://leetcode.com/problems/palindromic-substrings/)是类似的，可以采用动态规划进行。但是因为回文序列不是给定字符串的连续子串，貌似不能使用中心扩散法求解。\n\n使用动态规划，我们设`dp[i][j]`表示从第`i`个字符到到`j`个字符回文序列的长度，则有：\n\n1. 当`s[i] == s[j]`时，`dp[i][j] = dp[i+1][j-1] + 2`\n2. 当`s[i] != s[j]`时，`dp[i][j] = max(dp[i+1][j], dp[i][j-1])`\n\n## C++实现\n\n```C++\nclass Solution {\npublic:\n    int longestPalindromeSubseq(string s) {\n        const int length = s.length();\n        vector<vector<int>> dp(length, vector<int>(length));\n        for (auto i = length - 1; i >= 0; --i) {\n            dp[i][i] = 1;\n            for (auto j = i + 1; j < length; ++j) {\n                dp[i][j] = s[i] == s[j] ?\n                        dp[i + 1][j - 1] + 2 :\n                        max(dp[i + 1][j], dp[i][j - 1]);\n            }\n        }\n        return dp[0][length - 1];\n    }\n};\n```\n\n## Scala实现\n\nScala版本是对C++版本的翻译\n\n```Scala\nobject Solution {\n  def longestPalindromeSubseq(s: String): Int = {\n    val length = s.length\n    val dp = Array.ofDim[Int](length, length)\n    for (i <- length - 1 to 0 by -1) {\n      dp(i)(i) = 1\n      for (j <- i + 1 until length) {\n        dp(i)(j) = if (s(i) == s(j)) dp(i + 1)(j - 1) + 2\n        else math.max(dp(i + 1)(j), dp(i)(j - 1))\n      }\n    }\n    return dp(0)(length - 1)\n  }\n}\n```\n\n","tags":["LeetCode","动态规划","回文"],"categories":["算法"]},{"title":"Longest Palindromic Substring","url":"/algorithm/Longest-Palindromic-Substring/","content":"\n版权声明：本文为博主原创文章，转载请注明原文出处！\n\n写作时间：2019-02-10 00:04:34\n\n# LeetCode-Longest Palindromic Substring\n\n## 题目描述\n\nLeetCode第5道题目：[5. Longest Palindromic Substring](https://leetcode.com/problems/longest-palindromic-substring/)\n\nGiven a string **s**, find the longest palindromic substring in **s**. You may assume that the maximum length of **s** is 1000.\n\n**Example 1:**\n\n```\nInput: \"babad\"\nOutput: \"bab\"\nNote: \"aba\" is also a valid answer.\n```\n\n**Example 2:**\n\n```\nInput: \"cbbd\"\nOutput: \"bb\"\n```\n\n题目要求我们找到给定字符串中的所有回文子串中最长子串。\n\n## 思路分析\n\n这个题目和之前的[LeetCode-Palindromic Substrings](https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/)题目的思路是一样的，[Palindromic Substrings](https://leetcode.com/problems/palindromic-substrings/)是找回文的个数。在这个过程中其实我们是找出了所有的回文子串，接下来我们统计一下每个回文的长度，选择出最长的那个就是本文的答案了（当然，在代码实现过程中统计最长子串不一定是找到了所有的子串之后再统计，可能是边寻找边统计）。所以，方法还是老方法，可以利用动态规划，也可以利用中心扩散法。\n\n有不明白的地方可以参见我之前的博文《[LeetCode-Palindromic Substrings](https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/)》，这里我只给出了使用中心扩散法进行求解的代码实现。\n\n## C++实现\n\n```C++\nclass Solution {\nprivate:\n    int longest = 0;            // 记录最长子串的字符个数\n    string palindrome = \"\";     // 保存最长的子串\n    void extendPalindrome(const string& s, int left, int right) {\n        while ((left >= 0) && (right < s.size()) && (s[left] == s[right])) {\n            --left;\n            ++right;\n        }\n        int count = right - left - 1;\n        if (count > longest) {\n            longest = count;\n            palindrome = s.substr(left + 1, longest);\n        }\n    }\n\npublic:\n    string longestPalindrome(string s) {\n        for (int i = 0; i < s.size(); ++i) {\n            extendPalindrome(s, i, i);\n            extendPalindrome(s, i, i + 1);\n        }\n        return palindrome;\n    }\n};\n```\n\n可以对比一下，和[LeetCode-Palindromic Substrings](https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/)的答案是不是没多大变化？\n\n## Scala实现\n\n```Scala\nobject Solution {\n  def longestPalindrome(s: String): String = {\n    s.length match {\n      case 0 => s\n      case _ => {\n        val longest = (for {\n          i <- s.indices\n          j <- List(i, i + 1)\n          k <- (i to 0 by -1).zip(j until s.length).takeWhile(p => s(p._1) == s(p._2))\n        } yield (k._1, k._2)).maxBy(p => p._2 - p._1)\n        s.substring(longest._1, longest._2 + 1)\n      }\n    }\n  }\n}\n```\n\n这里需要注意的是：\n\n1. 需要对于空字符串的特殊处理（这里使用的是`match`匹配，当然也可以使用`if...else`条件语句）\n\n2. 我们使用`yield`每次生成回文子串的左右指针，然后再使用`maxBy`得到最长子串对应的左右指针。\n3. `takeWhile`函数对集合进行遍历过程中当条件不满足的时候会立即停止判断，返回的是最后那个满足的元素。（`filter`函数会返回集合中所有满足条件的元素）","tags":["LeetCode","回文"],"categories":["算法"]},{"title":"LeetCode-Palindromic Substrings","url":"/algorithm/LeetCode-Palindromic-Substrings/","content":"\n# LeetCode-Palindromic Substrings\n\n## 题目描述\n\n这是第647道题目：[Palindromic Substrings](https://leetcode.com/problems/palindromic-substrings/)\n\nGiven a string, your task is to count how many palindromic substrings in this string.\n\nThe substrings with different start indexes or end indexes are counted as different substrings even they consist of same characters.\n\n**Example 1:**\n\n```\nInput: \"abc\"\nOutput: 3\nExplanation: Three palindromic strings: \"a\", \"b\", \"c\".\n```\n\n**Example 2:**\n\n```\nInput: \"aaa\"\nOutput: 6\nExplanation: Six palindromic strings: \"a\", \"a\", \"a\", \"aa\", \"aa\", \"aaa\".\n```\n\n题目要求是需要计算出给定字符串中所有回文子串的个数（单个字符也算一个回文子串，不同索引位置的相同内容的回文子串也算不同的回文）\n\n## 思路分析\n\n有两种思路：一种是采用动态规划的方法；另一种是采用中心扩散的方法。\n\n1. 动态规划\n\n   如果用`dp[i][j]`表示从第`i`个字符到第`j`个字符是不是回文子串，`s`表示给定字符串，则有\n\n   `dp[i][j]`= (`s[i]` == `s[j]`) && (`i` - `j` < 2) （这里表示的是子串是一个字符，两个字符的情形）\n\n   `dp[i][j]`= (`s[i]` == `s[j]`) && (`dp[i + 1][j -1]` ) （这里表示的是除了前面两种情形之外的情形）\n\n   注：三个字符串的情形既可以归类到第一种情况（如果归类到第一种情况，则条件需要变为`i` - `j` < 3），也可以归类到第二种情形\n\n2. 中心扩散\n\n   扩散法假定一个中心，然后采用左右两个指针同时向两边走来判断是不是回文。\n\n   注：中心扩散法需要区分回文子串中的字符个数是奇数和偶数两种情况。\n\n## C++实现\n\n1. 动态规划\n\n   ```C++\n   class Solution {\n   public:\n       int countSubstrings(string s) {\n           const int N = static_cast<int>(s.size());   // 如果不强转就会超时，好奇怪\n           int count = 0;\n           // 下面这一行换成原生数组也是可以的int dp[N][N]\n           vector<vector<bool>> dp(N, vector<bool>(N));\n           // 从后面遍历是为了让求dp[i][j]的时候dp[i + 1][j - 1]是已经计算过的\n           for (auto i = N - 1; i >= 0; --i) {\n               for (auto j = i; j < N; ++j) {\n                   // (j - i < 2)包含了两种情况，使得dp[i + 1][j - 1]可以包含剩下的所有情况\n                   dp[i][j] = (s[i] == s[j]) && ((j - i < 2) || dp[i + 1][j - 1]);\n                   if (dp[i][j]) count++;\n               }\n           }\n   \n           return count;\n       }\n   };\n   ```\n\n   在使用C++实现的时候，我发现一些有意思的现象：\n\n   1. 在第四行`s.size()`的返回类型本来是`size_t`，但是如果直接使用`size_t`的话，运行直接超时。我强制转换为`int`以后就可以通过测试。有童鞋能帮我解答一下疑惑吗？🙏\n   2. 用于存储`dp`的使用动态数组`vector`是一般都会想到的，但是我看到一些提交中也有直接使用C++原生数组的。我就奇怪了，C++原生数组的话需要使用`new`操作符去动态申请，为什么直接使用也可以通过编译呢？我后来查了一些资料，原来C99标准中支持了原生动态数组（标准中称之为变成数组variable length array）。但是C++标准中这个特性是可选的，就是说可能有的编译器支持这样写，而有的编译器不行。不过，原生数组相对`vector`容器，效率会更高一些。如果你的编译器支持，大胆地使用吧！\n\n2. 中心扩散\n\n   ```C++\n   class Solution {\n   private:\n       int count = 0;\n       void extendPalindrome(const string& s, int left, int right) {\n           while ((left >= 0) && (right < s.size()) && (s[left] == s[right])) {\n               --left;\n               ++right;\n               ++count;\n           }\n       }\n   \n   public:\n       int countSubstrings(string s) {\n           for (int i = 0; i < s.size(); ++i) {\n               extendPalindrome(s, i, i);      // 对于奇数个字符的回文\n               extendPalindrome(s, i, i + 1);  // 对于偶数个字符的回文\n           }\n           return count;\n       }\n   };\n   ```\n\n## Scala实现\n\nScala的实现是在LeetCode上看到一个大神的答案，使用纯函数实现，写得很美妙，拿过来与大家分享！\n\n```Scala\nobject Solution {\n    def countSubstrings(s: String): Int = {\n        (for {\n            i <- 0 until s.length\n            j <- List(i, i + 1)\n            _ <- (i to 0 by -1).zip(j until s.length).takeWhile(p => s(p._1) == s(p._2))\n        } yield true).length\n    }\n}\n```\n\n这也是采用中心扩散法实现的。\n\n`for`循环中的`i`从左到右依次遍历给定字符串，`j`控制的是奇数个数的子串情况和偶数个数的子串情况，`for`循环中的第三个匿名变量其实相当于一个条件判断。整个`for`循环返回一个`vector`（里面都是`true`），最后统计这个`vector`个中包含元素的个数即可。\n\n这里重点说一下`for`循环中的第三个匿名循环控制语句。`(i to 0 by -1).zip(j until s.length)`生成一个从中间向两边扩散的`List`（其实是`List`的子类`::`非空链表），这个`List`中的每个元素是一个`Tuple2`包含的是左指针`i`和右指针`j`。`takeWhile`方法是起到一个过滤作用，将左指针和右指针指向的值相等的这`Tuple2`返回（其实返回类型是`::`,只是里面只有一个元素）。如果左指针和右指针指向的值不相等，则返回`Nil（一个空的List）`。如果返回的是`Nil`的话，则不会生成一个`true`。这样子，其实第三个循环控制语句起到的是判断的作用。\n\n注：\n\n1. Scala中的`Vector`类似于Java中的`ArrayList`，而Scala中的`List`类似于Java中的`LinkedList`\n2. Scala中的`List`有两个特殊的子类：`::`表示非空的`List`，`Nil`表示空的`List`\n3. 函数`filter`和`takeWhile`都可以起到过滤的作用，`filter`会过滤出给定集合中所有满足条件的元素，而`takeWhile`只会返回第一个满足条件的元素。但是两者返回的都是集合，即使`takeWhile`返回的集合只有一个元素。\n\n感觉函数式编程是挺好玩的，只是现在水平有限，还玩不起来！继续加油！","tags":["LeetCode","动态规划","回文"],"categories":["算法"]},{"title":"LeetCode-Minimum Falling Path Sum","url":"/algorithm/LeetCode-Minimum-Falling-Path-Sum/","content":"\n# Minimum Falling Path Sum\n\n## 题目描述\n\n本题目链接：[931. Minimum Falling Path Sum](https://leetcode.com/problems/minimum-falling-path-sum/)\n\nGiven a **square** array of integers `A`, we want the **minimum** sum of a *falling path*through `A`.\n\nA falling path starts at any element in the first row, and chooses one element from each row.  The next row's choice must be in a column that is different from the previous row's column by at most one.\n\n题目的意思是在一个给定的二维方格中，从上往下走。列方向每次只走一步，行方向上最多只能跨越一个单元格。即就是只能向正下方，左下方，右下方行进。每个方格都有一个值，目标是走到最后一行的路径中包含的值之和最小。\n\n## 问题分析\n\n还是使用动态规划，而动态规划的重中之重就是建立递推关系。\n\n显然，对于第一行，我们选择最小的数进行开始；\n\n然后，对于后面的，我们每次只要选择正下方，左下方，右下方中最小的数即可。\n\n递推公式为：`dp[i][j] = dp[i-1][j] + min(A[i][j-1], A[i][j], A[i][j+1])`（注意对数组越界的处理）\n\n## C++实现\n\n使用`A`当做`dp`数组，这样可以节省空间，但是我觉得对输入参数直接进行了修改，这样不是很好。\n\n```C++\nclass Solution {\npublic:\n    int minFallingPathSum(vector<vector<int>> &A) {\n        for (auto i = 1; i < A.size(); ++i) {\n            for (auto j = 0; j < A.size(); ++j) {\n                A[i][j] +=\n                        min({A[i - 1][max(0, j - 1)],\n                             A[i - 1][j],\n                             A[i - 1][min(static_cast<int>(A.size() - 1), j + 1)]});\n            }\n\n        }\n        return *min_element(A.back().begin(), A.back().end());\n    }\n};\n```\n\n## Scala实现\n\nScala版本的对输入参数`A`保持不变，但是这仍然不是纯函数的实现。如果有朋友有纯函数实现的方案，请不吝赐教！\n\n```Scala\nobject Solution {\n  def minFallingPathSum(A: Array[Array[Int]]): Int = {\n    val dp = A.clone()\n    for (i <- 1 until dp.length; j <- dp.indices) {\n      dp(i)(j) += List(\n        dp(i - 1)(math.max(0, j - 1)),\n        dp(i - 1)(j),\n        dp(i - 1)(math.min(dp.length - 1, j + 1))).min\n    }\n    dp.last.min\n  }\n}\n```\n\n","tags":["LeetCode","动态规划"],"categories":["算法"]},{"title":"LeetCode-Minimum Cost for Tickets","url":"/algorithm/LeetCode-Minimum-Cost-For-Tickets/","content":"\n# Minimum Cost For Tickets\n\n## 题目描述\n\nLeetCode地址：[983. Minimum Cost For Tickets](https://leetcode.com/problems/minimum-cost-for-tickets/)\n\n\nIn a country popular for train travel, you have planned some train travelling one year in advance.  The days of the year that you will travel is given as an array `days`.  Each day is an integer from `1` to `365`.\n\nTrain tickets are sold in 3 different ways:\n\n- a 1-day pass is sold for `costs[0]` dollars;\n- a 7-day pass is sold for `costs[1]` dollars;\n- a 30-day pass is sold for `costs[2]` dollars.\n\nThe passes allow that many days of consecutive travel.  For example, if we get a 7-day pass on day 2, then we can travel for 7 days: day 2, 3, 4, 5, 6, 7, and 8.\n\nReturn the minimum number of dollars you need to travel every day in the given list of `days`.\n\ndays数组中存储的是该年中去旅游的日期（范围为1到365之间的数字），costs数组大小为3，存储的是1天，7天和30天火车票的价格。我们需要做一个方案选择合适的购票方案达到旅游days天最省钱的目的。\n\n## 算法描述\n\n采用动态规划进行解决，假设现在是第days[i]天，我们在该天出行旅游需要选择买票方案，现在我们有三种方案：第一，购买一天的通行票，当天出行，花费就是第days[i-1]天的花费加上一天的通行票价；第二，购买七天的通行票，而七天的通行票可以在连续的七天之内使用，所以花费是第days[i-7]天的花费加上七天的通行票价（即从第days[i-8]天到days[i]天的花费都包含在这七天的通行票中）；第三，购买三十天的通行票，同理，花费是days[i-30]天加上三十天的通行票价。然后我们在这三种方案中选择最实惠的。最后，在实现代码中注意数组越界的问题。\n\n使用dp[j]代表着我们旅行到i天为止需要的最少旅行价格，递推公式为：\n\n1. dp[j] = dp[j-1] （第j天不用旅行）\n2. dp[j] = min(dp[j-1] + costs[0], dp[j-7] + costs[1], dp[j-30] + costs[2]) （第j天需要旅行）\n\n## C++实现\n\n```C++\nclass Solution {\npublic:\n    int mincostTickets(vector<int> &days, vector<int> &costs) {\n        if (days.size() == 0) return 0;\n        assert(costs.size() == 3);\n        // dp[i]代表着我们旅行到i天需要的最少旅行价格, dp[0]为0，没实际含义\n        array<int, 366> dp = {0};\n        for (int i = 1; i < dp.size(); ++i) {\n            // 如果这一天不旅行\n            if (find(days.begin(), days.end(), i) == days.end()) dp[i] = dp[i - 1];\n            else {\n                dp[i] = min({\n                   dp[i - 1] + costs[0],\n                   dp[max(0, i - 7)] + costs[1],\n                   dp[max(0, i - 30)] + costs[2]\n                });\n            }\n        }\n        return dp[365];\n    }\n};\n```\n\n## Scala实现\n\n注：如果有童鞋有纯函数的实现，希望分享出来！共享！\n\n```Scala\nobject Solution {\n  def mincostTickets(days: Array[Int], costs: Array[Int]): Int = {\n    if (days.length == 0) return 0\n    assert(costs.length == 3)\n    val travels = days.toSet\n    val dp = Array.fill[Int](366)(0)\n\n    for (i <- 1 until 366) {\n      if (!travels.contains(i)) dp(i) = dp(i - 1)\n      else dp(i) = List(\n        dp(i - 1) + costs(0),\n        dp(math.max(0, i - 7)) + costs(1),\n        dp(math.max(0, i - 30)) + costs(2)\n      ).min\n    }\n\n    dp(365)\n  }\n}\n```\n\n","tags":["LeetCode","动态规划"],"categories":["算法"]},{"title":"小波变换三之Haar变换","url":"/math/小波变换三之Haar变换/","content":"\n# 小波变换三之Haar变换\n\n## 什么是基（Basis）\n\n数学上有一个常用神秘专有名词“基”，那么什么是“基”呢？举个例子：在平面直角坐标系中的的一个点$(x, y)$的坐标可以表示为$x\\cdot{(1, 0)} + y\\cdot{(0, 1)}$，这里的$(1, 0)$和$(0, 1)​$就是二维直角坐标系中的基，因为任意的点都可以通过这两个向量的加权进行表示。\n\n其实，数学中很多定理或者法则都有这样的表示形式。比如：泰勒公式将任意一个可微函数表示为在该函数在某点的各阶导数的多项式的和；傅里叶级数任何周期函数都可以用正弦函数和余弦函数构成的无穷级数来表示。这些定理都是用无穷项的和来毕竟一个函数，而无穷项中的每一项都是一个系数乘以一个给定的函数，这些函数一起构成了所谓的“基”。\n\n## Haar小波基\n\n其实，小波变换也是有“基”的。我们先直观来看，然后给出形式化的定义。\n\n看例子，对于一个信号$f = \\{4, 6, 10, 12, 8, 6, 5, 5\\}$，我们可以通过在《[小波变换一之Haar变换](https://theonegis.github.io/math/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E4%B8%80%E4%B9%8BHaar%E5%8F%98%E6%8D%A2/)》中讲述的方法计算其第一层的变换结果，我们也可以通过“基”辅助计算。\n\n### 第一层的基\n\n对于第一层的计算，Haar基是这样的：\n\n对于近似表示的基，我们有：$$\\begin{matrix}V_1^1 = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0, \\cdots, 0) \\\\ V_2^1 = (0, 0, \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, \\cdots, 0) \\\\ V_{N/2}^1 = (0, 0, 0, 0, \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}})\\end{matrix}​$$\n\n所以，变换以后的近似系数为$a^1 = (fV_1^1, fV_2^1, \\cdots, fV_{N/2}^1) = (5\\sqrt{2}, 11\\sqrt{2}, 7\\sqrt{2}, 5\\sqrt{2})​$\n\n类似的，对于细节表示的基，我们有：$$\\begin{matrix}W_1^1 = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0, 0, \\cdots, 0) \\\\ W_2^1 = (0, 0, \\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, \\cdots, 0) \\\\ W_{N/2}^1 = (0, 0, 0, 0, \\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})\\end{matrix}​$$\n\n所以，变换以后的细节系数为$d^1 = (fW_1^1, fW_2^1, \\cdots, fW_{N/2}^1) = (-\\sqrt{2}, -\\sqrt{2}, -\\sqrt{2}, 0)$\n\n### 第二层的基\n\n对于第二层的计算（对$a_1$进行小波分解），Haar基是这样的：\n\n对于近似表示的基，我们有：$$\\begin{matrix}V_1^2 = (\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2},0, 0, 0, 0, \\cdots, 0, 0, 0, 0) \\\\ V_2^2 = (0, 0, 0, 0, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\cdots, 0, 0, 0, 0) \\\\ V_{N/4}^2 = (0, 0, 0, 0, 0, 0, 0, 0,  \\cdots, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2})\\end{matrix}​$$\n\n变换以后的近似系数为$a^2 = (fV_1^2, fV_2^2, \\cdots, fV_{N/4}^2) = (16, 12)$\n\n对于细节表示的基，我们有：$$\\begin{matrix}W_1^2 = (\\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, -\\frac{1}{2},0, 0, 0, 0, \\cdots, 0, 0, 0, 0) \\\\ W_2^2 = (0, 0, 0, 0, \\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, -\\frac{1}{2}, \\cdots, 0, 0, 0, 0) \\\\ W_{N/4}^2 = (0, 0, 0, 0, 0, 0, 0, 0,  \\cdots, \\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, -\\frac{1}{2})\\end{matrix}$$\n\n变换以后的细节系数为$d^1 = (fW_1^2, fW_2^2, \\cdots, fW_{N/4}^2) = (-6, 2)​$\n\n后面，如果要继续再分解的话，我们可以找到类似上面的“基”做进一步分解。\n\n可以看到Haar小波基都是正交的（与除了自己以外的其它基的內积为0），而且都经过了单位化（模为1）。\n\n## 母小波和父小波\n\n在小波变换中有两个重要的术语：母小波（mother wavelet）和父小波（father wavelet），而我们的小波基就是由父小波和母小波经过平移和缩放得到的。母小波也叫做小波函数（wavelet function），对应着细节系数的基，父小波也叫做缩放函数（scaling function），对应着近似系数的基。\n\nHaar小波的母小波定义为$$\\psi(x) = \\begin{cases}1, & 0 \\le x \\lt \\frac{1}{2} \\\\-1, & \\frac{1}{2}\\le x \\lt 1\\\\ 0, & \\mathrm{其它}\\end{cases}​$$\n\nHaar小波的父小波定义为$$\\phi(x) = \\begin{cases}1, & 0 \\le x \\le 1\\\\ 0, & \\mathrm{其它}\\end{cases}$$\n\n不止对于Haar小波，任何小波的基都是对其母小波和父小波缩放和平移后的集合。感兴趣的朋友可以在下面的网址中查看一下，如何对小波函数进行缩放和平移：[Haar Functions](http://demonstrations.wolfram.com/HaarFunctions/)","tags":["小波变换","Haar变换","Wavelet"],"categories":["数学"]},{"title":"C++中的万能引用和完美转发","url":"/cxx/C-中的万能引用和完美转发/","content":"\n# C++中的万能引用和完美转发\n\n1. 阅读这篇博文需要了解C++中的左值（lvalue）和右值（rvalue）的概念，详情参见我的另外一篇博文：[C++移动语义及拷贝优化](https://theonegis.github.io/cxx/C-%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89%E5%8F%8A%E6%8B%B7%E8%B4%9D%E4%BC%98%E5%8C%96/)\n2. 万能引用和完美转发多涉及到模板的使用，如若不是自己写模板，则可不用关心\n\n## 万能引用（Universal Reference）\n\n首先，我们来看一个例子：\n\n```C++\n#include <iostream>\n\nusing std::cout;\nusing std::endl;\n\n\ntemplate<typename T>\nvoid func(T& param) {\n    cout << param << endl;\n}\n\n\nint main() {\n    int num = 2019;\n    func(num);\n    return 0;\n}\n```\n\n这样例子的编译输出都没有什么问题，但是如果我们修改成下面的调用方式呢？\n\n```C++\nint main() {\n    func(2019);\n    return 0;\n}\n```\n\n则会得到一个大大的编译错误。因为上面的模板函数只能接受左值或者左值引用（左值一般是有名字的变量，可以取到地址的），我们当然可以重载一个接受右值的模板函数，如下也可以达到效果。\n\n```C++\ntemplate<typename T>\nvoid func(T& param) {\n    cout << \"传入的是左值\" << endl;\n}\ntemplate<typename T>\nvoid func(T&& param) {\n    cout << \"传入的是右值\" << endl;\n}\n\n\n\nint main() {\n    int num = 2019;\n    func(num);\n    func(2019);\n    return 0;\n}\n```\n\n输出结果：\n\n```\n传入的是左值\n传入的是右值\n```\n\n第一次函数调用的是左值得版本，第二次函数调用的是右值版本。但是，有没有办法只写一个模板函数即可以接收左值又可以接收右值呢？\n\nC++ 11中有万能引用（Universal Reference）的概念：使用`T&&`类型的形参既能绑定右值，又能绑定左值。\n\n但是注意了：**只有发生类型推导的时候，T&&才表示万能引用**；否则，表示右值引用。\n\n所以，上面的案例我们可以修改为：\n\n```C++\ntemplate<typename T>\nvoid func(T&& param) {\n    cout << param << endl;\n}\n\n\nint main() {\n    int num = 2019;\n    func(num);\n    func(2019);\n    return 0;\n}\n```\n\n## 引用折叠（Universal Collapse）\n\n万能引用说完了，接着来聊引用折叠（Univers Collapse），因为完美转发（Perfect Forwarding）的概念涉及引用折叠。一个模板函数，根据定义的形参和传入的实参的类型，我们可以有下面四中组合：\n\n- 左值-左值 T& &        # 函数定义的形参类型是左值引用，传入的实参是左值引用\n- 左值-右值 T& &&      # 函数定义的形参类型是左值引用，传入的实参是右值引用\n- 右值-左值 T&& &      # 函数定义的形参类型是右值引用，传入的实参是左值引用\n- 右值-右值 T&& &&    # 函数定义的形参类型是右值引用，传入的实参是右值引用\n\n但是C++中不允许对引用再进行引用，对于上述情况的处理有如下的规则：\n\n所有的折叠引用最终都代表一个引用，要么是左值引用，要么是右值引用。规则是：**如果任一引用为左值引用，则结果为左值引用。否则（即两个都是右值引用），结果为右值引用**。\n\n即就是前面三种情况代表的都是左值引用，而第四种代表的右值引用。\n\n## 完美转发（Perfect Forwarding）\n\n下面接着说完美转发（Perfect Forwarding），首先，看一个例子：\n\n```C++\n#include <iostream>\n\nusing std::cout;\nusing std::endl;\n\ntemplate<typename T>\nvoid func(T& param) {\n    cout << \"传入的是左值\" << endl;\n}\ntemplate<typename T>\nvoid func(T&& param) {\n    cout << \"传入的是右值\" << endl;\n}\n\n\ntemplate<typename T>\nvoid warp(T&& param) {\n    func(param);\n}\n\n\nint main() {\n    int num = 2019;\n    warp(num);\n    warp(2019);\n    return 0;\n}\n```\n\n猜一下，上面的输出结果是什么？\n\n```\n传入的是左值\n传入的是左值\n```\n\n是不是和我们预期的不一样，下面我们来分析一下原因：\n\n`warp()`函数本身的形参是一个万能引用，即可以接受左值又可以接受右值；第一个`warp()`函数调用实参是左值，所以，`warp()`函数中调用`func()`中传入的参数也应该是左值；第二个`warp()`函数调用实参是右值，根据上面所说的引用折叠规则，warp()`函数接收的参数类型是右值引用，那么为什么却调用了调用`func()的左值版本了呢？这是因为在`warp()`函数内部，左值引用类型变为了右值，因为参数有了名称，我们也通过变量名取得变量地址。\n\n那么问题来了，怎么保持函数调用过程中，变量类型的不变呢？这就是我们所谓的“完美转发”技术，在C++11中通过`std::forward()`函数来实现。我们修改我们的`warp()`函数如下：\n\n```C++\ntemplate<typename T>\nvoid warp(T&& param) {\n    func(std::forward<T>(param));\n}\n```\n\n则可以输出预期的结果。\n\n","tags":["C++11","完美转发","万能引用","引用折叠"],"categories":["C++"]},{"title":"小波变换二之Haar变换","url":"/math/小波变换二之Haar变换/","content":"\n# Haar变换\n\n这是小波变换的第二篇，我们继续谈Haar变换。在第一篇中，我们介绍了一位情况下的Haar变换，这篇博文中主要介绍二维Haar变换。最后，通过一个图像压缩的案例说明二维Haar变换的应用。\n\n## 原理说明\n\n给定一个二维信号，我们这里假设是一个$4\\times4$的图片，\n\n$f=\\begin{bmatrix}2&1&5&6\\\\7&6&5&8\\\\2&1&5&5\\\\7&7&2&10\\end{bmatrix}$\n\n如何进行二维的哈尔变换呢？\n\n步骤是这样的：（1）首先，沿着矩阵的每一行做一维的Haar变换；（2）然后，沿着矩阵的每一列做一维的哈尔变换；（3）对于每个低频分量矩阵（近似信息）重复步骤（1）和（2）直到完成指定的等级划分。下图给出了两级划分的示意图：\n\n![二维Haar变换示意图](/images/math/Haar2D.png)\n\n这里的A表示近似信息（approximation coefficients），H表示水平细节信息（horizontal detail coefficients），V表示垂直细节信息（vertical detail coefficients），D表示对角线细节信息（diagonal detail coefficients）。很多数学软件中是这样称呼的，了解了这个可以帮助我们快速上手软件进行实际操作。\n\n行分解和列分解的顺序是可以互换的，保持一致即可。\n\n明白了基本原理，下面我们来进行实际计算，对于$f$，（如果不清楚如何做一维高频和低频分解，可参看博文[《小波变换一之Haar变换》](https://blog.csdn.net/theonegis/article/details/86517377)）\n\n第一次行分解得到低频信息$L=\\begin{bmatrix}\\frac{3}{\\sqrt{2}}&\\frac{11}{\\sqrt{2}}\\\\\\frac{13}{\\sqrt{2}}&\\frac{13}{\\sqrt{2}}\\\\\\frac{3}{\\sqrt{2}}&5\\sqrt{2}\\\\7\\sqrt{2}&6\\sqrt{2}\\end{bmatrix}$\n\n第一次列分解得到高频信息$H=\\begin{bmatrix}\\frac{1}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}&-\\frac{3}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}&0\\\\0&-4\\sqrt{2}\\end{bmatrix}$\n\n对$L$进行列高频分解得到$A_1=\\begin{bmatrix}8&12\\\\8.5&11\\end{bmatrix}$\n\n对$L$进行列低频分解得到$H_1=\\begin{bmatrix}-5&-1\\\\-5.5&-1\\end{bmatrix}$\n\n对$H$进行列高频分解得到$V_1=\\begin{bmatrix}1&-2\\\\0.5&-4\\end{bmatrix}$\n\n对$H$进行列低频分解得到$D_1=\\begin{bmatrix}0&1\\\\0.5&4\\end{bmatrix}​$\n\n我们还可以对$A_1​$继续进行二层分解，这里就不做演示了。\n\n## 实例演示\n\n这里我们通过对一张图片做Haar变换，然后我们去掉其高频信息部分，实现对图像的压缩。\n\n下面是进行了三次分解，然后分别过了到第一层的高频信息和第一层兼第二层的高频信息的效果！过滤掉第一层的高频信息，图像压缩为原来的四分之一，可以看到图像还是基本清晰的。过滤掉第二层和第二层的高频信息以后，可以看到图片稍微有点模糊了。\n\n![Haar变换实现图像压缩](/images/math/Haar-Compress.jpg)\n\n## MATLAB实现\n\n下面是使用MATLAB实现上面变换的代码，有兴趣的童鞋可以参考一下。\n\n```matlab\nclear, clc;\n\n% 读取原始图像\nX = rgb2gray(imread('http://www.lenna.org/lena_std.tif'));\n% 进行小波分解\n[C, S] = wavedec2(X, 3, 'haar');\n\n% 获得分解以后的低频近似信息\nL = appcoef2(C, S, 'haar', 3);\n% 分别获得各层级的高频细节信息\n[H3, V3, D3] = detcoef2('all', C, S, 3);\n[H2, V2, D2] = detcoef2('all', C, S, 2);\n[H1, V1, D1] = detcoef2('all', C, S, 1);\n\n% 去掉第一层的高频信息（替换成0），然后进行小波重建\n% 注意这里乘以3是有HVD三种高频信息\nD = [C(1: end - 3*size(H1, 1)*size(H1, 2)), zeros(1, 3*size(H1, 1)*size(H1, 2))];\nCD1 = waverec2(D, S, 'haar');\n% 去掉第一和第二层的高频信息，然后进行小波重建\nD = [C(1: end - 3*size(H1, 1)*size(H1, 2) - 3*size(H2, 1)*size(H2, 2)), ...\n    zeros(1, 3*size(H1, 1)*size(H1, 2) + 3*size(H2, 1)*size(H2, 2))];\nCD2 = waverec2(D, S, 'haar');\n\n%按照分解层级将分解系数排列拼接为一副图像\nDD1 = [L, H3; V3, D3];\nDD2 = [DD1, H2; V2, D2];\nDD3 = [DD2, H1; V1, D1];\n% 结果显示\nsubplot(2, 2, 1), imshow(X, []), title('原始图像');\nsubplot(2, 2, 2), imshow(DD3, []), title('小波分解系数');\nsubplot(2, 2, 3), imshow(CD1, []), title('压缩一（去掉第一层高频信息）');\nsubplot(2, 2, 4), imshow(CD2, []), title('压缩二（去掉第二层高频信息）');\n```\n\n","tags":["小波变换","Haar变换","图像压缩"],"categories":["数学"]},{"title":"小波变换一之Haar变换","url":"/math/小波变换一之Haar变换/","content":"\n注：\n\n- 小波变换系列博文打算记录自己学习小波变换的心路历程，每篇博文尽量简短，宗旨是用最少的数学公式说明白如何使用小波变换\n- 我的博客即将同步至腾讯云+社区，邀请大家一同入驻：[https://cloud.tencent.com/developer/support-plan?invite_code=1roiym8d609t1](https://cloud.tencent.com/developer/support-plan?invite_code=1roiym8d609t1)\n\n# Haar变换\n\n## 案例一简单一维信号变换\n\n下面是一个一维信号（一组数）：$f = \\{2, 2, 2, 4, 4, 4\\}$\n\n我对这个信号进行如下处理：\n\n$a_m = \\sqrt{2}\\frac{f_{2m-1}+f_{2m}}{2} = \\frac{f_{2m-1}+f_{2m}}{\\sqrt{2}}$（相邻两个数相加，求平均，然后乘以$\\sqrt{2}$）\n\n$d_m = \\sqrt{2}\\frac{f_{2m-1}-f_{2m}}{2} = \\frac{f_{2m-1}-f_{2m}}{\\sqrt{2}}$（相邻两个数相减，求平均，然后乘以$\\sqrt{2}$）\n\n注：至于为什么要乘以$\\sqrt{2}$呢？我们这里先不解释，放到后面再说。\n\n然后按照先$a$后$d$的顺序排列${a_1,a_2,...,a_{N/2}, d_1, d_2, ..., d_{N/2}}$（$N$是离散信号中的值的个数）\n\n则，$a = \\{2\\sqrt{2}, 3\\sqrt{2}, 4\\sqrt{2}\\}$，$d=\\{0, -\\sqrt{2}, 0\\}$\n\n我们可以得到结果：$tf = \\{2\\sqrt{2}, 3\\sqrt{2}, 4\\sqrt{2}, 0, -\\sqrt{2}, 0\\}$\n\n这就是传说中的Haar变换了……\n\n$a$表示的是信号的趋势（trend），近似（approximation），是低频信息；而$d$表示的是信号的细节（detail），是高频信息。\n\n那么我们怎么变回去呢？我们对变换以后的信号进行如下处理：\n\n$f_{2m-1} = \\sqrt{2}\\frac{a_m +d_m}{2} = \\frac{a_m +d_m}{\\sqrt{2}}$（第$m$个$a$和$d$相加，求平均，然后乘以$\\sqrt{2}$）\n\n$f_{2m} = \\sqrt{2}\\frac{a_m -d_m}{2} = \\frac{a_m -d_m}{\\sqrt{2}}$ （第$m$个$a$和$d$相减，求平均，然后乘以$\\sqrt{2}$）\n\n我们可以得到结果$if = \\{2, 2, 2, 4, 4, 4\\}$\n\n这样就是Haar变换的逆变换。\n\n通过观察，我们可以发现：\n\n- $d$中的数字绝大部分都很小（这是做信息压缩很重要的依据）\n- 变换前后信号的能量保持不变，即$\\sum{f_i^2} = \\sum{a_m^2} + \\sum{d_i^2}$（有兴趣的同学可以算一下对于$f$和$tf$的能量都是60，刚好相等）\n\n## 案例二多分辨率一维信号变换\n\n我们可以按照上面的思路将信号对得到的低频信号（$a$）一直一直划分下去，直到$\\mathrm{log}_2N$（离散信号的值的数目不是偶数的，可以在后面补0）\n\n给定如下的一个信号：$f(t) = 20x^2(1-x)^4\\cos(12\\pi x)$\n\n我们通过在[0, 1]之间取样1024个点可以得到信号的振幅，绘制出信号图像如下：\n\n![原始信号](/images/math/原始信号.png)\n\n我们可以通过案例一种描述的方法进行Haar变换，我们这里对$f(t)$信号进行两次Haar变换，如下图所示：\n\n![Haar多分辨率分析](/images/math/Haar多分辨率.png)\n\n这是多分辨率分析（Multi-Resolution Analysis，MRA）以及图像压缩（JPEG2000编码）等的基础理念，这里现有一个大概理解，后面我们会继续谈到。\n\n变换的结果如下（感兴趣的朋友可以使用Mathematica或者MATLAB是一样，这两个数学软件都提供了对Haar变换的直接支持）：\n\n![Haar变换](/images/math/Haar变换.png)\n\n好了，这一节先到这里，我们以后有时间慢慢聊！","tags":["小波变换","Haar变换"],"categories":["数学"]},{"title":"变分法入门介绍","url":"/math/变分法入门介绍/","content":"\n# 变分法入门介绍\n\n读完这篇博文你可以了解变分的基本概念，以及使用变分法求解最简泛函的极值。本文没有严密的数学证明，只是感性地对变分法做一个初步了解。\n\n## 泛函和变分法\n\n给定两点$A(x_0, y_0)$和$B(x_1, y_1)$，求AB两点之间的最短距离。两点之间直线最短，这还用球吗？可是为什么是直线最短呢，而不是其它曲线？\n\n设链接AB两点的曲线为$f(x)$,则AB之间的距离可以表示为在区间$[x_0, x_1]$上求$\\Delta{S}=\\sqrt{(\\Delta{x})^2 + (\\Delta{y})^2}$线段的累积长度（积分的思想）：\n\n$$S=\\int_{x_0}^{x_1}\\sqrt{1+f'(x)^2}dx$$\n\n在这里该函数的变量是$f$，即函数的变量为函数，我们需要求解出合适的$f$使得$S$最小。我们把这样的函数$S$称为泛函数。\n\n定义：**泛函是以函数为变量的函数。**\n\n那么什么是变分法呢？**求泛函极值的方法称为变分法。**\n\n## 变分法求泛函极值\n\n### 变分的定义\n\n下面给出变分的定义：对于任意定值$x\\in [x_0, x_1]$，可取函数$y(x)$与另一可取函数$y_0(x)$之差$y(x) - y_0(x)$称为函数$y(x)$在$y_0(x)$处的变分或函数的变分，记做$\\delta{y}$，这时有$\\delta{y}=y(x) - y_0(x)=\\epsilon\\eta(x)$，$\\epsilon$是一个很小的数，$\\eta(x)$是$x$的任意参数\n\n对于泛函$J[y(x)]$的增量$\\Delta{J}=J[y(x)+\\delta{y}] - J[y(x)] = \\delta{J} + \\mathcal{o}(\\delta{y})$\n\n泛函的增量$\\Delta{J}$与变分$\\delta{J}$之差是一个比一阶距离更高阶的无穷小，泛函的变分是泛函增量的线性主要部分。\n\n变分的定义是不是跟微分很像（微分的定义$\\Delta{y}=A\\Delta{x}+\\mathcal{o}(\\Delta{x})=dy+\\mathcal{o}(\\Delta(x)$，$A$是该点的导数）。类比一下，我们在高等数学中学习到的函数极值的必要条件是函数导数等于0，而泛函极值的必要条件也是泛函的变分等于0。\n\n所以有如下定理：若泛函$J[y(x)]$在$y=y(x)$上达到极值，则它在$y=y(x)$上的变分$\\delta{J}$等于零。这就是变分原理。\n\n### 拉格朗日函数\n\n设$F(x, y(x), y'(x))$是三个独立变量$x$，$y(x)$，$y'(x)$在区间$[x_0, x_1]$上的已知函数，且二阶连续可微，其中$y(x)$和$y'(x)$是$x$的未知函数，则泛函\n\n$$J[y(x)]=\\int_{x_0}^{x_1}F(x, y(x), y'(x))dx$$\n\n称为最简单的积分形泛函，简称最简泛函，被积函数$F$称为拉格朗日函数。\n\n对于拉格朗日函数，其泛函的变分为\n\n$$\\delta{J} = \\int_{x_0}^{x_1}(F_y\\delta{y} +F_{y'}\\delta{y'})dx = \\int_{x_0}^{x_1}(F_y\\delta{y})dx + (F_{y'}\\delta_{y}|_{x_0}^{x_1} - \\int_{x_0}^{x_1}(\\delta_{y}\\frac{d}{dx}F_{y'}d{x})=\\int_{x_0}^{x_1}(F_y-\\frac{d}{dx}F_{y'})\\delta{y}dx$$\n\n### 欧拉方程\n\n利用变分原理，使最简泛函$J[y(x)]=\\int_{x_0}^{x_1}F(x, y(x), y'(x))dx$取得极值且满足固定边界条件$y(x_0)=y_0$，$y(x_1)=y_1$的极值曲线$y=y(x)$应满足必要条件\n\n$$F_y-\\frac{d}{dx}F_{y'}=0$$\n\n式中$F$是$x, y, y'$的已知函数并有二阶连续偏导数。上述必要条件中的方程叫做泛函的欧拉方程，也叫欧拉-拉格朗日方程。而$F_y-\\frac{d}{dx}F_{y'}$称为$F$关于$y$的变分导（函）数。\n\n## 案例分析--两点之间直线最短\n\n好的，我们利用欧拉方程来证明博文刚开始提出的两点之间直线最短的问题。\n\n这里的$F=\\sqrt{1+f'(x)^2}$，求得$F_y=0$，$F_{y'}=\\frac{y'}{\\sqrt{1+{y'}^2}}$，再求得$\\frac{d}{dx}F_{y'}=y''(1+{y'}^2)^{-\\frac{3}{2}}$\n\n根据欧拉方程有$-y''(1+{y'}^2)^{-\\frac{3}{2}}=0$，则$y''=0 \\Rightarrow y'=C \\Rightarrow y=C_1x + C_2$\n\n此时，我们就得到了这条曲线确实就是连接两点的直线。\n\n## 在Mathematica中使用变分法\n\n鉴于本人计算能力超级差，手动求导对我来说实在太痛苦了，我将上述的计算借助于Mathematica计算了一遍，下面是计算过程。不得不说Mathematica真的太强大了。\n\n![Mathematica变分法](/images/math/Mathematica-Variational.png)\n\n## 参考文献\n\n老大中. 变分法基础[M]. 北京: 国防工业出版社. 2004.","tags":["变分法","泛函"],"categories":["数学"]},{"title":"相关系数r和决定系数R2的那些事","url":"/math/相关系数r和决定系数R2的那些事/","content":"\n# 相关系数$r$和决定系数$R^2$的那些事\n\n有人说相关系数（correlation coefficient，$r$）和决定系数（coefficient of determination，$R^2$，读作R-Squared）都是评价两个变量相关性的指标，且相关系数的平方就是决定系数？这种说法对不对呢？请听下文分解！\n\n## 协方差与相关系数\n\n要说相关系数，我们先来聊聊协方差。在之前的博文《[使用Python计算方差协方差相关系数](https://blog.csdn.net/theonegis/article/details/85059105)》中提到协方差是计算两个随机变量$X$和$Y$ 之间的相关性的指标，定义如下：\n\n$$\\mathrm{Cov}(X, Y) = \\mathrm{E}[(X - \\mathrm{E}X)(Y - \\mathrm{E}Y)]$$\n\n但是协方差有一个确定：它的值会随着变量量纲的变化而变化（covariance is not scale invariant），所以，这才提出了相关系数的概念：\n\n$$r = \\mathrm{Corr}(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\cdot \\sigma_Y} = \\frac{\\mathrm{E}[(X - \\mathrm{E}X)(Y - \\mathrm{E}Y)]}{\\sqrt{\\mathrm{E}[X - \\mathrm{E}X]^2}\\sqrt{\\mathrm{E}[Y - \\mathrm{E}Y]^2}}$$\n\n对于相关系数，我们需要注意：\n\n1. 相关系数是用于描述两个变量*线性*相关程度的，如果$r \\gt 0$，呈正相关；如果$r = 0$，不相关；如果$r \\lt 0$，呈负相关。\n2. 如果我们将$X - \\mathrm{E}X$和$Y - \\mathrm{E}Y$看成两个向量的话，那$r$刚好表示的是这两个向量夹角的余弦值，这也就解释了为什么$r$的值域是[-1, 1]。\n3. 相关系数对变量的平移和缩放（线性变换）保持不变（Correlation is invariant to scaling and shift，不知道中文该如何准确表达，😅）。比如$\\mathrm{Corr}(X, Y) = \\mathrm{Corr}(aX + b, Y)$恒成立。\n\n## 决定系数（R方）\n\n下面来说决定系数，R方一般用在回归模型用用于评估预测值和实际值的符合程度，R方的定义如下：\n\n$$R^2 = 1 - \\mathrm{FVU} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\sum\\limits_i(y_i - f_i)^2}{\\sum\\limits_i(y_i - \\hat{y})^2}$$\n\n上式中$y$是实际值，$f$是预测值，$\\hat{y}$是实际值的平均值。$\\mathrm{FVU}$被称为fraction of variance unexplained，RSS叫做Residual sum of squares，TSS叫做Total sum of squares。根据$R^2$的定义，可以看到$R^2$是有可能小于0的，所以$R2$不是$r$的平方。一般地，$R^2$越接近1，表示回归分析中自变量对因变量的解释越好。\n\n对于$R^2$可以通俗地理解为使用均值作为误差基准，看预测误差是否大于或者小于均值基准误差。\n\n此外，我们做这样一个变形：$R^2 = 1 - \\frac{\\sum\\limits_i(y_i - f_i)^2 / n}{\\sum\\limits_i(y_i - \\hat{y})^2 / n} = 1 - \\frac{\\mathrm{RMSE}}{\\mathrm{Var}}$，可以看到变成了1减去均方根误差和方差的比值（有利于编程实现）。\n\n另外有一个叫做Explained sum of squares，$\\mathrm{ESS} = \\sum\\limits_i(f_i - \\hat{y})^2$\n\n在一般地线性回归模型中，有$\\mathrm{ESS} + \\mathrm{RSS} = \\mathrm{TSS}$（证明过程参见：[Partitioning in the general ordinary least squares model](https://en.wikipedia.org/wiki/Explained_sum_of_squares#Partitioning_in_the_general_ordinary_least_squares_model)）\n\n在这种情况下：我们有$R^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}  = \\frac{\\mathrm{ESS}}{\\mathrm{TSS}}  = \\frac{\\sum\\limits_i(f_i - \\hat{y})^2}{\\sum\\limits_i(y_i - \\hat{y})^2}$\n\n对于$R^2$我们需要注意：\n\n1. $R^2$一般用在线性模型中（虽然非线性模型总也可以用），具体参见：[Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)\n\n2. $R^2$不能完全反映模型预测能力的高低\n\n最后，这篇文章《[8 Tips for Interpreting R-Squared](https://www.displayr.com/8-tips-for-interpreting-r-squared/)》里面指出了不错误解读$R^2$的地方，读完之后，我觉得以后还是少用$R^2$，对于模型的评估可以选择其它一些更适合的指标。\n\n## 参考资料\n\n[1]. [The relationship between correlation and the coefficient of determination](http://danshiebler.com/2017-06-25-metrics/)\n\n[2]. [Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n\n[3]. [Explained sum of squares](https://en.wikipedia.org/wiki/Explained_sum_of_squares)\n\n[4]. [Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)\n\n[5]. [8 Tips for Interpreting R-Squared](https://www.displayr.com/8-tips-for-interpreting-r-squared/)","tags":["相关系数","决定系数"],"categories":["数学"]},{"title":"假设检验和P值那些事","url":"/math/假设检验和P值那些事/","content":"\n# 假设检验和P值那些事\n\n记得大学时候学习概率论与数理统计的时候，学习过假设检验，但我不记得课本上有提到过P值。后来翻阅了一些资料，大概弄明白了它们之间的关系，本文旨在以浅显易懂的语言描述严密的数学知识。\n\n## 假设检验\n\n在《Head First Statistics》一书中，作者给假设检验的定义是“Hypothesis tests give you a way of using samples to test whether or not statistical claims are likely to be true”。其实定义不重要，重要的是我们需要知道假设检验能做什么：以概率统计的视角判别一个统计假说是否成立。\n\n下面举一个烂大街的例子：我有一枚专门用于玩抛硬币猜正反面的游戏的硬币，我需要判断这枚硬币是否是正常的（抛硬币游戏中出现正反面的概率相等）。所以我做了一个假说：该枚硬币是正常的，即抛硬币游戏中出现正面的概率为0.5。\n\n那我现在需要做实验去验证我说的对不对。我抛了20次，正面朝上11次，背面朝上9次（设正面朝上记为1，反面朝上记为0）。基于这个实验结果，我应该做怎样的判断呢？\n\n根据假设检验的一般步骤：\n\n1. 建立假设\n\n2. 寻找检验统计量\n\n3. 确定显著性水平和拒绝域\n\n4. 做出判断\n\n第一步中我们的原假设$H_0$（null hypothesis）为该枚硬币是正常的，备择假设$H_1$（alternate hypothesis）为该硬币不正常。\n\n注：当原假设正确，而由于样本的随机性使得样本观测值落在拒绝域（critical region或rejection region）而拒绝原假设产生的错误称为第一类错误；当原假设错误，而样本观测值落在接受域而接受原假设产生的错误称为第二类错误。\n\n第二步中根据中心极限定理可知随机变量$\\bar{X}$服从正态分布。这里我们的检验统计量选择$t=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}$（这里的$t$服从自由度为$n-1$的$t$分布），所以我们使用$t$分布来估计投掷的均值（这里$\\bar{X}$为样本均值，$\\mu_0$为原假设中的均值（期望），$S$为样本标准差，$n$为样本个数）。\n\n$$t=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}} = \\frac{0.55 - 0.5}{0.5104178 / \\sqrt{20}} = 0.4380858$$\n\n注：$t$分布用于根据小样本来估计呈正态分布且方差未知的总体的均值，称为$t$检验。如果总体方差已知（例如在样本数量足够多时），则应该用正态分布来估计总体均值，称为$U$检验。\n\n第三步中显著性水平$\\alpha$（significance level，拒绝原假设时概率阈值）我们一般采用0.05（当然，你也可以使用0.1或者其它）。这个0.05的意思是观测值落在拒绝域的概率为0.05，概率为0.05说明这是小概率事件，而在一次测试中发生了小概率事件，所以我们有足够的理由拒绝原假设。\n\n接下来我们应该计算拒绝域了。对于$t$分布求0.025和0.975的分位数分别为-2.093024和2.093024（即$t$的上下界，左右两边各是0.025，合起来就是0.05的拒绝域），我们可以反推出$\\bar{X}$的上下界为0.3111171和0.7888829（这个区间就是接受域）。\n\n注：对于拒绝域来说，有单边和双边情况，我们这里显然是双边的情况。\n\n第四步做出判断，我们实验的结果的均值是0.4380858，我们在0.05的显著性水平下得到的接受域是$(0.3111171, 0.7888829)$，实验结果落在接受域，所以我们不能拒绝原假设$H_0$。这里的不能拒绝指的是我们没有足够的理由推翻原假设，但是这并不代表原假设一定正确。\n\n## P值\n\n上面讲了检验假设的一般过程，好像跟P值没什么关系？但是P值其实和检验假设息息相关的。上面的求解过程是通过判断样本观测值是否落在拒绝域而做出判断的，其实我们还可以通过计算P值直接进行判断。\n\n那么什么是$P$值呢？《Head First Statistics》给出的定义是“A p-value is the probability of getting the results in the sample, or something more extreme, in the direction of the critical region.”。我的理解就是P值是在原假设成立的情况下，出现比当前样本观测值更极端（包括当前样本观测值）情况的概率。\n\n其实这样说还是挺抽象的，我们通过计算来进行说明。\n\n我们把检验假设步骤中的第三步修改为：确定显著性水平和计算P值\n\n在我们的实验中$t=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}} = \\frac{0.55 - 0.5}{0.5104178 / \\sqrt{20}} = 0.4380858$\n\n然后我们通过查表可以得到0.4380858对应的上侧分位数为 0.3331321（和-0.4380858对应的下侧分位数相同），因为我们的实验中是双边情况，所以$P = 0.4380858\\times2 = 0.6662642 \\gt 0.05$\n\n在确定了显著性水平$\\alpha$的情况下（$\\alpha=0.05$），如果计算出的$P\\le0.05$，说明观察值不合理，也就是样本均值离假设均值太远了，因此拒绝原假如果计算计算出的$P\\gt0.05$，则我们不能拒绝原假设。\n\n注：设连续型随机变量$X$的分布函数为$F(x)$，密度函数为$f(x)$，对于任意$\\alpha (0\\lt\\alpha\\lt1)$，假如$x_\\alpha$满足条件\n\n$$F(x_\\alpha) = \\int_{-\\infty}^{x_\\alpha} f(x) dx = \\alpha$$\n\n则$x_\\alpha$称为$X$分布的$\\alpha$分位数，或称为$\\alpha$下侧分位数。假如$x^{\\prime}_\\alpha$满足\n\n$$1 - F(x^{\\prime}_\\alpha) = \\int_{-\\infty}^{x_\\alpha} f(x) dx = \\alpha$$\n\n则$x^{\\prime}_\\alpha$称为$X$分布的$\\alpha$分位数。\n\n通俗地理解分位数就是对应某个概率面积的横坐标，如果是左侧面积（概率）叫下侧分位数，如果是右侧面积（概率）叫上侧分位数。\n\n## R中的实践\n\n好的，下面我们来看如何在R中重复上面的实验：\n\n产生一个随机的模拟序列（二项分布，生成0和1）\n\n`flips <- rbinom(20, 1, 0.4)`\n\n结果如下：`1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1`\n\n使用R内置的$t$检验函数如下：\n\n`t.test (flips, mu=0.5)`\n\n输出结果如下：\n\n```\n\tOne Sample t-test\n\ndata:  flips\nt = 0.43809, df = 19, p-value = 0.6663\nalternative hypothesis: true mean is not equal to 0.5\n95 percent confidence interval:\n 0.3111171 0.7888829\nsample estimates:\nmean of x \n     0.55 \n```\n\n如果需要我们手动计算上面这些值，我们是否可以计算出来呢？计算的过程就是上面讲解假设减压和P值的过程。\n\n首先$t$值的计算很简单，使用公式$t=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}$即可，代码如下，结果为0.4380858\n\n`(mean(flips) - 0.5) / (sd(flips) / sqrt(length(flips)))`\n\n$df$指代的是自由度$n-1=20-1=19$\n\n接下来我们计算P值，通过计算-0.4380858的下侧分位数再乘以2可得，代码如下，结果为0.6662642\n\n`pt(-0.4380858, 19) * 2`\n\n在来看我们的置信区间，其实这里就是我们所要求的接受域\n\n左侧计算代码：`qt(0.025, 19) * (sd(flips) / sqrt(20)) + 0.55` 结果为：0.3111171\n\n右侧计算代码：`qt(0.975, 19) * (sd(flips) / sqrt(20)) + 0.55` 结果为：0.7888829\n\n## 参考文献\n\n[1] 茆诗松. 概率论与数理统计 (第二版)[M]. 2000.\n\n[2] Griffiths D. Head first statistics[M]. Oreilly Vlg Gmbh & Co, 2009.","tags":["假设检验","P值"],"categories":["数学"]},{"title":"使用Python计算方差协方差相关系数","url":"/math/使用Python计算方差协方差相关系数/","content":"\n# 使用Python计算方差，协方差和相关系数\n\n[TOC]\n\n## 数学定义\n\n### 期望\n\n设随机变量$X$只取有限个可能值$a_i (i=0, 1, ..., m)$，其概率分布为$P (X = a_i) = p_i$. 则$X$的数学期望，记为$E(X)$或$EX$，定义为：\n\n$$E(X) = \\sum\\limits_ia_ip_i$$\n\n### 方差\n\n设$X$为随机变量，分布为$F$，则\n\n$$Var(X) = E(X-EX)^2 $$\n\n称为$X$(或分布$F$)的方差，其平方根$\\sqrt{Var(X)}$称为$X$(或分布$F$)的标准差.\n\n方差和标准差是刻画随机变量在其中心位置附近散布程度的数字特征。\n\n注意：样本方差和总体方差的区别\n\n统计学上对于样本方差的无偏估计使用如下公式计算：\n\n$$s^2 = \\frac{1}{n-1} \\sum\\limits_{i=1}^n(x_i -\\bar{x})^2 $$\n\n前面有一个系数$\\frac{1}{n-1}$，当时当样本数量很大的时候，$\\frac{n}{n-1}$近似为1，可以直接使用总体方差公式进行计算。\n\n### 协方差\n\n协方差用来刻画两个随机变量$X, Y$之间的相关性，定义为\n\n$$Cov(X, Y) = E[(X - EX)(Y-EY)]$$\n\n如果协方差为正，说明X，Y同向变化，协方差越大说明同向程度越高；如果协方差为负，说明X，Y反向运动，协方差越小说明反向程度越高\n\n### 相关系数\n\n相关系数可以理解为标准化以后的协方差，设$X$的标准差为$\\sigma_x$，$Y$的标准差为$\\sigma_y$定义为\n\n$$\\rho = \\frac{Cov(X, Y)}{\\sigma_x\\sigma_y}$$\n\n相关系数消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度\n\n### 协方差矩阵\n\n协方差只能表示两个随机变量的相关程度（二维问题），对于大于二维的随机变量，可以使用协方差矩阵表示.\n\n协方差矩阵的每一个值就是对应下标的两个随机变量的协方差\n\n对于三维协方差矩阵，$C=\\begin{bmatrix}Cov(X, X) & Cov(X, Y) & Cov(X, Z) \\\\ Cov(Y, X) & Cov(Y, Y) & Cov(X, Y) \\\\ Cov(Z, X) & Cov(Z, Y) & Cov(Z, Z)\\end{bmatrix}$\n\n## 使用NumPy包计算\n\n```Python\nimport numpy as np\n\n# 随机生成两个样本\nx = np.random.randint(0, 9, 1000)\ny = np.random.randint(0, 9, 1000)\n\n# 计算平均值\nmx = x.mean()\nmy = y.mean()\n\n# 计算标准差\nstdx = x.std()\nstdy = y.std()\n\n# 计算协方差矩阵\ncovxy = np.cov(x, y)\nprint(covxy)\n\n# 我们可以手动进行验证\n# covx等于covxy[0, 0], covy等于covxy[1, 1]\n# 我们这里的计算结果应该是约等于，因为我们在计算的时候是使用的总体方差(总体方差和样本方差是稍微有点区别的)\ncovx = np.mean((x - x.mean()) ** 2) \ncovy = np.mean((y - y.mean()) ** 2) \nprint(covx)\nprint(covy)\n# 这里计算的covxy等于上面的covxy[0, 1]和covxy[1, 0]，三者相等\ncovxy = np.mean((x - x.mean()) * (y - y.mean()))\nprint(covxy)\n\n# 下面计算的是相关系数矩阵(和上面的协方差矩阵是类似的)\ncoefxy = np.corrcoef(x, y)\nprint(coefxy)\n```\n\n一组可能的输出结果：\n\n```\n[[6.83907508 0.10925926]\n [0.10925926 6.53390891]]\n6.832236\n6.527375\n0.10914999999999989\n[[1.         0.01634455]\n [0.01634455 1.        ]]\n```\n\n","tags":["数理统计","方差","协方差","相关系数"],"categories":["数学"]},{"title":"一文详解卷积和逆卷积","url":"/dl/一文详解卷积和逆卷积/","content":"\n# 一文详解卷积和逆卷积\n\n卷积神经网络（CNN）在计算机视觉大放异彩，入门CNN的第一步就是理解什么是卷积（Convolution）运算。本文旨在以通俗易懂的方式让读者理解卷积的概念。\n\n注：本文的图片素材全部来源于网络，如有侵权，请联系作者删除。\n\n## 卷积运算\n\n卷积在数学上是两个变量在某范围内相乘后求和的结果。在数字图像处理中，卷积操作其实就是利用卷积核（卷积模板）在图像上滑动，将图像点上的像素灰度值与对应的卷积核上的数值相乘，然后将所有相乘后的值相加作为卷积核中间像素对应的图像上像素的灰度值，并最终滑动完所有图像的过程，如图下图所示（图中的蓝色代表输入，青色代表输出）。\n\n![卷积运算](/images/ml/no_padding_no_strides.gif)\n\n在卷积运算中，我们有下面几个概念：\n\n- Padding 有的时候对于给定输入图像的大小，我们需要得到制定大小的输出。这时候，我们可以通过给图像的边缘增加0像素值得方法获得。这个0像素值区域的大小，我们一般称之为padding。\n- Stride 在一般的卷积过程中，我们使用步长为1进行卷积核在图像上的滑动；但是有时候出于缩小输出图片尺寸的原因，我们也会采用大于1的步长。卷积核每次滑动的步长，我们称之为stride。\n\n| <img src=\"/images/ml/same_padding_no_strides.gif\" alt=\"Padding为1的卷积\" width=\"260\"/> | <img src=\"/images/ml/no_padding_strides.gif\" alt=\"Stride为2的卷积\" width=\"300\"/> |\n| :----------------------------------------------------------: | :----------------------------------------------------------: |\n|                Padding为$1\\times1$的卷积操作                 |                 Stride为$2\\times2$的卷积操作                 |\n\n\n\n### 单通道\n\n对于一个长为$H$，宽为$W$的灰度图像来说，其尺寸可以表示为$(1, W, H)$。这里的1我们称之为通道（Channel）。所谓“一图胜千言”，下面的动图以单通道为例，演示了如何进行卷积运算得到最终的输出结果。\n\n![步长为1的卷积](/images/ml/stride1.gif)\n\n![步长为2的卷积](/images/ml/stride2.gif)\n\n### 多通道\n\n那如果对于一个RGB彩色图像我们的通道就是3，对于多通道的输入，我们如何进行卷积操作，输出为多通道或者单通道呢？\n\n![多通道卷积](/images/ml/rgb.gif)\n\n从上图可以知道，对于多通道的输入，卷积操作在每个Channel上分别进行，然后进行求和得到输出。比如，我的输入是$(32, W, H)$，我的输出是$(64, W, H)$，则需要新的卷积核的个数是$32 \\times 64$。因为，对于在输入的32个通道的每个通道都需要64个卷积核，每个通道做完卷积运算，然后再求和，得到最后的64个通道的输出。\n\n### 卷积运算的参数计算\n\n根据前面的分析，到这里，卷积运算的参数的求解就很明确了。\n\n设我们的输入通道是$p$，输出通道是$q$，则\n\n如果我们不考虑Bias（增益），那么对于一个$m \\times n$的卷积核（我们一般取$m =n$），我们需要学习的参数为$(m \\times n \\times p) \\times q$；\n\n如果加上Bias（就是给卷积核作用以后的结果添加一个常数），那么我们需要学习的参数为$(m \\times n \\times p + 1) \\times q $。\n\n## 逆卷积\n\n在CNN中，我们经常会使用所谓的逆卷积（Decovolution）进行输入尺寸的放大，但是注意这个逆卷积不是卷积的逆操作。下面，我们还是看图说话，到底什么是逆卷积呢？(图中蓝色代表的是输入，青色代表的是输出)\n\n![逆卷积](/images/ml/no_padding_no_strides_transposed.gif)\n\n可以看到其实逆卷积和卷积操作并没有本质的区别，只是在输出的尺寸上有所区别。\n\n## 卷积运算的矩阵实现\n\n那么在计算机内部我们如何实现卷积操作呢？答案是矩阵乘法。\n\n我们还是看图说话，对于卷积操作，我们对输入图像以及卷积核做Unroll操作以后，进行矩阵相乘得到输出。\n\n![卷积操作的矩阵实现](/images/ml/deconvolution-unroll.jpg)\n\n对于逆卷积操作，我们对卷积核进行Unroll操作以后进行转置，然后再做矩阵乘法，得到输入。所以基于这个原因，我们一般称逆卷积为转置卷积（Transposed Convolution）。\n\n![逆卷积操作的矩阵实现](/images/ml/deconvolution.jpg)\n\n## 参考资料\n\n1. [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n2. [Undrestanding Convolutional Layers in Convolutional Neural Networks](http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html)\n3. [Convolution Arithmetic in Deep Learning](https://nrupatunga.github.io/convolution-2/)\n4. [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n\n","tags":["深度学习","卷积","转置卷积","逆卷积"],"categories":["深度学习"]},{"title":"C++中argmin和argmax的实现","url":"/cxx/C-中argmin和argmax的实现/","content":"\n# C++中argmin和argmax的实现\n\n在Python中`argmin`和`argmax`这两个函数一般是用来就一列数中的最小值和最大值的索引。C++中我们如何实现呢？\n\n## 实现思路\n\n1. 使用STL中的`std::min_element`函数求出最小值；\n2. 使用STL中的`std::distance`计算最小值跟迭代器的头部的距离；\n\n## 实现代码\n\n```C++\n#include <algorithm>\n\ntemplate<class ForwardIterator>\ninline size_t argmin(ForwardIterator first, ForwardIterator last)\n{\n    return std::distance(first, std::min_element(first, last));\n}\n\ntemplate<class ForwardIterator>\ninline size_t argmax(ForwardIterator first, ForwardIterator last)\n{\n    return std::distance(first, std::max_element(first, last));\n}\n```\n\n## 测试代码\n\n```C++\nint main() {\n    array<int, 7> numbers{2, 4, 8, 0, 6, -1, 3};\n    size_t minIndex =  argmin(numbers.begin(), numbers.end());\n    cout << minIndex << '\\n';\n    vector<float> prices = {12.5, 8.9, 100, 24.5, 30.0};\n    size_t maxIndex = argmax(prices.begin(), prices.end());\n    cout << maxIndex << '\\n';\n    return 0;\n}\n```\n\n输出结果：\n\n```\n5\n2\n```\n\n\n\n","tags":["C++","argmin","argmax"],"categories":["C++"]},{"title":"C++17使用std::apply和fold Expression对tuple进行遍历","url":"/cxx/C-17使用std-apply和fold-expression对tuple进行遍历/","content":"\n# C++17使用std::apply和fold expression对std::tuple进行遍历\n\n## std::apply函数\n\n先来看这个`std::apply`函数，这个函数定义在`tuple`头文件中，函数签名如下：\n\n```C++\ntemplate <class F, class Tuple>\nconstexpr decltype(auto) apply(F&& f, Tuple&& t);\n```\n\n该函数接受两个参数，第一个是一个函数对象，第二个是一个Tuple对象\n\n来看一个最简单的示例：\n\n```C++\n#include <tuple>\n#include <iostream>\n\nint main() {\n    // 两个元素相加\n    std::cout << std::apply([](auto x, auto y) { return x + y; }, \n                            std::make_tuple(1, 2.0)) << '\\n';\n}\n```\n\n输出结果是3\n\n这个例子中第一个参数使用Lambda匿名函数将`tuple`中的两个元素相加，第二个使用`std::make_tuple`函数构造一个只含有两个元素的`tuple`\n\n## fold expression\n\n这个特性是C++ 17中我觉得很有用的一个新特性，使用规则有下面四条：\n\n1) Unary right fold ($E$ $op$ ...) becomes ($E_1$ $op$ (... $op$ ($E_{N-1}$ $op$ $E_N$)))\n\n2) Unary left fold (... $op$ $E$) becomes ((($E_1$ $op$ $E_2$) $op$ ...) $op$ $E_N$)\n\n3) Binary right fold ($E$ $op$ ... $op$ $I$) becomes ($E_1$ $op$ (... $op$ ($E_{N−1}$ $op$ ($E_N$ $op$ $I$))))\n\n4) Binary left fold ($I$ $op$ ... $op$ $E$) becomes (((($I$ $op$ $E_1$) $op$ $E_2$) $op$ ...) $op$ $E_N$)\n\n这里的$E$指的是Expression（符合C++语法的表达式），$op$指的是operator（操作符），$N$是parameter pack（可变参数）的个数，$I$是一个常数。\n\n可能看这个规则有些抽象，我们来看一些具体的例子：\n\n```C+++\n#include <tuple>\n#include <iostream>\n\nint main() {\n    // 多个元素相加，使用parameter pack\n    std::cout << std::apply([](auto&& ... args) { return (args + ...); },\n                            std::make_tuple(1, 2.f, 3.0)) << '\\n';\n    // 遍历tuple并输出，注意逗号操作符的使用\n    std::apply([](auto&&... args) { ((std::cout << args << '\\n'), ...); },\n                std::make_tuple(1, 2.f, 3.0));\n}\n```\n\n输出如下：\n\n```\n6\n1\n2\n3\n```\n\n\n\n第6行中，`std::apply`函数的第一个参数是一个Lambda匿名函数，函数的参数是一个可变参数`args`，函数体中只有一条语句`args + ...`，这个情况就是上面的第一种情况：这里的$E$就是`args`，$op$就是`+`，所以展开来就是$args_1 + args_2 + args_3$（因为参数的个数是3）。\n\n第9行中，Lambda匿名函数的函数体是`((std::cout << args << '\\n'), ...)`这是一个逗号操作符，也属于上面四种情况中的第一种：这里的$E$就是`std::cout << args << '\\n')`，$op$就是`,`，所以这一行就打印输出了`tuple`的每一个元素。如果在C++17之前想要遍历`tuple`就比较麻烦，需要很多额外的操作。\n\n## 参考资料\n\n1. [std::apply](https://zh.cppreference.com/w/cpp/utility/apply)\n2. [fold expression](https://en.cppreference.com/w/cpp/language/fold)","tags":["C++","fold expression"],"categories":["C++"]},{"title":"C++函数指针和std::function对象","url":"/cxx/C-函数指针和std-function对象/","content":"\n# C++函数指针和std::function对象\n\n这篇博文中通过实现对String字符串大小写转换为列来说明C++中函数指针和std::function对象的使用。\n\n我们在博文《[C++实现一个简单的String类](https://theonegis.github.io/cxx/C-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84String%E7%B1%BB/)》中的自定义的`String`类为基础，再添加两个成员函数用于将字符串全部转为大写（`toUpperCase`）和全部转为小写（`toLowerCase`）。\n\n分析一下这两个函数，我们可以发现，两个函数的实现有相同之处，都需要变量字符串中的每个字符，然后使用大写转换函数（`std::touuper`）和小写转换函数（`std::tolower`）进行转换即可。\n\n既然两个函数有相同的部分，我们可以将相同的部分抽取出来，抽取出来的这部分负责对字符串进行遍历，然后将对于单个字符转换的函数作为参数传递到该用于字符串遍历的函数中。\n\n下面我们分别使用函数指针的方式和C++ 11中的`std::function`对象进行实现。本文不对`std::function`的优点进行介绍，这是以一个简单示例进行入门介绍。\n\n## 函数指针\n\n头文件：\n\n头文件实现中我们使用了`typedef`定义了一个函数指针类型，当然我们也可以使用`using`关键字进行定义，两者类似。\n\n`String::map`函数用于对字符串进行遍历操作，然后通过传进来的函数指针对每个字符进行操作。\n\n注意我们定义的`transform`函数指针的返回值是`int`，函数参数也是`int`，这是因为`cctype`头文件中的`std::toupper`和`std::tolower`函数的签名也是这样的。\n\n```C++\nclass String {\nprivate:\n    char* _buffer;\n    size_t _length;\n    // 使用using和typedef都可以: typedef int (*transform)(int);\n    using transform = int (*)(int);\n    void init(const char* str);\n    String map(transform fun);\n\npublic:\n    String(const char* str= nullptr);       // 默认构造函数\n    String(const String& other);            // 拷贝构造函数\n    String(String&& other) noexcept;        // 移动构造函数\n    ~String();                              // 析构函数\n\n    size_t length();\n    const char* data();\n\n    char& operator[](size_t index);\n    String& operator=(const String& other);\n    String& operator=(String&& other) noexcept;\n    String operator+(const String& other);\n    bool operator==(const String& other);\n\n    String toUpperCase();\n    String toLowerCase();\n\n    friend std::ostream& operator<<(std::ostream& output, const String& str);\n    friend std::istream& operator>>(std::istream& input, String& str);\n};\n```\n\n具体实现：\n\n```C++\nString String::map(String::transform fun) {\n    char* transformed = new char[_length];\n    for (size_t i = 0; i < _length; ++i) {\n        transformed[i] = static_cast<char>(fun(static_cast<int>(_buffer[i])));\n    }\n    return String(transformed);\n}\n\nString String::toUpperCase() {\n    return map(std::toupper);\n}\n\nString String::toLowerCase() {\n    return map(std::tolower);\n}\n```\n\n测试文件：\n\n```C++\n#include \"strings.h\"\n#include <iostream>\n\nusing std::cout;\n\nint main() {\n    String str1(\"Hello World\");\n    String str2 = str1.toUpperCase();\n    cout << str2 << '\\n';\n    return 0;\n}\n```\n\n输出结果：\n\n```\n默认构造函数(Hello World)\n默认构造函数(HELLO WORLD)\nHELLO WORLD\n析构函数(HELLO WORLD)\n析构函数(Hello World)\n```\n\n\n\n## std::function对象\n\n头文件\n\n可以看到我们这里使用了`std::function`类型作为`String::map`函数的参数类型，`std::function`是一个模板类，尖括号中标识了返回值，圆括号中标识了参数列表（可以是多个）。\n\n这里我们的`std::function`对象类型的返回值和参数列表都是`char`。\n\n（为什么不跟前面一样都用`int`呢？不感兴趣的可以忽略这一段。我做了测试：如果用`int`的话，会跟`locale`中定义的`toupper`和`tolower`函数定义冲突。`locale`头文件中的这两个函数的返回值和参数是`char_type`类型，编译不通过。所以我将`std::function`对象类型的返回值和参数列表定义为`char`，然后在`String::toUpperCase`和`String::toLowerCase`函数中使用匿名函数（Lambda）将`cctype`中的`std::toupper`和`std::tolower`函数的返回值和参数类型由`int`强制转换为`char`即可。）\n\n```C++\nclass String {\nprivate:\n    char* _buffer;\n    size_t _length;\n    void init(const char* str);\n    String map(std::function<char(char)> fun);\n\npublic:\n    String(const char* str= nullptr);       // 默认构造函数\n    String(const String& other);            // 拷贝构造函数\n    String(String&& other) noexcept;        // 移动构造函数\n    ~String();                              // 析构函数\n\n    size_t length();\n    const char* data();\n\n    char& operator[](size_t index);\n    String& operator=(const String& other);\n    String& operator=(String&& other) noexcept;\n    String operator+(const String& other);\n    bool operator==(const String& other);\n\n    String toUpperCase();\n    String toLowerCase();\n\n    friend std::ostream& operator<<(std::ostream& output, const String& str);\n    friend std::istream& operator>>(std::istream& input, String& str);\n};\n```\n\n实现代码：\n\n在在`String::toUpperCase`和`String::toLowerCase`函数中使用可匿名函数（Lambda）对`std::toupper`和`std::tolower`函数的返回值和参数类型`int`进行了强制转换，这样才可以跟定义的`std::function`类型的函数签名相符。\n\n```C++\nString String::map(function<char(char)> fun) {\n    char* transformed = new char[_length];\n    for (size_t i = 0; i < _length; ++i) {\n        transformed[i] = (fun(_buffer[i]));\n    }\n    return String(transformed);\n}\n\nString String::toUpperCase() {\n    return map([](char c){return static_cast<char>(toupper(static_cast<int>(c)));});\n}\n\nString String::toLowerCase() {\n    return map([](char c){return static_cast<char>(toupper(static_cast<int>(c)));});\n}\n```\n\n主函数跟上面函数指针的一样，输出结果也完全一样。\n\n这个案例虽然不能体现出使用`std::function`类型的优势，但是对于它的简单使用可以有一个参考。","tags":["c++","函数指针","function"],"categories":["C++"]},{"title":"C++移动语义及拷贝优化","url":"/cxx/C-移动语义及拷贝优化/","content":"\n# C++移动语义及拷贝优化\n\n我们知道在传统C++程序中，如果函数的返回值是一个对象的话，可能需要对函数中的局部对象进行拷贝。如果该对象很大的话，则程序的效率会降低。\n\n在C++ 11以后，出现的移动语义（Move Semantic）及拷贝优化（Copy Elision）都是解决这个问题的方法。这篇博文简单探探这些技术。\n\n## 再谈移动语义\n\n对于C++ 11移动语义的介绍，我之前写过一篇博客《[C++11中的移动语义](https://blog.csdn.net/theonegis/article/details/50512469)》进行了介绍，这里我再进行简单的总结。\n\n### 左值和右值\n\nC++中如何区分一个变量是左值还是右值呢？\n\n1. 左值一般是可寻址的变量，右值一般是不可寻址的字面常量或者是在表达式求值过程中创建的可寻址的无名临时对象； \n2. 左值具有持久性，右值具有短暂性。\n\n左值引用的符号为\"&\"（传统C++中的引用）；右值引用的符号为\"&&\"（C++ 11中的新特性）\n\n### 移动构造函数和移动赋值函数\n\n移动语义和拷贝语义是相对于的，移动类似于计算机中对文件操作的剪切，而拷贝类似于文件的复制。\n\n我们可以定义拷贝构造函数和赋值函数进行对象的复制，如果没有定义，编译器会帮我们生产默认的实现。要实现转移语义，需要定义转移构造函数，当然还可以定义转移赋值操作符。对于右值的拷贝和赋值会调用转移构造函数和转移赋值操作符。如果转移构造函数和转移拷贝操作符没有定义，那么拷贝构造函数和赋值操作符会被调用。\n\n移动构造函数和移动赋值函数都是形参（Parameter）为右值引用的函数，下面看一个例子。\n\n```C++\nstruct Foo {\n  Foo() { cout << \"Constructed\" << endl; }\n  Foo(const Foo &) { cout << \"Copy-constructed\" << endl; }\n  Foo(Foo &&) { cout << \"Move-constructed\" << endl; }\n  ~Foo() { cout << \"Destructed\" << endl; }\n};\n```\n\n可以看到第4行的移动构造函数就是一个形参为右值引用的构造器。\n\n我们通过一个示例观察其输出：\n\n```C++\nint main() { \n    vector<Foo> vec;\n    vec.push_back(Foo());\n    return 0; \n}\n```\n\n这里使用`g++`或者`clang++`编译器进行编译运行：`g++-8 foo.cpp -std=c++11 && ./a.out`\n\n我们首先注释掉`Foo`定义中的第4行的移动构造函数，结果如下：\n\n```\nConstructed\nCopy-constructed\nDestructed\nDestructed\n```\n\n可以看到拷贝构造函数被调用了。在主函数中的第3上，`Foo()`会生成一个右值对象（调用默认构造函数），然后进行拷贝构造以后传递给`vec`集合。\n\n如果我们加上移动构造函数，则运行结果如下：\n\n```\nConstructed\nMove-constructed\nDestructed\nDestructed\n```\n\n这时，因为`Foo()`是右值，所以调用了移动构造函数。\n\nNOTE：拷贝构造函数中是对传进来的对象进行了实实在在的拷贝工作；而移动构造函数中只是对传进来的对象进行了所有权的转让，即掏空传进来的对象，然后把所有权转给当前对象（`this`指针指向的那个对象）。\n\n### std::move函数\n\n编译器只对右值引用才能调用转移构造函数和转移赋值函数，而所有命名对象都只能是左值引。如果已知一个命名对象不再被使用而想对它调用转移构造函数和转移赋值函数，也就是把一个左值引用当做右值引用来使用，怎么实现呢？标准库提供了函数` std::move`，这个函数以非常简单的方式将左值引用转换为右值引用。\n\n`std::move`的实现即使将一个对象强制转型为右值引用类型的对象而已，并不做任何移动工作。\n\n## 拷贝优化\n\n现在说说第二个问题拷贝优化（Copy Elision），这是一个编译器端的技术，而移动语义是代码端的技术。虽然两者都可以减少不必要的拷贝工作。\n\n一般来说，对于支持拷贝优化的编译器会优先执行拷贝优化，如果不能进行拷贝优化，则调用移动构造函数，如果没有定义移动构造函数，则调用拷贝构造函数。当然，拷贝优化效率最高，移动构造次之。\n\n拷贝优化在两种情况下进行：一是对于函数返回值的拷贝优化；而是对于向函数中传递临时对象的优化。\n\n### 返回值的优化\n\n返回值的优化分为Named Return Value Optimization (NRVO)和Regular Return Value Optimization (RVO)\n\n还是以`Foo`为例，我们定义如下两个函数：\n\n```C++\n// Named Return Value Optimization (NRVO)\nFoo f1() {\n  Foo foo;\n  return foo;\n}\n\n// Return Value Optimization (RVO)\nFoo f2() {\n    return Foo();\n}\n\nint main() { \n    f1();\n    return 0; \n}\n```\n\n运行结果如下：\n\n```\nConstructed\nDestructed\n```\n\n可以看到并没有拷贝构造或者移动构造的发生。虽然理论上说，`f1()`函数的返回值是局部变量，会有一次拷贝构造的发生，但是实际并没有。这是因为编译器帮我们做了优化，减少了不必要的拷贝。\n\n`g++`和`clang++`都提供了`-fno-elide-constructors`选项可以关闭拷贝优化，我们重新进行编译运行`g++-8 foo.cpp -std=c++11 -fno-elide-constructors && ./a.out`\n\n结果如下，可以看到发生了一次移动构造（如果没有定义移动构造函数的话，就会调用拷贝构造函数）\n\n```\nConstructed\nMove-constructed\nDestructed\nDestructed\n```\n\n`f1()`和`f2()`会有相同的运行结果\n\n我们再来修改一下`main()`函数：\n\n```C++\nint main() { \n    Foo foo = f1();\n    return 0; \n}\n```\n\n猜一下，在有拷贝优化和没有拷贝优化的情况下会发生什么？\n\n如果没有拷贝优化的结果如下：\n\n```\nConstructed\nMove-constructed\nDestructed\nMove-constructed\nDestructed\nDestructed\n```\n\n可以看到发生了两次移动拷贝，第一次是在函数局部对象进行返回的时候拷贝到了一个临时对象中，第二次是将该临时对象用以初始化`foo`变量（注意对象初始化跟赋值的区别）。\n\n而如果有拷贝优化呢？\n\n```\nConstructed\nDestructed\n```\n\n一次移动构造或者拷贝构造都没有，是不是很爽。\n\n### 传递临时对象的优化\n\n对于函数参数传递的优化，示例如下：\n\n```C++\n// Passing a Temporary by Value\nvoid f3(Foo f) {\n    cout << \"F3 called\" << endl;\n}\n\nint main() { \n    f3(Foo());\n    return 0; \n}\n```\n\n没有拷贝优化的结果如下：\n\n```\nConstructed\nMove-constructed\nF3 called\nDestructed\nDestructed\n```\n\n有拷贝优化的结果如下：\n\n```\nConstructed\nF3 called\nDestructed\n```\n\nThere is always a but...\n\n拷贝优化不总是生效的，就是有时候拷贝优化不能成功实施。下面举一个例子：\n\n```C++\n// Copy Elision does not always work\nFoo f4(int i) {\n    Foo x, y;\n    if (i > 0) return x;\n    else return y;\n}\n\nint main() { \n    Foo foo = f4(0);\n    return 0; \n}\n```\n\n有拷贝优化的结果：\n\n```C++\nConstructed\nConstructed\nMove-constructed\nDestructed\nDestructed\nDestructed\n```\n\n没有拷贝优化的结果：\n\n```C++\nConstructed\nConstructed\nMove-constructed\nDestructed\nDestructed\nMove-constructed\nDestructed\nDestructed\n```\n\n可以看到，编译器的拷贝优化只是把在`foo`变量初始化过程中的移动构造函数给优化掉了，而`f4()`函数的返回值并没有得到优化。这是因为由于`if...else…`分支结构的存在，编译器不确定`f()`函数具体的返回对象，无法实施优化。\n\n## 结论\n\nC++移动语义即提出了一个右值引用，使用`std::move`可以强制将左值引用转为右值引用。而对于右值引用，程序可以调用移动构造函数进行对象的构造，减少了原来调用拷贝构造函数的时候很大的开销。移动构造函数和移动赋值运算符的实现即是对象所有权的转让，让那些左值对象（临时对象）变成右值对象的过程。\n\n编译器的拷贝优化确实效率很高，但是不能保证总是成功实施的。所以，好的编程习惯应该是对于自定义的类最好添加移动构造函数，重载移动赋值运算符。这样编译器的拷贝优化不成功的时候，可以调用移动构造减轻复制的开销，提高程序运行的效率。\n\n顺便提一下，在C++11以前，我们的编程习惯是为了减少不必要的复制操作，我们可能会把需要返回的对象以对象引用（左值引用，当时还没有右值引用的说法）的形式传进函数，这样在函数之外我们也可以不用拷贝获得该对象。\n\n所以C++移动语义和拷贝优化确实是C++规范中很重要的特征，对我们写程序有很大的影响。\n\n顺便提一下STL中的容器都提供了对右值引用的重载，所以当我们自定义类中实现了移动构造函数，使用STL容器的时候就没有多大的拷贝开销了，效率会有很大的提升。\n\n## 参考文献\n\n1. [右值引用与转移语义](https://www.ibm.com/developerworks/cn/aix/library/1307_lisl_c11/index.html)\n2. [Guaranteed Copy Elision](https://jonasdevlieghere.com/guaranteed-copy-elision/)\n\n\n\n","tags":["C++","移动语义","Copy Elision"],"categories":["C++"]},{"title":"C++实现一个简单的String类","url":"/cxx/C-实现一个简单的String类/","content":"\n# C++实现一个简单的String类\n\n使用基本的C++知识实现一个简单的String类，这个类中包含了C++常用的知识点。感觉是很有意思的一个小代码片段。\n\n跟大家分享一下我的实现，欢迎大家批评指正。\n\n\n## 类声明\n\n1. 该类中包含了三个构造函数：有参数的构造函数，拷贝构造函数已经移动构造函数\n\n2. 重载了[]，=（一个普通赋值运算符，一个移动赋值运算符），+，==四个运算符\n\n3. 一个用于求字符长度的方法；一个用于获取C语言类型字符串的方法\n\n4. 以友元的方式重载了输入流>>和输出流<<操作符\n\n头文件（strings.h）\n\n\n```C++\n//\n// Created by Zhenyu Tan on 2018/10/3.\n//\n#include <iostream>\n\nclass String {\nprivate:\n    char* _buffer;\n    size_t _length;\n    void init(const char* str);\n\npublic:\n    String(const char* str= nullptr);       // 默认构造函数\n    String(const String& other);            // 拷贝构造函数\n    String(String&& other) noexcept;        // 移动构造函数\n    ~String();                              // 析构函数\n\n    size_t length();\n    const char* data();\n\n    char& operator[](size_t index);\n    String& operator=(const String& other);\n    String& operator=(String&& other) noexcept;\n    String operator+(const String& other);\n    bool operator==(const String& other);\n\n    friend std::ostream& operator<<(std::ostream& output, const String& str);\n    friend std::istream& operator>>(std::istream& input, String& str);\n};\n```\n\n## 类实现\n\n源文件（strings.cpp）\n\n```C++\n//\n// Created by Zhenyu Tan on 2018/10/5.\n//\n\n#include \"strings.h\"\n#include <cstring>\n#include <exception>\n#include <iostream>\n\nusing std::cout;\nusing std::ostream;\nusing std::istream;\n\nsize_t String::length() {\n    if (0 == _length) {\n        _length = std::strlen(_buffer);\n    }\n    return _length;\n}\n\nconst char* String::data() {\n    return _buffer;\n}\n\n\nvoid String::init(const char* str) {\n    if (nullptr == str) {\n        _length = 0;\n        _buffer = nullptr;\n    } else {\n        _length = std::strlen(str);\n        _buffer = new char[_length + 1];\n        std::strcpy(_buffer, str);\n    }\n}\n\n\nString::String(const char* str) {\n    init(str);\n    cout << \"默认构造函数(\" << *this << \")\\n\";\n}\n\n\nString::String(const String& other) {\n    // 在类的成员函数中可以访问同类型实例的私有变量\n    init(other._buffer);\n    cout << \"拷贝构造函数(\" << *this << \")\\n\";\n}\n\nString::String(String&& other) noexcept {\n    // 把other对象掏空用来填充this\n    _buffer = nullptr;\n    _buffer = other._buffer;\n    _length = other._length;\n    other._buffer = nullptr;\n    other._length = 0;\n    cout << \"移动构造函数(\" << *this << \")\\n\";\n}\n\n\nString::~String() {\n    delete[] _buffer;\n    cout << \"析构函数(\" << *this << \")\\n\";\n}\n\n/*\n * 拷贝构造函数使用传入对象的值生成一个新的对象的实例\n * 赋值运算符是将对象的值复制给一个已经存在的实例\n */\nString& String::operator=(const String& other) {\n    if (this != &other) {\n        delete[] _buffer;\n        init(other._buffer);\n    }\n    cout << \"拷贝赋值操作(\" << *this << \")\\n\";\n    return *this;\n}\n\n/*\n * 移动赋值操作即把参数传进来的对象的所有权转移到this指向的对象\n * 掏空other对象的所有\n */\nString& String::operator=(String&& other) noexcept {\n   if (this != &other) {\n       _buffer = nullptr;\n       _buffer = other._buffer;\n       _length = other._length;\n       other._buffer = nullptr;\n       other._length = 0;\n   }\n    cout << \"移动赋值操作(\" << *this << \")\\n\";\n    return *this;\n}\n\n\nchar& String::operator[](size_t index) {\n    if (index >= _length) {\n        throw std::out_of_range(\"Index out of range\");\n    } else {\n        return _buffer[index];\n    }\n}\n\n\nbool String::operator==(const String& other) {\n    if (_length != other._length) {\n        return false;\n    } else {\n        return 0 == std::strcmp(_buffer, other._buffer);\n    }\n}\n\n/*\n * 关于是返回对象本身还是返回对象引用\n * 如果函数返回在函数中创建的临时对象，则不要使用引用\n * 如果函数返回的是通过引用或指针传递给它的对象，则应当按引用返回对象\n * 如果先创建一个对象，然后返回改对象的副本，则可以使用返回对象\n */\nString String::operator+(const String& other) {\n    String _str;\n    if (nullptr == _buffer) {\n        _str = other;\n    } else if (nullptr == other._buffer) {\n        _str = *this;\n    } else {\n        _str._buffer = new char[_length + other._length + 1];\n        std::strcpy(_str._buffer, _buffer);\n        std::strcat(_str._buffer, other._buffer);\n        _str._length = std::strlen(_str._buffer);\n    }\n    return _str;\n}\n\n\nostream& operator<<(ostream &output, const String& str) {\n    if (nullptr == str._buffer) {\n        output << \"\";\n    } else {\n        output << str._buffer;\n    }\n    return output;\n}\n\nistream& operator>>(istream &input, String& str) {\n    input >> str._buffer;\n    return input;\n}\n```\n\n## 调用示例\n\n```C++\n#include \"strings.h\"\n#include <iostream>\n\nusing std::cout;\n\nint main() {\n    String str1(\"Hello\");\n    cout << str1.data() << '\\n';\n    cout << str1.length() << '\\n';\n    cout << \"----------\\n\";\n    String str2 = \"Word\";\n    cout << str2 << '\\n';\n    cout << \"----------\\n\";\n    String str3 = str1 + str2;\n    cout << str3.data() << '\\n';\n    cout << str3.length() << '\\n';\n    return 0;\n}\n```\n\n运行结果：\n\n```\n默认构造函数(Hello)\nHello\n5\n----------\n默认构造函数(Word)\nWord\n----------\n默认构造函数()\nHelloWord\n9\n析构函数(HelloWord)\n析构函数(Word)\n析构函数(Hello)\n```\n\n主程序中的第7行和第11行各自调用一次默认的有参构造函数，第14行是重载的加法运算符中调用了一次无参的构造函数（由于C++编译器的优化，函数返回值没有调用拷贝构造函数）\n\n","tags":["C++","String"],"categories":["C++"]},{"title":"Python求解正态分布置信区间","url":"/math/Python求解正态分布置信区间/","content":"\n#  Python求解正态分布置信区间\n\n## 正态分布和置信区间\n\n正态分布（Normal Distribution）又叫高斯分布，是一种非常重要的概率分布。其概率密度函数的数学表达如下：\n\n$$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n\n当$\\mu=0​$，$\\sigma = 1​$时，称为标准正太分布。\n\n置信区间是对该区间能包含未知参数的可置信的程度的描述。\n\n## 使用SciPy求解置信区间\n\n```Python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nN = 10000\nx = np.random.normal(0, 1, N)\n# ddof取值为1是因为在统计学中样本的标准偏差除的是(N-1)而不是N，统计学中的标准偏差除的是N\n# SciPy中的std计算默认是采用统计学中标准差的计算方式\nmean, std = x.mean(), x.std(ddof=1)\nprint(mean, std)\n# 计算置信区间\n# 这里的0.9是置信水平\nconf_intveral = stats.norm.interval(0.9, loc=mean, scale=std)\nprint(conf_intveral)\n```\n\n输出如下：\n\n```\n0.0033541207210673997 0.9986647964318905\n(-1.639303291798682, 1.6460115332408163)\n```\n\n这里的-1.639303291798682是置信上界，1.6460115332408163是置信下界，两个数值构成的区间就是置信区间\n\n## 使用Matplotlib绘制正态分布密度曲线\n\n```\n# 绘制概率密度分布图\nx = np.arange(-5, 5, 0.001)\n# PDF是概率密度函数\ny = stats.norm.pdf(x, loc=mean, scale=std)\nplt.plot(x, y)\nplt.show()\n```\n\n这里的`pdf()`函数是Probability density function，就是本文最开始的那个公式\n\n最后的输出图像如下，可以看到结果跟理论上的正太分布还是比较像的：\n\n![](/images/math/norm.png)\n\n## 正态分布置信区间规律\n\n函数曲线下68.268949%的面积在平均数左右的一个标准差范围内\n\n函数曲线下95.449974%的面积在平均数左右两个标准差的范围内\n\n函数曲线下99.730020%的面积在平均数左右三个标准差的范围内\n\n函数曲线下99.993666%的面积在平均数左右四个标准差的范围内","tags":["正态分布","SciPy"],"categories":["数学"]},{"title":"Stone Game","url":"/leetcode/Stone-Game/","content":"\n# Stone Game\n\n## 题目描述\n\nAlex and Lee play a game with piles of stones.  There are an even number of piles **arranged in a row**, and each pile has a positive integer number of stones `piles[i]`.\n\nThe objective of the game is to end with the most stones.  The total number of stones is odd, so there are no ties.\n\nAlex and Lee take turns, with Alex starting first.  Each turn, a player takes the entire pile of stones from either the beginning or the end of the row.  This continues until there are no more piles left, at which point the person with the most stones wins.\n\nAssuming Alex and Lee play optimally, return `True` if and only if Alex wins the game.\n\n**Example 1:**\n\n```\nInput: [5,3,4,5]\nOutput: true\nExplanation: \nAlex starts first, and can only take the first 5 or the last 5.\nSay he takes the first 5, so that the row becomes [3, 4, 5].\nIf Lee takes 3, then the board is [4, 5], and Alex takes 5 to win with 10 points.\nIf Lee takes the last 5, then the board is [3, 4], and Alex takes 4 to win with 9 points.\nThis demonstrated that taking the first 5 was a winning move for Alex, so we return true.\n```\n\n**Note:**\n\n1. `2 <= piles.length <= 500`\n2. `piles.length` is even.\n3. `1 <= piles[i] <= 500`\n4. `sum(piles)` is odd.\n\n## 算法分析\n\n每次都是Alex先取，从前后两个中选取最大的，然后Lee再取，Lee再从前后两个中取最大的\n\n这告诉我们什么，毫无悬念，Alex肯定会赢呀\n\n## C++实现\n\n直接返回`true`就好了\n\n```C++\nbool stoneGame(vector<int>& piles) {\n    return true;\n}\n```\n\n\n\n模拟他们玩游戏的过程，使用两个指针分别代表指向最前面和最后面，依次移动指针\n\n```C++\nbool stoneGame(vector<int>& piles) {\n    const size_t count = piles.size();\n    vector<int> sum(count + 2, 0);\n    for (size_t i = 0, j = count - 1, k = 2; i != j ; k++) {\n        if (piles[i] > piles[j]) {\n              sum[k] = piles[i++]  +  sum[k - 2];\n        } else {\n            sum[k] = piles[j--] + sum[k - 2];\n        }\n    }\n    return sum[count] > sum[count + 1];\n}\n```","tags":["LeetCode","算法"],"categories":["LeetCode"]},{"title":"Leetcode: Counting Bits","url":"/leetcode/Leetcode-Counting-Bits/","content":"\n# 338. Counting Bits\n\n## 题目描述\n\nGiven a non negative integer number **num**. For every numbers **i** in the range **0 ≤ i ≤ num** calculate the number of 1's in their binary representation and return them as an array.\n\n**Example 1:**\n\n```\nInput: 2\nOutput: [0,1,1]\n```\n\n**Example 2:**\n\n```\nInput: 5\nOutput: [0,1,1,2,1,2]\n```\n\n**Follow up:**\n\n- It is very easy to come up with a solution with run time **O(n\\*sizeof(integer))**. But can you do it in linear time **O(n)** possibly in a single pass?\n- Space complexity should be **O(n)**.\n- Can you do it like a boss? Do it without using any builtin function like **\\_\\_builtin\\_popcount** in c++ or in any other language.\n\n## 思路分析\n\n### 思路一\n\n我们来看探究一下规律：\n\n| 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 0    | 1    | 10   | 11   | 100  | 101  | 110  | 111  | 1000 | 1001 | 1010 | 1011 | 1100 | 1101 | 1110 | 1111 |\n| 0    | 1    | 1    | 2    | 1    | 2    | 2    | 3    | 1    | 2    | 2    | 3    | 2    | 3    | 3    | 4    |\n\n如果用动态规划思路求解的话，我们有如下规律：\n\ndp[0] = 0;\n\ndp[1] = dp[0] + 1;\n\ndp[2] = dp[0] + 1;\n\ndp[3] = dp[1] +1;\n\ndp[4] = dp[0] + 1;\n\ndp[5] = dp[1] + 1;\n\ndp[6] = dp[2] + 1;\n\ndp[7] = dp[3] + 1;\n\ndp[8] = dp[0] + 1;\n\ndp[9] = dp[1] + 1;\n\n...\n\n进一步一般化这个规律：\n\ndp[0] = 0;\n\ndp[1] = dp[1-1] + 1;\n\ndp[2] = dp[2-2] + 1;\n\ndp[3] = dp[3-2] +1;\n\ndp[4] = dp[4-4] + 1;\n\ndp[5] = dp[5-4] + 1;\n\ndp[6] = dp[6-4] + 1;\n\ndp[7] = dp[7-4] + 1;\n\ndp[8] = dp[8-8] + 1;\n\ndp[9] = dp[9-8] + 1;\n\n...\n\n所以使用动态规划解决的思路如下：\n\ndp[0] = 0 \n\ndp[index] = dp[index - offset] + 1\n\n这个offset是2的倍数\n\n### 思路二\n\n第二个思路比较奇妙，有这样一个规律：对于一个整数i, i&(i-1)这个操作会去掉以二进制表示的i的最后一个1\n\n比如：10=1010，11=1011，则11&10=1010；13=1101，14=1110，则14&13=1100\n\n这样的话，使用动态规划进行解决的思路如下：\n\ndp[0] = 1\n\ndp[i] = dp[i & (i-1)] + 1\n\ni&{i-1}去掉了一个1，我们再给加上，则可以得到i中包含的1的总个数\n\n## C++实现\n\n### 思路一\n\n```C++\nclass Solution {\npublic:\n    vector<int> countBits(int num) {\n        vector<int> bits(num + 1, 0);\n        int offset = 1;\n        for (int i = 1; i <= num; ++i) {\n            offset = i == offset * 2 ?  i : offset;\n            bits[i] = bits[i - offset] +  1;\n        }\n        return bits;\n    }\n};\n```\n\n### 思路二\n\n```C++\nclass Solution {\npublic:\n    vector<int> countBits(int num) {\n        vector<int> bits(num + 1, 0);\n        for (int i = 1; i <= num; ++i) {\n            bits[i] = bits[i & (i - 1)] + 1;\n        }\n        return bits;\n    }\n};\n```\n\n\n\n## Scala实现\n\n```Scala\nobject CountingBits {\n  def countBits(num: Int): Array[Int] = {\n    val bits = Array.ofDim[Int](num + 1)\n    for (i <- 1 to num) {\n      bits(i) = bits(i & (i - 1)) + 1\n    }\n    bits\n  }\n}\n```\n\n注意：使用Scala版本在LeetCode提交会报Time Limit Exceeded错误，而使用Java相同的代码就不会。我觉得是Scala中使用Java原生数组实现的，而且还有装箱拆箱的操作，所以效率比较低，就导致了超时了。","tags":["LeetCode","动态规划","算法"],"categories":["LeetCode"]},{"title":"反向传播算法大揭秘","url":"/dl/反向传播算法大揭秘/","content":"\n# 反向传播算法大揭秘\n\n注： 该篇博文是我阅读《[How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)》一文的笔记，该博文详细介绍了反向传播算法，并给出了反向传播算法四个基本公式中的前两个证明，我顺着作者的思路证明了后面两个，并记录了证明过程，希望能帮助到需要了解反向传播算法数学原理的童鞋。\n\n## 符号说明\n\n- $w^l_{jk}$表示$l-1$层的第$k$个神经元到$l$层的第$j$个神经元连接的权重.\n- $b^l_j$表示$l$层的第$j$个神经元的偏置，$a^l_j$表示$l$层的第$j$个神经元作用于激活函数以后的取值.\n- 对于$a^l_j$的计算，我们可以使用如下公式计算：$a^l_j=\\sigma (\\sum_\\limits{k}w^l_{jk}a^{l-1}_k + b^l_j)$，其中，$\\sigma$表示的是激活函数，求和符号表示的是第$l-1$层的所有神经元与$l$层第$j$个神经元连接的加权和.\n- 上式使用矩阵表示则有：$a^l = \\sigma (w^la^{l-1} + b^l)$，其中，$w^l$表示$l$层的权重矩阵，矩阵的第$j$行第$k$列的元素为$w^l_{jk}$，类似的，$b^l$和$a^l$用列向量表示第$j$层神经元的偏置和激活值. \n- 对于$z^l = w^la^{l-1} + b^l$我们称之为$l$层的加权输出.\n- 设推导反向传播过程中的代价函数为$C$.\n- 我们使用$\\odot$表示两个矩阵对应元素的乘积，即$(s\\odot t)_{i,j} = s_{i,j} \\cdot t_{i,j}$，称之为Hadamard乘积.\n\n## 反向传播的四个基本公式\n\n反向传播过程中的四个基本公式：\n\n$$\\delta^L = \\nabla_aC\\odot \\sigma'(z^L) \\tag{BP1}$$\n\n$$\\delta^l = ((w^{l+1})^\\mathrm{T}\\delta^{l+1})\\odot\\sigma'(z^l) \\tag{BP2}$$\n\n$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j \\tag{BP3}$$\n\n$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j \\tag{BP4}$$\n\n## 反向传播算法\n\n1. 输入$x$：输入层的激活值$a^1$可以假定就是其输入$x$\n2. 前向传播：对于$l=2,3,\\cdots,L$，依次通过$z^l = w^la^{l-1} + b^l$和$a^l = \\sigma(z^l)$公式进行计算激活值\n3. 计算最终输出误差$\\delta^L$：通过公式计BP1算误差向量\n4. 反向误差传播：对于$l=L-1, L-2, \\cdots, 2$，使用公式分BP2别计算每层神经元对应的误差\n5. 更新权重和增益：根据代价函数的梯度更新权重和增益，如公式BP3和BP4所示\n\n## 推导过程\n\n下面我们来进行公式的推导\n\n### 对于公式BP1的推导\n\n设最后一层$L$的第$j$个神经元的误差是\n\n$$\\delta^L_j = \\frac{\\partial C}{\\partial z^L_j} \\tag{1}$$\n\n通过链式法则，我们可以得到\n\n$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j} \\tag{2}$$\n\n将$a^l_j=\\sigma(z^L_j)$带入可得\n\n$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j) \\tag{3}$$\n\n公式BP1即是上式的矩阵形式\n\n### 对于公式BP2的推导\n\n对于$l$层的第$j$个神经元，我们使用链式法则有：\n\n$$\\delta^l_j = \\frac{\\partial C}{\\partial z^l_j} = \\sum\\limits_k\\frac{\\partial C}{\\partial z^{l+1}_k}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = \\sum\\limits_k\\frac{\\partial z^{l+1}_k}{\\partial z^l_j}\\delta^{l+1}_k \\tag{4}$$\n\n此外，我们有\n\n$$z^{l+1}_k = \\sum\\limits_jw^{l+1}_{kj}a^l_j + b^{l+1}_k = \\sum\\limits_jw^{l+1}_{kj}\\sigma(z^l_j) + b^{l+1}_k \\tag{5}$$\n\n对上式微分，得\n\n$$\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj}\\sigma'(z^l_j) \\tag{6}$$\n\n带入公式4，可得\n\n$$\\delta^l_j = \\sum\\limits_kw^{l+1}_{kj}\\delta^{l+1}_k\\sigma'(z^l_j) \\tag{7}$$\n\n### 对于公式BP3的推导\n\n对于$l$层的第$j$个神经元，我们使用链式法则有：\n\n$$\\frac{\\partial C}{\\partial b^L_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j} \\tag{8}$$\n\n由于$\\frac{\\partial z^l_k}{\\partial b^l_j}$恒等于1，所以有\n\n$$\\frac{\\partial C}{\\partial b^L_j} = \\frac{\\partial C}{\\partial z^l_j} = \\delta^l_j \\tag{9}$$\n\n### 对于公式BP4的推导\n\n因为$z^{l}_j = \\sum\\limits_kw^{l}_{jk}a^{l-1}_k + b^{l+1}_j$，取导数有\n\n$$\\frac{\\partial z^l_j}{\\partial w^{l}_{jk}} = a^{l-1}_k \\tag{10}$$\n\n对于$l​$层的第$j​$个神经元，我们使用链式法则有：\n\n$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_{j}} \\frac{\\partial z^l_{j}}{\\partial w^l_{jk}}    \\tag{11}$$\n\n将$\\delta^l_j = \\frac{\\partial C}{\\partial z^l_j}$和$\\frac{\\partial z^l_{jk}}{\\partial w^l_{jk}} = a^{l-1}_k$带入公式11，得\n$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\delta^l_ja^{l-1}_k \\tag{12}$$\n\n终于，推导完毕！\n\n\n\n","tags":["深度学习","反向传播","backpropagation"],"categories":["深度学习"]},{"title":"Range Sum Query - Immutable","url":"/leetcode/Range-Sum-Query-Immutable/","content":"\n# 303. Range Sum Query - Immutable\n\n## 题目描述\n\nGiven an integer array *nums*, find the sum of the elements between indices *i* and *j* (*i* ≤ *j*), inclusive.\n\n**Example:**\n\n```\nGiven nums = [-2, 0, 3, -5, 2, -1]\n\nsumRange(0, 2) -> 1\nsumRange(2, 5) -> -1\nsumRange(0, 5) -> -3\n```\n\n**Note:**\n\n1. You may assume that the array does not change.\n2. There are many calls to *sumRange* function.\n\n## 思路分析\n\n题目中提示给定的`nums`数组不变，而且`sumRange()`函数可能需要多次被调用。\n\n这个就很简单了，我们计算出来当前位置元素到初始元素之间所有元素之和，并进行存储，每次需要求解给定区间元素之和的时候收尾相减即可。这样就可以大大减少调用多次`sumRange()`函数的计算时间。\n\n注意：我们的和元素组成的数组中第一个元素是0。\n\n其实，我觉得这道题并不完全能算得上一道动态规划的题目。但是LeetCode把这道题归类到动态规划中也说得过去吧。\n\n## C++示例\n\n```C++\nclass NumArray {\nprivate:\n    vector<int> sums;\npublic:\n    NumArray(vector<int> nums) {\n        sums.push_back(0);\n        for (int i = 0; i < nums.size(); ++i) {\n            sums.push_back(nums[i] + sums[i]);\n        }\n    }\n\n    int sumRange(int i, int j) {\n        return sums[j + 1] - sums[i];\n    }\n};\n\nint main() {\n    vector<int> nums{-2, 0, 3, -5, 2, -1};\n    NumArray obj(nums);\n    int result = obj.sumRange(2, 5);\n    cout << result << endl;\n    return 0;\n}\n```\n\n\n\n## Scala示例\n\n```Scala\nobject RangeSumQuery {\n\n  class NumArray(nums: Array[Int]) {\n\n    private val sums = new Array[Int](nums.length + 1)\n    for (i <- nums.indices)\n      sums(i + 1) = sums(i) + nums(i)\n\n    def sumRange(i: Int, j: Int): Int = {\n      sums(j + 1) - sums(i)\n    }\n  }\n\n  def main(args: Array[String]): Unit = {\n    val nums = Array(-2, 0, 3, -5, 2, -1)\n    val obj = new NumArray(nums)\n    val result = obj.sumRange(2, 5)\n    println(result)\n  }\n}\n```\n\n","tags":["LeetCode","动态规划","算法"],"categories":["LeetCode"]},{"title":"Python调用C++代码","url":"/python/Python调用C-代码/","content":"# Python调用C++代码\n\n今天在研究PyTorch中Tensor的一些操作的时候，发现其底层Tensor的操作都是用C++写的，并使用[pybind11](https://github.com/pybind/pybind11)进行C++和Python的桥接。所以，我就想着探索一下Python中如何调用C++代码？\n\n## 可行方案\n\n其实，方案还是挺多的：\n\n- Python内置的[ctypes](https://docs.python.org/3/library/ctypes.html)接口（可以将C/C++代码编译为动态库，在Python中进行调用）\n- [CFFI](https://cffi.readthedocs.io/en/latest/)（提供了一种在Python代码中混合C代码的途径）\n- [Cython](http://docs.cython.org/en/latest/index.html) (C后端版本的Python实现，建立了Python类型和C语言之间的映射关系，提供了使用Python代码调用C函数库的能力)\n- [Boost.Python](https://www.boost.org/doc/libs/1_68_0/libs/python/doc/html/index.html) (提供手动导出C++代码接口的能力供Python调用)\n- [SWIG](http://www.swig.org/)（不止支持C/C++到Python的桥接，SWIG支持很多语言跟C/C++的桥接，是广泛使用的一种解决方案）\n\n## Hello Word示例\n感觉ctypes简单很多，对于小型程序感觉还是挺友好的，所以该篇博客介绍一下使用ctypes调用C/C++的步骤。\n\n### 编写C或者C++代码\n   \n头文件（test.h）\n```c\nint sum(int, int);\n```\n\nC++代码（test.cpp）\n```c++\n// Windows需要__declspec(dllexport)申明\n// extern \"C\"指示以C语言规范进行编译\n#define DLLEXPORT extern \"C\"\nDLLEXPORT int sum(int a, int b) {\n    return a + b;\n}\n```\n\n或者C代码（test.c）\n```c\n// 同样的Windows在函数前面需要__declspec(dllexport)申明\nint sum(int a, int b) {\n    return a + b;\n}\n```\n\n### C代码编译成动态链接库\n   \n我使用的是macOS，选择使用[gcc](https://gcc.gnu.org/)进行编译`gcc -Wall -Wextra -O -ansi -pedantic -shared test.c -o test.so`\n\n### 在Python中调用\n\n```\nIn [1]: from ctypes import cdll\n\nIn [2]: mydll = cdll.LoadLibrary('test.so')\n\nIn [3]: mydll.sum\nOut[3]: <_FuncPtr object at 0x10f4cf688>\n\nIn [4]: mydll.sum(5, 2)\nOut[4]: 7\n```\n\n\n","tags":["C++","Python"],"categories":["Python"]},{"title":"Min Cost Climbing Stairs","url":"/leetcode/Min-Cost-Climbing-Stairs/","content":"\n# Min Cost Climbing Stairs\n\n## 题目\n\nOn a staircase, the `i`-th step has some non-negative cost `cost[i]` assigned (0 indexed).\n\nOnce you pay the cost, you can either climb one or two steps. You need to find minimum cost to reach the top of the floor, and you can either start from the step with index 0, or the step with index 1.\n\n**Example 1:**\n\n```\nInput: cost = [10, 15, 20]\nOutput: 15\nExplanation: Cheapest is start on cost[1], pay that cost and go to the top.\n```\n\n**Example 2:**\n\n```\nInput: cost = [1, 100, 1, 1, 1, 100, 1, 1, 100, 1]\nOutput: 6\nExplanation: Cheapest is start on cost[0], and only step on 1s, skipping cost[3].\n```\n\n## 思路分析\n\n其实，刚看到这个题目的时候，我是有点迷惑的，甚至浪费了不少时间。\n\n比如对于第一个例子，我以为结果应该是10。因为我从第0个台阶开始，跨两步就可以到达最顶层。\n\n然而，这样的理解和题目的本意是不相符的。题目中的“top of the floor”只的是最后一层台阶的更上一层。也就是说，对于第一个例子，我需要跨到3层（从0开始）台阶上去。如果从第0个台阶开始的话，花费是（10+20）= 30，而如果从第1个台阶开始，直接跨两步的话，则花费是15。\n\n理解了题目含义，我们开始做题。很显然，这是一道动态规划问题。\n\n而且我们有如下递归公式：\n\n$$\n\\left\\{  \n\\begin{array}{**lr**}  \n\\mathrm{dp}[0] = \\mathrm{cost}[0] &  \\\\  \n\\mathrm{dp}[1] = \\mathrm{cost}[1] & \\\\  \n\\mathrm{dp}[i] = \\mathrm{cost}[i] + \\mathrm{min}(\\mathrm{dp}[i-1], \\mathrm{dp}[i-2]) &    \n\\end{array}  \n\\right.\n$$\n\n从第2层开始，我们往上走的选择是保持当前的花费最小，从而我们从前面的花费中选择最小的和当前的楼层的$\\mathrm{cost}[i]$相加。最后的返回值应该是$\\mathrm{min}(\\mathrm{dp}[i], \\mathrm{dp}[i-1])$。这种思路是一种正向思考。\n\n这是一种思路，其实我们还有另外一种思路。\n\n$$\n\\left\\{  \n\\begin{array}{**lr**}  \n\\mathrm{dp}[0] = 0 &  \\\\  \n\\mathrm{dp}[1] = 0 & \\\\  \n\\mathrm{dp}[i] = \\mathrm{min}(\\mathrm{cost}[i - 1] + \\mathrm{dp}[i-1], \\mathrm{cost}[i - 2] + \\mathrm{dp}[i-2]) &    \n\\end{array}  \n\\right.\n$$\n因为从2开始，第$i$层的花费可以由通过$i-2$层走2层或者通过$i-1$走1层到达，而$i-2$和$i-1$层所要花费的值分别为cost[$i-2$]和cost[$i-1$]。最后的返回值应该是$\\mathrm{dp}[i]$。这种思路是一种反向思考，假设我们已经到达了顶点，然后列出我们达到顶点之前的两种可能选择。\n\n## C++实现\n\n```C++\n#include <iostream>\n#include <vector>\n#include <cmath>\n\nusing namespace std;\n\nint minCostClimbingStairs(vector<int> &cost) {\n    // 第二种思路\n    int dp0 = 0;\n    int dp1 = 0;\n    int dp = 0;\n    for (int i = 2; i <= cost.size(); i++) {\n        dp = min(dp0 + cost[i - 2], dp1 + cost[i -1]);\n        dp0 = dp1;\n        dp1 = dp;\n    }\n    return dp;\n    /* 第一种思路\n    int dp0 = cost[0];\n    int dp1 = cost[1];\n    int dp = 0;\n    for (int i = 2; i < cost.size(); i++) {\n        dp = cost[i] +  min(dp0, dp1);\n        dp0 = dp1;\n        dp1 = dp;\n    }\n    return min(dp0, dp1);\n    */\n}\n\nint main() {\n    unsigned int n = 0;\n    cout << \"请输入楼梯层数：\\n\";\n    cin >> n;\n    vector<int> cost(n);\n    cout << \"请输入每层的权值：\\n\";\n    for (int i = 0; i < n; i++) {\n        cin >> cost[i];\n    }\n    int result = minCostClimbingStairs(cost);\n    cout << result << endl;\n    return 0;\n}\n```\n\n\n\n## Scala实现\n\n```Scala\npackage leetcode\n\nimport scala.math._\nimport scala.io.StdIn\n\nobject MinCostClimbingStairs {\n  def minCostClimbingStairs(cost: Array[Int]): Int = {\n    val dp = cost.clone\n    for (i <- 2 until cost.length) {\n      dp(i) = cost(i) + min(dp(i - 1), dp(i -2))\n    }\n    return min(dp(dp.length - 1), dp(dp.length - 2))\n  }\n\n  def main(args: Array[String]): Unit = {\n    println(\"请输入楼梯层数：\")\n    val count = StdIn.readInt()\n    val cost = Array.fill[Int](count)(0)\n    println(\"请输入每层的权值：\")\n    for (i <- 0 until cost.length) {\n      cost(i) = StdIn.readInt()\n    }\n\n    println(minCostClimbingStairs(cost))\n  }\n\n}\n```\n\n\n\n","tags":["LeetCode","动态规划","算法"],"categories":["LeetCode"]},{"title":"使用GDAL读取Sentinel数据","url":"/geos/使用GDAL读取Sentinel数据/","content":"\n# 使用GDAL读取Sentinel数据\n\nGDAL 2.1已经原生支持对于Sentinel数据的读取，我这里使用Sentinel-2光学卫星数据给出使用GDAL工具对其进行读取的方法。\n\n这里我们要大概知道Sentinel数据的组织。下载下来的Sentinel数据是一个ZIP压缩包，里面包含了JPEG2000格式的影像数据以及一些XML格式的元数据文件。\n\nGDAL将Sentinel数据看做一个数据集（概念上类似HDF格式的数据集），里面包含了很多子数据文件。所以，对于Sentinel数据的读取就和对于HDF数据的读取是相同的啦。\n\n对于HDF或者NetCDF格式数据的读取参考我的博文：[读取HDF或者NetCDF格式的栅格数据](https://theonegis.github.io/geos/%E5%8F%96HDF%E6%88%96%E8%80%85NetCDF%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%A0%85%E6%A0%BC%E6%95%B0%E6%8D%AE/)\n\n## 使用GDAL命令行读取Sentinel数据的元数据信息\n\n直接使用`gdalinfo [文件名]`可以查看Sentinel文件的元信息，如下图所示：\n\n![使用GDAL命令行读取Sentinel数据](/images/geos/gdal-sentinel-1.png)\n\n从上面的图中我们可以看到所有的`Subdatasets`的文件全名，这样我们可以继续使用`gdalinfo [子数据集全路径]`的方式查看具体的子数据集的元数据信息\n\n下图显示的数据子集中包含四个波段的数据（红，绿，蓝，近红外）\n\n![使用GDAL命令行读取Sentinel数据的元数据信息](/images/geos/gdal-sentinel-2.png)\n\n## 使用GDAL命令行工具将Sentinel数据转为GeoTIFF格式\n\n转换是针对具体的子数据集而言的，所以使用`gdal_translate [sentinel subdataset full name] [output filename]`命令进行\n\n下面的例子将包含红绿蓝近红外波段的数据子集转为GeoTIFF影像\n\n`gdal_translate SENTINEL2_L1C:/vsizip/S2A_MSIL1C_20180504T173911_N0206_R098_T13TGF_20180504T212111.zip/S2A_MSIL1C_20180504T173911_N0206_R098_T13TGF_20180504T212111.SAFE/MTD_MSIL1C.xml:10m:EPSG_32613 B2-3-4-8.tif`\n\n## 使用Python脚本读取Sentinel数据\n\n```Python\nfrom osgeo import gdal\n\nimport os\nos.environ['CPL_ZIP_ENCODING'] = 'UTF-8'\n\nfilename = ('/Users/tanzhenyu/Desktop/'\n            'S2A_MSIL1C_20180504T173911_N0206_R098_T13TGF_20180504T212111.zip')\n# 打开栅格数据集\nroot_ds = gdal.Open(filename)\n# 返回结果是一个list，list中的每个元素是一个tuple，每个tuple中包含了对数据集的路径，元数据等的描述信息\n# tuple中的第一个元素描述的是数据子集的全路径\nds_list = root_ds.GetSubDatasets()\n\nvisual_ds = gdal.Open(ds_list[0][0])  # 取出第12个数据子集（MODIS反射率产品的第一个波段）\nvisual_arr = visual_ds.ReadAsArray()  # 将数据集中的数据转为ndarray\ndel visual_arr\n\n# 获得栅格数据的一些重要信息\nprint(f'打开数据为：{ds_list[0][1]}')\nprint(f'投影信息：{visual_ds.GetProjection()}')\nprint(f'栅格波段数：{visual_ds.RasterCount}')\nprint(f'栅格列数（宽度）：{visual_ds.RasterXSize}')\nprint(f'栅格行数（高度）：{visual_ds.RasterYSize}')\n```\n\n程序输出如下：\n\n```\n打开数据为：Bands B2, B3, B4, B8 with 10m resolution, UTM 13N\n投影信息：PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]\n栅格波段数：4\n栅格列数（宽度）：10980\n栅格行数（高度）：10980\n```\n\n","tags":["Sentinel","Python","GDAL"],"categories":["空间数据处理"]},{"title":"Matplotlib中的两种绘图API说明","url":"/plot/Matplotlib中的两种绘图API说明/","content":"\n# Matplotlib中的两种绘图API说明\n\n在Matplotlib库中提供了两种风格的API供开发者使用：一种是Pyplot编程接口（state-based），一种是面向对象对象的编程接口（object-based）。\n\nPyplot封装了底层的绘图函数提供了一种绘图环境，使得我们可以直接像在MATLAB那样绘制图形。当我们使用`import matplotlib.pyplot as plt`语句导入`pyplot`模块，并使用`plt.plot()`绘制图形的时候，默认的`Figure`以及`Axes`等对象会自动创建以支持图形的绘制。Pyplot一来使得对MATLAB绘图熟悉的童鞋更加容易上手，二来屏蔽了一些底层通用的绘图对象的创建细节，使用更加简洁。\n\n在使用面向对象的编程接口时候，我们需要自己创建画布（FigureCanvas），自己创建图对象（Figure），自己创建Axes（一个Figure可以包含一个或者多个Axes，一个Axes可以理解为一个子图，使用一次`plot()`绘图函数便会创建一个Axes），所有对象一起才能完成一次完整的绘图。使用面向对象编程接口有利于我们对于图形绘制的完整控制，但是相对于Pyplot接口可能需要书写更多的代码。\n\n在Matplotlib官方文档中，虽然说推荐大家使用面向对象接口进行绘图，但是其中提供的例子大部分都是基于Pyplot接口的。下面我们以一个简单例子（绘制一条直线）体验一下使用两种编程接口编程的异同。\n\n## PyPlot接口\n\n```Python\nimport matplotlib.pyplot as plt\nplt.plot([1,2,3,4])\nplt.title('Title')\nplt.grid(True)\nplt.xlabel('X Axis')\nplt.ylabel('Y Axis')\nplt.show()\n```\n\n输出结果如下：\n\n![Matplotlib绘制直线](/images/tools/Simple-Matplotlib-Example.png)\n\n## 面向对象接口\n\n```Python\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nfig = Figure()\nFigureCanvas(fig)\nax = fig.add_subplot(111)  # add_subplot()方法中的111表示的是1×1格网，第1个子图\nax.plot([1, 2, 3, 4])\nax.set_title('Title')\nax.grid(True)\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nfig.savefig('test.png', dpi=120)\n```\n\n同样的例子，我们使用面向对象接口用了更多代码，但是其绘制过程也更加明了。另外，提一点，使用面向对象接口不能使用交互式的`show()`方法对图像直接进行显示。\n\n面向对象的接口需要我们对绘图的backends也有所了解，提供对应的FigureCanvas，这是很不方便的，不利于代码的跨平台移植。所以，我在编程过程中，进程使用两个接口折中的一种方案：\n\n```Python\nimport matplotlib.pyplot as plt\n\nax = plt.subplot(111)\nax.plot([1, 2, 3, 4])\nax.set_title('Title')\nax.grid(True)\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nplt.show()\n```\n\n我推荐大家平时也多多使用这种方案，自己创建`Axes`对象，然后使用面向对象接口做图形绘制。","tags":["Matplotlib","绘图"],"categories":["图表"]},{"title":"PyTorch踩坑记","url":"/dl/PyTorch踩坑记/","content":"\n# PyTorch踩坑记\n\n## 前言\n\n自己刚开始使用深度学习框架做事情的时候，选择了最容易入门的[Keras](https://keras.io/)。Keras是在其它深度学习框架（谷歌的[TensorFlow](https://github.com/tensorflow/tensorflow)，微软的[CNTK](https://github.com/Microsoft/cntk)以及[Theano](https://github.com/Theano/Theano)）的基础上，抽象了底层实现的差异，提供的更高层的API接口。说说Keras的好处吧！个人觉得Keras最吸引人的地方就是API接口的设计特别人性化，对于样本的训练，结果的测试都有一种使用传统机器学习库的感觉；函数式接口设计使得深度网络的时候特别容易，简直就像在玩乐高。如果有人想入门深度学习，我一定也会推荐Keras。\n\n后来，我为什么转到[PyTorch](https://pytorch.org/)呢？因为PyTorch大部分框架是基于Python实现的（虽然底层也有C代码），PyTorch提供了很简单的接口使得`tensor`和NumPy中的`ndarray`互相转换，这样基于NumPy的各种库我们也可以直接拿来使用。当然，这不是最重要的。我选择PyTorch的原因是因为：第一，基于Python实现，而不是像其它库一样只是提供了一个Python的调用接口而已。这样对于深度框架的调试就特别容易，如果你使用TensorFlow或者Keras，底层的实现都是C/C++，无法很好地进行底层的调试；第二，PyTorch使用动态图，而TensorFlow这样的框架使用静态图。这就是说当你使用TensorFlow框架编译一个深度模型，模型就是固定的，不容易改变，而PyTorch的动态图提供了更多的灵活性，特别是对RNN网络。所以，我在PyTorch脱离了Beta版本（0.4）以后，我果断转到了PyTorch，开始了新的学习之旅。\n\n下面记录的是我在使用PyTorch遇到的一些问题及其解决方案：\n\n## In-place operation\n\n这个问题是在我设计一个残差网络（[ResNet](https://arxiv.org/abs/1512.03385)）的时候遇到的，报错如下：`RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation.`\n\n我是参考了PyTorch官方的[ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)实现来设计我自己的网络的。其实，问题主要出在`forward()`函数中的`out += residual`这句代码。\n\n我们首先来看一下`+=`这个操作符，这是一个原位操作符因为`+=`是对`out`张量直接进行的`+`操作，就是说执行完`+=`操作以后原来`out`指向的那个张量已经改变了。如果使用`out = out + residual`会有什么不同呢？这个操作是将`out`和`residual`相加，然后将结果赋值给`out`变量。在这个过程中原来`out`变量指向的那个张量并没有被修改。\n\n那么问题来了，为什么PyTorch官方的实现中，使用`+=`的写法没有问题，而我自己代码中这样写就有问题了呢？这是因为官方的ResNet中`forward()`函数中进行相加赋值操作以后就是一个`relu`激活函数，而激活函数层不需要反向传播，所以是没问题的；而我自己设计的网络中后面还有别的层，所以就不能这样写了。\n\n## Input type and weight type should be the same\n\n这个问题是我将代码移植到GPU上运行时遇到的问题，报错如下：`RuntimeError: Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same`\n\n有人可能说，这个简单！这是你的输入数据在GPU上，而模型参数不在GPU上，使用`to()`方法将模型复制到GPU上即可。非也，我这里说的不是个问题。当然，如果有人遇到这个错误了，第一要检查的是你是不是使用`to()`或者`cuda()`方法将模型搬运到GPU上去了。\n\n我的代码已经使用`to()`将模型复制到GPU上去了，为什么还会有这个问题呢？通过两天的调试，我发现我的模型大部分参数是位于GPU上的，而模型中的一些层却在CPU上，所以导致了这个问题。\n\n注：在调试程序的时候怎么查看模型是否在GPU上呢？使用如下函数可以进行测试：`next(model.parameters()).is_cuda`\n\n我后来发现，是我在设计ResNet的时候使用了`list`存储我的残差层导致的。如果在定义模型的时候，使用普通的`list`存储的模型层，PyTorch提供的`to()`方法是不会将对应的层复制到GPU上去的。解决办法也很简单，使用`torch.nn.ModuleList`容器来存储就好了。","tags":["PyTorch"],"categories":["深度学习"]},{"title":"使用Rasterio做投影变换","url":"/geos/使用Rasterio做投影变换/","content":"\n# 使用Rasterio做投影变换\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-11\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 思路分析\n\n在之前GDAL系列文章中的《[栅格数据投影转换](https://blog.csdn.net/theonegis/article/details/80543988)》提到过，做投影转换最重要的是计算数据在目标空间参考系统中的放射变换参数（GeoTransform）和图像的尺寸（行数和列数）。而且我们使用GDAL基本库自己写代码进行了计算。\n\n在rasterio中提供了`calculate_default_transform`，可以直接计算目标系统中的放射变换参数和图像尺寸。\n\n这样我们直接根据计算的结果更新目标文件的元信息即可。\n\n## 代码实现\n\n```python\nimport numpy as np\nimport rasterio\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom rasterio import crs\n\nsrc_img = 'example.tif'\ndst_img = 'reproject.tif'\n\n# 转为地理坐标系WGS84\ndst_crs = crs.CRS.from_epsg('4326')\n\n\nwith rasterio.open(src_img) as src_ds:\n    profile = src_ds.profile\n\n    # 计算在新空间参考系下的仿射变换参数，图像尺寸\n    dst_transform, dst_width, dst_height = calculate_default_transform(\n        src_ds.crs, dst_crs, src_ds.width, src_ds.height, *src_ds.bounds)\n\n    # 更新数据集的元数据信息\n    profile.update({\n        'crs': dst_crs,\n        'transform': dst_transform,\n        'width': dst_width,\n        'height': dst_height,\n        'nodata': 0\n    })\n\n    # 重投影并写入数据\n    with rasterio.open(dst_img, 'w', **profile) as dst_ds:\n        for i in range(1, src_ds.count + 1):\n            src_array = src_ds.read(i)\n            dst_array = np.empty((dst_height, dst_width), dtype=profile['dtype'])\n\n            reproject(\n                # 源文件参数\n                source=src_array,\n                src_crs=src_ds.crs,\n                src_transform=src_ds.transform,\n                # 目标文件参数\n                destination=dst_array,\n                dst_transform=dst_transform,\n                dst_crs=dst_crs,\n                # 其它配置\n                resampling=Resampling.cubic,\n                num_threads=2)\n\n            dst_ds.write(dst_array, i)\n```\n\n","tags":["空间数据处理","Python","Rasterio","投影变换"],"categories":["空间数据处理"]},{"title":"使用Rasterio创建栅格数据","url":"/geos/使用Rasterio创建栅格数据/","content":"\n# 使用Rasterio创建栅格数据\n\n作者：阿振 邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-09\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 方法描述\n\n使用Rasterio创建并写入栅格数据比GDAL还简单一些，基本使用到两个函数：\n\n- `rasterio.open()`\n- `write()`\n\n在`open()`函数当中，我们可以像GDAL中的`Create()`方法一样，设置数据类型，数据尺寸，投影定义，仿射变换参数等一系列信息\n\n另外，Rasterio中的数据集提供了一个`profile`属性，通过该属性可以获取这些信息的集合，这样我们读取源数据文件的时候获得该属性，然后对源数据进行处理，再创建写入文件的时候，在`open()`函数中传入`profile`即可，这样就有点像GDAL中的`CreateCopy()`函数。但是Rasterio比`CreateCopy()`更为强大的地方是：你可以修改`profile`以适配你的目标文件，而`CreateCopy()`通过提供的原型文件进行创建，无法直接对这些元信息进行修改。\n\n## 代码示例\n\n下面的代码通过读取一个三个波段的Landsat影像，计算NDVI指数，然后创建输出并保存的例子。\n\n注意计算NDVI的时候对于除数为0的处理。\n\n```python\nimport rasterio\nimport numpy as np\n\n# 读入的数据是绿，红，近红外波段的合成数据\nwith rasterio.open('LC08_122043_20161207.tif') as src:\n    raster = src.read()  # 读取所有波段\n    # 源数据的元信息集合（使用字典结构存储了数据格式，数据类型，数据尺寸，投影定义，仿射变换参数等信息）\n    profile = src.profile\n    # 计算NDVI指数（对除0做特殊处理）\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ndvi = (raster[2] - raster[1]) / (raster[2] + raster[1])\n        ndvi[ndvi == np.inf] = 0\n        ndvi = np.nan_to_num(ndvi)\n    # 写入数据\n    profile.update(\n        dtype=ndvi.dtype,\n        count=1\n    )\n    '''也可以在rasterio.open()函数中依次列出所有的参数\n    with rasterio.open('NDVI.tif', mode='w', driver='GTiff',\n                       width=src.width, height=src.height, count=1,\n                       crs=src.crs, transform=src.transform, dtype=ndvi.dtype) as dst:\n    '''\n    with rasterio.open('NDVI.tif', mode='w', **profile) as dst:\n        dst.write(ndvi, 1)\n```\n\n","tags":["空间数据处理","Python","Rasterio","栅格数据"],"categories":["空间数据处理"]},{"title":"使用Rasterio读取栅格数据","url":"/geos/使用Rasterio读取栅格数据/","content":"\n# 使用Rasterio读取栅格数据\n\n作者：阿振 邮箱：tanzhenyugis@163.com \n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375> \n\n修改时间：2018-06-06 \n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## Rasterio简介\n\n有没有觉得用GDAL的Python绑定书写的代码很不Pythonic，强迫症的你可能有些忍受不了。不过，没关系，MapBox旗下的开源库Rasterio帮我们解决了这个痛点。\n\nRasterio是基于GDAL库二次封装的更加符合Python风格的主要用于空间栅格数据处理的Python库。\n\nRasterio中栅格数据模型基本和GDAL类似，需要注意的是：\n\n在Rasterio 1.0以后，对于GeoTransform的表示弃用了GDAL风格的放射变换，而使用了Python放射变换的第三方库[affine](https://github.com/sgillies/affine)库的风格。\n\n对于放射变换\n\n```\naffine.Affine(a, b, c,\n              d, e, f)\n```\n\nGDAL中对应的参数顺序是：`(c, a, b, f, d, e)`\n\n采用新的放射变换模型的好处是，如果你需要计算某个行列号的地理坐标，直接使用行列号跟给放射变换对象相乘即可，完全符合数学上矩阵乘法的操作，更加直观和方便。\n\n# 栅格数据读取代码示例\n\n下面的示例程序中演示了如何读取一个GeoTIFF文件并获取相关信息，需要注意的是：\n\n1. rasterio使用`rasterio.open()`函数打开一个栅格文件\n2. rasterio使用`read()`函数可以将数据集转为`numpy.ndarray`，该函数如果不带参数，将把数据的所有波段做转换（第一维是波段数），如果指定波段，则只取得指定波段对应的数据（波段索引从1开始）\n3. 数据的很多元信息都是以数据集的属性进行表示的\n\n```python\nimport rasterio\n\nwith rasterio.open('example.tif') as ds:\n    print('该栅格数据的基本数据集信息（这些信息都是以数据集属性的形式表示的）：')\n    print(f'数据格式：{ds.driver}')\n    print(f'波段数目：{ds.count}')\n    print(f'影像宽度：{ds.width}')\n    print(f'影像高度：{ds.height}')\n    print(f'地理范围：{ds.bounds}')\n    print(f'反射变换参数（六参数模型）：\\n {ds.transform}')\n    print(f'投影定义：{ds.crs}')\n    # 获取第一个波段数据，跟GDAL一样索引从1开始\n    # 直接获得numpy.ndarray类型的二维数组表示，如果read()函数不加参数，则得到所有波段（第一个维度是波段）\n    band1 = ds.read(1)\n    print(f'第一波段的最大值：{band1.max()}')\n    print(f'第一波段的最小值：{band1.min()}')\n    print(f'第一波段的平均值：{band1.mean()}')\n    # 根据地理坐标得到行列号\n    x, y = (ds.bounds.left + 300, ds.bounds.top - 300)  # 距离左上角东300米，南300米的投影坐标\n    row, col = ds.index(x, y)  # 对应的行列号\n    print(f'(投影坐标{x}, {y})对应的行列号是({row}, {col})')\n    # 根据行列号得到地理坐标\n    x, y = ds.xy(row, col)  # 中心点的坐标\n    print(f'行列号({row}, {col})对应的中心投影坐标是({x}, {y})')\n    # 那么如何得到对应点左上角的信息\n    x, y = (row, col) * ds.transform\n    print(f'行列号({row}, {col})对应的左上角投影坐标是({x}, {y})')\n```\n\n输出如下：\n\n```\n该栅格数据的基本数据集信息（这些信息都是以数据集属性的形式表示的）：\n数据格式：GTiff\n波段数目：3\n影像宽度：4800\n影像高度：4800\n地理范围：BoundingBox(left=725385.0, bottom=2648415.0, right=869385.0, top=2792415.0)\n反射变换参数（六参数模型）：\n | 30.00, 0.00, 725385.00|\n| 0.00,-30.00, 2792415.00|\n| 0.00, 0.00, 1.00|\n投影定义：CRS({'init': 'epsg:32649'})\n第一波段的最大值：5459\n第一波段的最小值：-313\n第一波段的平均值：489.80300625\n(投影坐标725685.0, 2792115.0)对应的行列号是(10, 10)\n行列号(10, 10)对应的中心投影坐标是(725700.0, 2792100.0)\n行列号(10, 10)对应的左上角投影坐标是(725685.0, 2792115.0)\n```\n\n","tags":["空间数据处理","Python","Rasterio","栅格数据"],"categories":["空间数据处理"]},{"title":"使用Fiona创建Shapefile矢量数据","url":"/geos/使用Fiona创建Shapefile矢量数据/","content":"\n# 使用Fiona创建Shapefile矢量数据\n\n作者：阿振 邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-10\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 基本思路\n\n使用Fiona写入Shapefile数据，主要是构建一个Schema，然后将空间对象转为GeoJSON的形式进行写入。\n\n这个Schema是一个字典结构，定义了Geometry的类型，属性字段的名称及其类型。\n\n## 代码实现\n\n这里我们举两个例子进行说明：第一是将GeoJSON数据转为Shapefile，第二个是新建一个Shapefile，然后再里面写入自定义的空间几何数据。\n\n因为从GeoJSON中读入的数据本身就是JSON格式，所以我们可以直接写入。GeoJSON的格式定义，参见：[创建Shapefile文件并写入数据](https://blog.csdn.net/theonegis/article/details/80554993)。\n\n```python\nimport fiona\nimport json\n\nwith open('China.json') as f:\n    data = json.load(f)\n\n# schema是一个字典结构，指定了geometry及其它属性结构\nschema = {'geometry': 'Polygon',\n          'properties': {'id': 'int', 'name': 'str'}}\n\n# 使用fiona.open方法打开文件，写入数据\nwith fiona.open('Provinces.shp', mode='w', driver='ESRI Shapefile',\n                schema=schema, crs='EPSG:4326', encoding='utf-8') as layer:\n    # 依次遍历GeoJSON中的空间对象\n    for feature in data['features']:\n        # 从GeoJSON中读取JSON格式的geometry和properties的记录\n        element = {'geometry': feature['geometry'],\n                   'properties': {'id': feature['properties']['id'],\n                                  'name': feature['properties']['name']}}\n        # 写入文件\n        layer.write(element)\n```\n\n第二种方法使用shapely包创建Geometry对象，然后利用`mapping`方法将创建的对象转为GeoJSON格式进行写入。\n\nShapely包提供了对空间几何体的定义，操作等功能。\n\n```python\nimport fiona\nfrom shapely.geometry import Polygon, mapping\n\n# schema是一个字典结构，指定了geometry及其它属性结构\nschema = {'geometry': 'Polygon',\n          'properties': {'id': 'int', 'name': 'str'}}\n\n# 使用fiona.open方法打开文件，写入数据\nwith fiona.open('Beijing.shp', mode='w', driver='ESRI Shapefile',\n                schema=schema, crs='EPSG:4326', encoding='utf-8') as layer:\n    # 使用shapely创建空间几何对象\n    coordinates = [[117.4219, 40.21], [117.334, 40.1221], [117.2461, 40.0781], [116.8066, 39.9902], [116.8945, 39.8145],\n                   [116.8945, 39.6826], [116.8066, 39.5947], [116.543, 39.5947], [116.3672, 39.4629],\n                   [116.1914, 39.5947], [115.752, 39.5068], [115.4883, 39.6387], [115.4004, 39.9463],\n                   [115.9277, 40.2539], [115.752, 40.5615], [116.1035, 40.6055], [116.1914, 40.7813],\n                   [116.4551, 40.7813], [116.3672, 40.9131], [116.6309, 41.0449], [116.9824, 40.6934],\n                   [117.4219, 40.6494], [117.2461, 40.5176], [117.4219, 40.21]]\n    polygon = Polygon(coordinates)  # 使用地理坐标定义Polygon对象\n    polygon = mapping(polygon)  # 将Polygon对象转为GeoJSON格式\n    feature = {'geometry': polygon,\n               'properties': {'id': 1, 'name': '北京市'}}\n    # 写入文件\n    layer.write(feature)\n```\n\n","tags":["空间数据处理","Python","Fiona","Shapefile"],"categories":["空间数据处理"]},{"title":"Fiona简介及Shapefile数据读取","url":"/geos/Fiona简介及Shapefile数据读取/","content":"\n# Fiona简介及Shapefile数据读取\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-06\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## Fiona简介\n\n用GDAL的Python绑定API书写程序有没有一种仍然在写C/C++的感觉，Fiona基于GDAL提供了更加Pythonic的读取空间矢量数据的API，参见：http://toblerity.org/fiona/index.html\n\n这里主要说一下Fiona中对数据的描述模型和GDAL中的不同：\n\nGDAL中对于矢量数据采用数据源（DataSource）- 图层（Layer）- 要素（Feature）- 属性和几何体（Attributes and Geometry）\n\nFiona采用Python中内置的数据结构表示矢量数据，一个要素以GeoJSON表示，使用Python内置的字典（`dict`）结构组织；一个图层包含在一个集合中（`Collection`）。可以对该集合进行迭代遍历，得到其中的要素。\n\n要素是以GeoJSON表示的，结构如下：\n\n```json\n{'type': 'Feature', 'id': '0', 'geometry': {'type': 'Polygon', 'coordinates': [[(96.416, 42.7588), (96.416, 42.7148), (95.9766, 42.4951), (96.0645, 42.3193), (96.2402, 42.2314), (95.9766, 41.9238), (95.2734, 41.6162), (95.1855, 41.792), (94.5703, 41.4844), (94.043, 41.0889), (93.8672, 40.6934), (93.0762, 40.6494), (92.6367, 39.6387), (92.373, 39.3311), (92.373, 39.1113), (92.373, 39.0234), (90.1758, 38.4961), (90.3516, 38.2324), (90.6152, 38.3203), (90.5273, 37.8369), (91.0547, 37.4414), (91.3184, 37.0898), (90.7031, 36.7822), (90.791, 36.6064), (91.0547, 36.5186), (91.0547, 36.0791), (90.8789, 36.0352), (90.0, 36.2549), (89.9121, 36.0791), (89.7363, 36.0791), (89.209, 36.2988), (88.7695, 36.3428), (88.5938, 36.4746), (87.3633, 36.4307), (86.2207, 36.167), (86.1328, 35.8594), (85.6055, 35.6836), (85.0781, 35.7275), (84.1992, 35.376), (83.1445, 35.4199), (82.8809, 35.6836), (82.4414, 35.7275), (82.002, 35.332), (81.6504, 35.2441), (80.4199, 35.4199), (80.2441, 35.2881), (80.332, 35.1563), (80.2441, 35.2002), (79.8926, 34.8047), (79.8047, 34.4971), (79.1016, 34.4531), (79.0137, 34.3213), (78.2227, 34.7168), (78.0469, 35.2441), (78.0469, 35.5078), (77.4316, 35.4639), (76.8164, 35.6396), (76.5527, 35.8594), (76.2012, 35.8154), (75.9375, 36.0352), (76.0254, 36.4746), (75.8496, 36.6943), (75.498, 36.7383), (75.4102, 36.958), (75.0586, 37.002), (74.8828, 36.9141), (74.7949, 37.0459), (74.5313, 37.0898), (74.5313, 37.2217), (74.8828, 37.2217), (75.1465, 37.4414), (74.8828, 37.5732), (74.9707, 37.749), (74.8828, 38.4521), (74.3555, 38.6719), (74.1797, 38.6719), (74.0918, 38.54), (73.8281, 38.584), (73.7402, 38.8477), (73.8281, 38.9795), (73.4766, 39.375), (73.916, 39.5068), (73.916, 39.6826), (73.8281, 39.7705), (74.0039, 40.0342), (74.8828, 40.3418), (74.7949, 40.5176), (75.2344, 40.4297), (75.5859, 40.6494), (75.7617, 40.2979), (76.377, 40.3857), (76.9043, 41.001), (77.6074, 41.001), (78.1348, 41.2207), (78.1348, 41.3965), (80.1563, 42.0557), (80.2441, 42.2754), (80.1563, 42.627), (80.2441, 42.8467), (80.5078, 42.8906), (80.4199, 43.0664), (80.7715, 43.1982), (80.4199, 44.165), (80.4199, 44.6045), (79.9805, 44.8242), (79.9805, 44.9561), (81.7383, 45.3955), (82.0898, 45.2197), (82.5293, 45.2197), (82.2656, 45.6592), (83.0566, 47.2412), (83.6719, 47.0215), (84.7266, 47.0215), (84.9023, 46.8896), (85.5176, 47.0654), (85.6934, 47.2852), (85.5176, 48.1201), (85.7813, 48.4277), (86.5723, 48.5596), (86.8359, 48.8232), (86.748, 48.9551), (86.8359, 49.1309), (87.8027, 49.1748), (87.8906, 48.999), (87.7148, 48.9111), (88.0664, 48.7354), (87.9785, 48.6035), (88.5059, 48.3838), (88.6816, 48.1641), (89.1211, 47.9883), (89.5605, 48.0322), (89.7363, 47.8564), (90.0879, 47.8564), (90.3516, 47.6807), (90.5273, 47.2412), (90.8789, 46.9775), (91.0547, 46.582), (90.8789, 46.3184), (91.0547, 46.0107), (90.7031, 45.7471), (90.7031, 45.5273), (90.8789, 45.2197), (91.582, 45.0879), (93.5156, 44.9561), (94.7461, 44.3408), (95.3613, 44.2969), (95.3613, 44.0332), (95.5371, 43.9014), (95.8887, 43.2422), (96.3281, 42.9346), (96.416, 42.7588)]]}, 'properties': OrderedDict([('Name', '新疆维吾尔自治区'), ('CenterX', 84.9023), ('CenterY', 42.148)])}\n```\n\n\n\n## Shapefile数据读取\n\n下面我们来体验一下Fiona的简洁之处，主要是使用Python内置的结构表示所有数据，所以使用Fiona操作空间数据就像操作Python内置的数据结构一样简单。\n\n```python\nimport fiona\n\nwith fiona.open('China.shp', encoding='utf-8') as c:\n    # 输出数据的基本信息\n    print(f'数据范围：{c.bounds}')\n    print(f'投影定义：{c.crs}')\n    print(f'数据格式：{c.driver}')\n    print(f'数据编码：{c.encoding}')\n    # 输出文件的属性字段信息\n    fields = c.schema['properties']\n    print('文件的属性字段信息：')\n    for k, v in fields.items():\n        print(f'{k} -> {v}')\n    # 遍历集合中的要素\n    # f是一个tuple，第一个元素是要素编号，第二个是dict格式的要素\n    for f in c.items():\n        # 输入要素的详细信息\n        # 要素是以GeoJSON表示的\n        print(f[1]['properties']['Name'])\n```\n\n特别需要注意数据的编码问题，要不然默认的编码会引起中文乱码，常见中文编码可能采用GBK或者UTF-8等。\n\n输出结果如下：\n\n```\n数据范围：(73.4766, 18.1055, 135.0879, 53.5693)\n投影定义：{'init': 'epsg:4326'}\n数据格式：ESRI Shapefile\n数据编码：utf-8\n文件的属性字段信息：\nName -> str:24\nCenterX -> float:24.15\nCenterY -> float:24.15\n新疆维吾尔自治区\n西藏自治区\n内蒙古自治区\n青海省\n四川省\n黑龙江省\n甘肃省\n云南省\n广西壮族自治区\n湖南省\n陕西省\n广东省\n吉林省\n河北省\n湖北省\n贵州省\n山东省\n江西省\n河南省\n辽宁省\n山西省\n安徽省\n福建省\n浙江省\n江苏省\n重庆市\n宁夏回族自治区\n海南省\n台湾省\n北京市\n天津市\n上海市\n香港特别行政区\n澳门特别行政区\n```\n\n","tags":["空间数据处理","Python","Fiona","Shapefile"],"categories":["空间数据处理"]},{"title":"矢量数据投影转换","url":"/geos/矢量数据投影转换/","content":"\n# 矢量数据投影转换\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-03\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 案例说明\n\n接着上一篇博文中，我们得到了WGS84坐标系下的中国省区图，而我们一般中国地图中使用的是割圆锥投影。\n\n由于我国位于中纬度地区，中国地图和分省地图经常采用割圆锥投影，中国地图的中央经线常位于东经105度，两条标准纬线分别为北纬25度和北纬47度，而各省的参数可根据地理位置和轮廓形状初步加以判定。\n\n在[SpatialReference](http://spatialreference.org)中查到我们一般使用的中国地图投影为：http://spatialreference.org/ref/sr-org/8657\n\nPROJ4格式的定义为：`+proj=aea +lat_1=25 +lat_2=47 +lat_0=30 +lon_0=105 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs `\n\n使用该投影，我们祖国雄鸡才会变得雄赳赳气昂昂，更好地展现我们神州大地的风采。\n\n## 方法介绍\n\n跟栅格数据投影转换一样，使用GDAL库，我们有两种方法进行矢量数据的重投影：\n\n1. 使用命令工具及其对应的命令行API接口进行转换（简单，准确，实践中一定要用这种方法）\n\n   GDAL提供了`ogr2ogr`命令行工具进行矢量数据投影转换，命令如下：`ogr2ogr -t_srs \"+proj=aea +lat_1=25 +lat_2=47 +lat_0=30 +lon_0=105 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs \" China_Projected.shp China.shp`\n\n   `-t_srs`选项制定输出数据投影，当然可以是ESPG，也可以是PROJ4或者OGC WKT格式的投影定义都OK\n\n   GDAL对该命令的封装的C/C++函数是`GDALVectorTranslate()`,Python中是`gdal.VectorTranslate()`\n\n2. 使用GDAL提供的基本API进行实现\n\n   如果要自己利用基本API函数实现的话，基本思路如下：\n\n   - 利用`osgeo.ogr.Driver.CreateDataSource()`创建输出数据\n   - 根据源文件创建目标文件的属性字段定义\n   - 利用`osgeo.osr.CoordinateTransformation`对象将源文件中的Geometry对象转为目标文件中的Geometry对象（其实质是进行不同投影系统下空间几何体的坐标转换）\n   - 遍历源文件，依次将所有几何体的Geometry及其属性写入目标文件\n\n## 代码实现\n\n1. 调用`gdal.VectorTranslate()`命令行工具的包装函数实现：\n\n```python\nfrom osgeo import gdal\nimport os\nos.environ['SHAPE_ENCODING'] = \"utf-8\"\n\n\nsrc_file = 'China.shp'\ndst_file = 'China_Reprojected.shp'\n\n# 使用命令行API转换\n# 输出数据投影定义，参考资料：http://spatialreference.org/ref/sr-org/8657\nsrs_def = \"\"\"+proj=aea +lat_1=25 +lat_2=47 +lat_0=30 +lon_0=105 +x_0=0 +y_0=0 \n+ellps=WGS84 +datum=WGS84 +units=m +no_defs \"\"\"\ngdal.VectorTranslate(dst_file, src_file, dstSRS=srs_def, reproject=True)\n\nsrc_ds = ogr.Open(src_file)\nsrc_layer = src_ds.GetLayer(0)\nsrc_srs = src_layer.GetSpatialRef()  # 输入数据投影\n```\n\n1. 调用基本API函数实现\n\n```python\nfrom osgeo import ogr\nfrom osgeo import osr\nimport os\nos.environ['SHAPE_ENCODING'] = \"utf-8\"\n\n\nsrc_file = 'China.shp'\ndst_file = 'China_Reprojected.shp'\n\nsrc_ds = ogr.Open(src_file)\nsrc_layer = src_ds.GetLayer(0)\nsrc_srs = src_layer.GetSpatialRef()  # 输入数据投影\n\n# 输出数据投影定义，参考资料：http://spatialreference.org/ref/sr-org/8657\nsrs_def = \"\"\"+proj=aea +lat_1=25 +lat_2=47 +lat_0=30 +lon_0=105 +x_0=0 +y_0=0 \n+ellps=WGS84 +datum=WGS84 +units=m +no_defs \"\"\"\ndst_srs = osr.SpatialReference()\ndst_srs.ImportFromProj4(srs_def)\n\n# 创建转换对象\nctx = osr.CoordinateTransformation(src_srs, dst_srs)\n\n# 创建输出文件\ndriver = ogr.GetDriverByName('ESRI Shapefile')\ndst_ds = driver.CreateDataSource(dst_file)\ndst_layer = dst_ds.CreateLayer('province', dst_srs, ogr.wkbPolygon)\n\n# 给输出文件图层添加属性定义\nlayer_def = src_layer.GetLayerDefn()\nfor i in range(layer_def.GetFieldCount()):\n    field_def = layer_def.GetFieldDefn(i)\n    dst_layer.CreateField(field_def)\n\n# 循环遍历源Shapefile中的几何体添加到目标文件中\nsrc_feature = src_layer.GetNextFeature()\nwhile src_feature:\n    geometry = src_feature.GetGeometryRef()\n    geometry.Transform(ctx)\n    dst_feature = ogr.Feature(layer_def)\n    dst_feature.SetGeometry(geometry)  # 设置Geometry\n    # 依次设置属性值\n    for i in range(layer_def.GetFieldCount()):\n        field_def = layer_def.GetFieldDefn(i)\n        field_name = field_def.GetName()\n        dst_feature.SetField(field_name, src_feature.GetField(field_name))\n    dst_layer.CreateFeature(dst_feature)\n    dst_feature = None\n    src_feature = None\n    src_feature = src_layer.GetNextFeature()\ndst_ds.FlushCache()\n\ndel src_ds\ndel dst_ds\n\n# 创建Shapefile的prj投影文件\ndst_srs.MorphToESRI()\n(dst_path, dst_name) = os.path.split(dst_file)\nwith open(dst_path + os.pathsep + dst_name + '.prj', 'w') as f:\n    f.write(dst_srs.ExportToWkt())\n```","tags":["空间数据处理","Python","矢量数据","投影转换"],"categories":["空间数据处理"]},{"title":"创建Shapefile文件并写入数据","url":"/geos/创建Shapefile文件并写入数据/","content":"\n# 创建Shapefile文件并写入数据\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-06-02\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 基本思路\n\n使用GDAL创建Shapefile数据的基本步骤如下：\n\n1. 使用`osgeo.ogr.Driver`的`CreateDataSource()`方法创建`osgeo.ogr.DataSource`矢量数据集\n2. 使用`osgeo.ogr.DataSource`的`CreateLayer()`方法创建一个图层\n3. 使用`osgeo.ogr.FieldDefn()`定义Shapefile文件的属性字段\n4. 创建`osgeo.ogr.Feature`对象，设置每个属性字段的值，使用`Feature`对象的`SetGeometry()`定义几何属性\n5. 创建`Feature`对象以后，使用`osgeo.ogr.Layer`的`CreateFeature()`添加`Feature`对象到当前图层\n6. 重复步骤4和5依次添加所有的`Feature`到当前图层即可\n\n## 代码实现\n\n下面的例子中，我们读取GeoJSON表示的中国省区数据，然后其转为Shapefile格式。\n\nGeoJSON编码片段如下：\n\n![GeoJSON格式表示的中国省区](../../../../../Resources/Books/Python%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/GDAL%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/GDAL%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/GeoJSON%E6%A0%BC%E5%BC%8F%E8%A1%A8%E7%A4%BA%E7%9A%84%E4%B8%AD%E5%9B%BD%E7%9C%81%E5%8C%BA.png)\n\n可以看到每个Feature都有一个properties字段和geometry字段，我们需要根据properties字段的信息创建Shapefile数据的属性表，根据geometry字段创建Shapefile中的几何数据。\n\n```python\nfrom osgeo import ogr\nfrom osgeo import osr\nimport json\nimport os\nos.environ['SHAPE_ENCODING'] = \"utf-8\"\n\n\nwith open('China.json') as f:\n    china = json.load(f)\n\n# 创建DataSource\ndriver = ogr.GetDriverByName('ESRI Shapefile')\nds = driver.CreateDataSource('China.shp')\n\n# 创建WGS84空间参考\nsrs = osr.SpatialReference()\nsrs.ImportFromEPSG(4326)\n\n# 创建图层\nlayer = ds.CreateLayer('province', srs, ogr.wkbPolygon)\n# 添加属性定义\nfname = ogr.FieldDefn('Name', ogr.OFTString)\nfname.SetWidth(24)\nlayer.CreateField(fname)\nfcx = ogr.FieldDefn('CenterX', ogr.OFTReal)\nlayer.CreateField(fcx)\nfcy = ogr.FieldDefn('CenterY', ogr.OFTReal)\nlayer.CreateField(fcy)\n\n# 变量GeoJSON中的features\nfor f in china['features']:\n    # 新建Feature并且给其属性赋值\n    feature = ogr.Feature(layer.GetLayerDefn())\n    feature.SetField('Name', f['properties']['name'])\n    feature.SetField('CenterX', f['properties']['cp'][0])\n    feature.SetField('CenterY', f['properties']['cp'][1])\n\n    # 设置Feature的几何属性Geometry\n    polygon = ogr.CreateGeometryFromJson(str(f['geometry']))\n    feature.SetGeometry(polygon)\n    # 创建Feature\n    layer.CreateFeature(feature)\n    del feature\nds.FlushCache()\n\ndel ds\n```\n\n","tags":["空间数据处理","Python","Shapefile","矢量数据"],"categories":["空间数据处理"]},{"title":"打开Shapefile文件的正确方式","url":"/geos/打开Shapefile文件的正确方式/","content":"\n# 打开Shapefile文件的正确方式\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-25\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## Shapefile文件简介\n\nShapefile文件是美国ESRI公司发布的文件格式，因其ArcGIS软件的推广而得到了普遍的使用，是现在GIS领域使用最为广泛的矢量数据格式。官方称Shapefile是一种用于存储地理要素的几何位置和属性信息的非拓扑简单格式。\n\n一般地，Shapefile文件是多个文件的集合，至少包括一个shp，shx以及dbf文件。\n\n- shp主文件使用变长记录存储空间几何数据，支持点，线，面等多种几何类型。\n- shx索引文件用于存储几何数据的索引信息，包含对主文件中每个记录长度的描述（注意不是空间索引）\n- dbf表文件是使用dBase数据库表文件进行空间属性数据存储的文件\n\n所以，我们如果要自己完全从底层写代码解析Shapefile文件的话，需要根据shx文件中的信息读取shp中的二进制数据并转化为几何对象，然后再读取dbf表格，将属性添加到几何对象上就完成了对一个Shapefile文件的解析.\n\n英文好的同学，请转移到这里：[ESRI Shapefile Technical Desc](https://www.esri.com/library/whitepapers/pdfs/shapefile.pdf)\n\n## GDAL中矢量数据组织\n\nGDAL中的栅格数据使用`OGRDataSource`表示(`OGRDataSoruce`是抽象类`GDALDataset`的子类)，一个`OGRDataSource`中包含一个或多个`OGRLayer`层，每个图层中又包含一个或者多个`OGRFeature`要素， 每个要素包含一个`OGRGeometry`及其关联的属性数据。\n\nGDAL中的空间要素模型是按照OGC的Simple Feature规范实现的，有兴趣的童鞋可以参考官方文档：[Simple Feature Access](http://www.opengeospatial.org/standards/sfa)\n\n## 使用GDAL打开Shapefile文件\n\n下面的例子演示了如何打开Shapefile文件，并读取空间要素及其属性。 实现代码如下：\n\n```python\nfrom osgeo import ogr\nimport json\n\ndata = ogr.Open('USA_adm1.shp')  # 返回一个DataSource对象\nlayer = data.GetLayer(0)  # 获得第一层数据（多数Shapefile只有一层）\n\nextent = layer.GetExtent()  # 当前图层的地理范围\nprint(f'the extent of the layer: {extent}')\n\nsrs = layer.GetSpatialRef()\nprint(f'the spatial reference system of the data: {srs.ExportToPrettyWkt()}')\n\nschema = []  # 当前图层的属性字段\nldefn = layer.GetLayerDefn()\nfor n in range(ldefn.GetFieldCount()):\n    fdefn = ldefn.GetFieldDefn(n)\n    schema.append(fdefn.name)\nprint(f'the fields of this layer: {schema}')\n\nfeatures = []  # 图层中包含的所有feature要素\nfor i in range(layer.GetFeatureCount()):\n    feature = layer.GetFeature(i)\n    features.append(json.loads(feature.ExportToJson()))\n\nprint(f'the first feature represented with JSON: {features[0]}')\n```\n\n","tags":["空间数据处理","Python","Shapefile","矢量数据"],"categories":["空间数据处理"]},{"title":"取HDF或者NetCDF格式的栅格数据","url":"/geos/取HDF或者NetCDF格式的栅格数据/","content":"\n# 读取HDF或者NetCDF格式的栅格数据\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-17\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## HDF和NetCDF简介\n\n### HDF\n\nHDF（Hierarchical Data Format）由NCSA（National Center for Supercomputing Applications）设计提出，官方对其定义是：HDF5 is a unique technology suite that makes possible the management of extremely large and complex data collections.\n\nHDF supports n-dimensional datasets and each element in the dataset may itself be a complex object.\n\nHDF是对HDF数据模型，数据格式以及HDF库API等一系列技术的总称. HDF的最新版本是HDF5.\n\nHDF数据模型基于组（groups）和数据集（datasets）概念：如果把HDF数据比作磁盘，那么组相当于文件夹，数据集相当于文件。组和数据集都有用户自定义的属性（attributes）.\n\nMODIS影像，以及我国的风云卫星数据都适用HDF格式进行存储.\n\n### NetCDF\n\nNetCDF（Network Common Data Format）由UCAR（University Corporation for Atmospheric Research）设计提出，其官方的定义是：NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.\n\nNetCDF是面向多维数组的数据集，一个NetCDF文件主要是Dimensions, Variables, Attributes, Data 四个部分组成的：\n\n- Dimension主要是对维度的定义说明，例如：经度，维度，时间等；\n- Variables是对数据表示的现象的说明，例如：温度，湿度，高程等；\n- Attributes是一些辅助的元信息说明，例如变量的单位等；\n- Data是主要对现象的观测数据集。\n\nNetCDF有两个数据模型：经典模型（NetCDF3之前模型）和增强模型（NetCDF4）\n\nNetCDF最新版本是NetCDF4，NetCDF4的API接口建立在HDF5之上，和HDF5是兼容的.\n\n如果搞大气研究的同学一定对NetCDF格式不陌生，接触到的大部分数据都是这种格式.\n\n## HDF和NetCDF栅格数据集特点\n\nHDF和NetCDF数据都可能包含数据子集（一个文件中包含多个子文件），我们需要找出需要的子集数据，然后就可以像普通的GeoTIFF影像那样进行读写和操作了.\n\n## GDAL读取实例\n\n下面的例子读取MODIS地标反射率（Surface Reflectance）数据中的第一波段，然后转为GeoTIFF进行存储.\n\n我们首先使用`gdal.Open()`函数读取HDF数据，然后使用`GetSubDatasets()`方法取出HDF数据中存储的子数据集信息，该方法返回的结果是一个`list`，`list`的每个元素是一个`tuple`，每个`tuple`中包含了对子数据集的表述信息.\n\n对于MODIS数据，`tuple`的第一个元素是子数据集的完整路径，所以我们取出该路径，然后使用`gdal.Open()`函数读取该子数据集.\n\n最后我们使用`CreateCopy()`方法将该子数据集存储为GeoTIFF格式的数据。\n\n所以，总结一下，我们读取HDF或者NetCDF数据子集的时候，最主要的是取出想要处理的子数据集的完整路径。然后就像读取普通GeoTIFF影像那样对子数据集进行读取就OK了.\n\n```python\nfrom osgeo import gdal\n\nroot_ds = gdal.Open('example.hdf')\n# 返回结果是一个list，list中的每个元素是一个tuple，每个tuple中包含了对数据集的路径，元数据等的描述信息\n# tuple中的第一个元素描述的是数据子集的全路径\nds_list = root_ds.GetSubDatasets()\n\nband_1 = gdal.Open(ds_list[11][0])  # 取出第12个数据子集（MODIS反射率产品的第一个波段）\narr_bnd1 = band_1.ReadAsArray()  # 将数据集中的数据转为ndarray\n\n# 创建输出数据集，转为GeoTIFF进行写入\nout_file = 'sr_band1.tif'\ndriver = gdal.GetDriverByName('GTiff')\nout_ds = driver.CreateCopy(out_file, band_1)\nout_ds.GetRasterBand(1).WriteArray(arr_bnd1)\nout_ds.FlushCache()\n\n# 关闭数据集\nout_ds = None\nroot_ds = None\n```","tags":["空间数据处理","Python","HDF","NetCDF"],"categories":["空间数据处理"]},{"title":"栅格数据创建与保存","url":"/geos/栅格数据创建与保存/","content":"\n# 栅格数据创建与保存\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-24\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 思路与方法\n\n使用Python进行栅格数据处理，很多时候，我们会将GDAL的`Dataset`对象转化为NumPy的`ndarray`对象，这样我们可以使用很多通用的Python库对数据进行处理，然后再借助GDAL库将数据写回到文件。\n\n不同于普通的二进制文件，空间栅格数据的写需要注意两点：\n\n1. 数据的投影信息（确定了平面坐标系）\n2. 数据的地理坐标信息（确定了图像在给定坐标系下的位置）\n\n在GDAL中，我们首先需要创建`Dataset`对象，然后给`Dataset`对象填充数据以及元数据信息就OK了。\n\n`Driver`或者说`GDALDriver`（Python版本的API中对象名称好像都去掉了前缀GDAL，而C/C++版本的API很多对象前面都是有GDAL前缀的，如GDALDataset对象在Python中对应的是Dataset对象）有两个方法：`Create()`和`CreateCopy()`\n\n所以，相应地，我们也有两种思路去创建一个`Dataset`对象：\n\n1. 如果我们有一个原型数据，比如我们对原始数据进行了处理，处理之后，空间信息，波段等都没有变化，则可以将原始数据作为原型数据，使用`CreateCopy()`方法创建一个和原始数据一样的`Dataset`对象，然后在创建好的对象中填充一个`ndarray`数据就好了。\n2. 如果我们没有一个原型数据，那么我们首先需要使用`Create()`方法创建一个空的`Dataset`对象，然后手动设置对象的波段，尺寸，空间信息等，然后再在对应的波段填空`ndarray`具体的数据。\n\n## 实现函数\n\n我把上面两种实现思路编码成一个函数，具体实现如下：\n\n```python\ndef array2raster(f_name, np_array, driver='GTiff',\n                 prototype=None,\n                 xsize=None, ysize=None,\n                 transform=None, projection=None,\n                 dtype=None, nodata=None):\n    \"\"\"\n    将ndarray数组写入到文件中\n    :param f_name: 文件路径\n    :param np_array: ndarray数组\n    :param driver: 文件格式驱动\n    :param prototype: 文件原型\n    :param xsize: 图像的列数\n    :param ysize: 图像的行数\n    :param transform: GDAL中的空间转换六参数\n    :param projection: 数据的投影信息\n    :param dtype: 数据存储的类型\n    :param nodata: NoData元数据\n    \"\"\"\n    # 创建要写入的数据集（这里假设只有一个波段）\n    # 分两种情况：一种给定了数据原型，一种没有给定，需要手动指定Transform和Projection\n    driver = gdal.GetDriverByName(driver)\n    if prototype:\n        dataset = driver.CreateCopy(f_name, prototype)\n    else:\n        if dtype is None:\n            dtype = gdal.GDT_Float32\n        if xsize is None:\n            xsize = np_array.shape[-1]  # 数组的列数\n        if ysize is None:\n            ysize = np_array.shape[-2]  # 数组的行数\n        dataset = driver.Create(f_name, xsize, ysize, 1, dtype)  # 这里的1指的是一个波段\n        dataset.SetGeoTransform(transform)\n        dataset.SetProjection(projection)\n    # 将array写入文件\n    dataset.GetRasterBand(1).WriteArray(np_array)\n    if nodata is not None:\n        dataset.GetRasterBand(1).SetNoDataValue(nodata)\n    dataset.FlushCache()\n    return f_name\n```\n\n在使用该函数的时候，要么传进去一个`prototype`原型数据集，要么传进去`transform`和`projection`等信息，这样写入的文件才具有空间参考。\n\n## 测试案例\n\n下面是一个计算NDVI（Normalized Difference Vegetation Index，归一化植被指数）和DVI（Difference Vegetation Index，差值植被指数）的例子。我们首先计算NDVI，然后通过从原始数据中读取的空间投影和空间变换六元组信息创建输出文件；然后再计算DVI，通过NDVI文件作为原型数据集，以创建DVI的输出数据集。\n\n具体实现如下：\n\n```python\n# 打开栅格数据集\nds = gdal.Open('example.tif') # example.tif有三个波段，分别是蓝，红，近红外\n\n# 获取数据集的一些信息\nx_size = ds.RasterXSize  # 图像列数\ny_size = ds.RasterYSize  # 图像行数\n\nproj = ds.GetProjection()  # 返回的是WKT格式的字符串\ntrans = ds.GetGeoTransform()  # 返回的是六个参数的tuple\n\n# 在数据集层面ReadAsArray方法将每个波段都转换为了一个二维数组\nimage = ds.ReadAsArray()\n\n# 获得波段对应的array\nbnd_red = image[1].astype(float)  # 红波段\nbnd_nir = image[2].astype(float)  # 近红外波段\n\nidx_ndvi = (bnd_nir - bnd_red) / (bnd_nir + bnd_red)  # 计算NDVI指数\n\nout1_file = 'NDVI.tif'\narray2raster(out1_file, idx_ndvi,\n             xsize=x_size, ysize=y_size,\n             transform=trans, projection=proj,\n             dtype=gdal.GDT_Float32)\n\nidx_dvi = bnd_nir - bnd_red  # 计算DVI指数\n\nout2_file = 'DVI.tif'\n# 这里我们使用out1_file作为原型图像作为参考来保存out2_file\narray2raster(out2_file, idx_ndvi, prototype=gdal.Open(out1_file))\n\n# 关闭数据集\nds = None\n```\n\n","tags":["空间数据处理","Python","栅格数据"],"categories":["空间数据处理"]},{"title":"栅格数据格式转换","url":"/geos/栅格数据格式转换/","content":"\n# 栅格格式转换\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-17\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 查看GDAL支持的栅格数据格式\n\n我们可以在终端中使用`gdal --formats`命令查看安装的GDAL库支持的栅格数据格式\n\n```\nSupported Formats:\n  VRT -raster- (rw+v): Virtual Raster\n  DERIVED -raster- (ro): Derived datasets using VRT pixel functions\n  GTiff -raster- (rw+vs): GeoTIFF\n  NITF -raster- (rw+vs): National Imagery Transmission Format\n  RPFTOC -raster- (rovs): Raster Product Format TOC format\n  ECRGTOC -raster- (rovs): ECRG TOC format\n  HFA -raster- (rw+v): Erdas Imagine Images (.img)\n  SAR_CEOS -raster- (rov): CEOS SAR Image\n  CEOS -raster- (rov): CEOS Image\n  JAXAPALSAR -raster- (rov): JAXA PALSAR Product Reader (Level 1.1/1.5)\n  GFF -raster- (rov): Ground-based SAR Applications Testbed File Format (.gff)\n  ELAS -raster- (rw+v): ELAS\n  AIG -raster- (rov): Arc/Info Binary Grid\n  AAIGrid -raster- (rwv): Arc/Info ASCII Grid\n  GRASSASCIIGrid -raster- (rov): GRASS ASCII Grid\n  SDTS -raster- (rov): SDTS Raster\n  DTED -raster- (rwv): DTED Elevation Raster\n  PNG -raster- (rwv): Portable Network Graphics\n  JPEG -raster- (rwv): JPEG JFIF\n  MEM -raster- (rw+): In Memory Raster\n  JDEM -raster- (rov): Japanese DEM (.mem)\n  GIF -raster- (rwv): Graphics Interchange Format (.gif)\n  BIGGIF -raster- (rov): Graphics Interchange Format (.gif)\n  ESAT -raster- (rov): Envisat Image Format\n  BSB -raster- (rov): Maptech BSB Nautical Charts\n  XPM -raster- (rwv): X11 PixMap Format\n  BMP -raster- (rw+v): MS Windows Device Independent Bitmap\n  DIMAP -raster- (rov): SPOT DIMAP\n  AirSAR -raster- (rov): AirSAR Polarimetric Image\n  RS2 -raster- (ros): RadarSat 2 XML Product\n  SAFE -raster- (rov): Sentinel-1 SAR SAFE Product\n  PCIDSK -raster,vector- (rw+v): PCIDSK Database File\n  PCRaster -raster- (rw+): PCRaster Raster File\n  ILWIS -raster- (rw+v): ILWIS Raster Map\n  SGI -raster- (rw+): SGI Image File Format 1.0\n  SRTMHGT -raster- (rwv): SRTMHGT File Format\n  Leveller -raster- (rw+): Leveller heightfield\n  Terragen -raster- (rw+): Terragen heightfield\n  GMT -raster- (rw): GMT NetCDF Grid Format\n  netCDF -raster,vector- (rw+s): Network Common Data Format\n  HDF4 -raster- (ros): Hierarchical Data Format Release 4\n  HDF4Image -raster- (rw+): HDF4 Dataset\n  ISIS3 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 3)\n  ISIS2 -raster- (rw+v): USGS Astrogeology ISIS cube (Version 2)\n  PDS -raster- (rov): NASA Planetary Data System\n  VICAR -raster- (rov): MIPL VICAR file\n  TIL -raster- (rov): EarthWatch .TIL\n  ERS -raster- (rw+v): ERMapper .ers Labelled\n  JP2OpenJPEG -raster,vector- (rwv): JPEG-2000 driver based on OpenJPEG library\n  L1B -raster- (rovs): NOAA Polar Orbiter Level 1b Data Set\n  FIT -raster- (rwv): FIT Image\n  GRIB -raster- (rov): GRIdded Binary (.grb)\n  RMF -raster- (rw+v): Raster Matrix Format\n  WCS -raster- (rovs): OGC Web Coverage Service\n  WMS -raster- (rwvs): OGC Web Map Service\n  MSGN -raster- (ro): EUMETSAT Archive native (.nat)\n  RST -raster- (rw+v): Idrisi Raster A.1\n  INGR -raster- (rw+v): Intergraph Raster\n  GSAG -raster- (rwv): Golden Software ASCII Grid (.grd)\n  GSBG -raster- (rw+v): Golden Software Binary Grid (.grd)\n  GS7BG -raster- (rw+v): Golden Software 7 Binary Grid (.grd)\n  COSAR -raster- (rov): COSAR Annotated Binary Matrix (TerraSAR-X)\n  TSX -raster- (rov): TerraSAR-X Product\n  COASP -raster- (ro): DRDC COASP SAR Processor Raster\n  R -raster- (rwv): R Object Data Store\n  MAP -raster- (rov): OziExplorer .MAP\n  KMLSUPEROVERLAY -raster- (rwv): Kml Super Overlay\n  PDF -raster,vector- (rw+vs): Geospatial PDF\n  Rasterlite -raster- (rws): Rasterlite\n  MBTiles -raster- (rw+v): MBTiles\n  PLMOSAIC -raster- (ro): Planet Labs Mosaics API\n  CALS -raster- (rw): CALS (Type 1)\n  WMTS -raster- (rwv): OGC Web Mab Tile Service\n  SENTINEL2 -raster- (rovs): Sentinel 2\n  MRF -raster- (rw+v): Meta Raster Format\n  PNM -raster- (rw+v): Portable Pixmap Format (netpbm)\n  DOQ1 -raster- (rov): USGS DOQ (Old Style)\n  DOQ2 -raster- (rov): USGS DOQ (New Style)\n  GenBin -raster- (rov): Generic Binary (.hdr Labelled)\n  PAux -raster- (rw+): PCI .aux Labelled\n  MFF -raster- (rw+v): Vexcel MFF Raster\n  MFF2 -raster- (rw+): Vexcel MFF2 (HKV) Raster\n  FujiBAS -raster- (ro): Fuji BAS Scanner Image\n  GSC -raster- (rov): GSC Geogrid\n  FAST -raster- (rov): EOSAT FAST Format\n  BT -raster- (rw+v): VTP .bt (Binary Terrain) 1.3 Format\n  LAN -raster- (rw+v): Erdas .LAN/.GIS\n  CPG -raster- (ro): Convair PolGASP\n  IDA -raster- (rw+v): Image Data and Analysis\n  NDF -raster- (rov): NLAPS Data Format\n  EIR -raster- (rov): Erdas Imagine Raw\n  DIPEx -raster- (rov): DIPEx\n  LCP -raster- (rwv): FARSITE v.4 Landscape File (.lcp)\n  GTX -raster- (rw+v): NOAA Vertical Datum .GTX\n  LOSLAS -raster- (rov): NADCON .los/.las Datum Grid Shift\n  NTv2 -raster- (rw+vs): NTv2 Datum Grid Shift\n  CTable2 -raster- (rw+v): CTable2 Datum Grid Shift\n  ACE2 -raster- (rov): ACE2\n  SNODAS -raster- (rov): Snow Data Assimilation System\n  KRO -raster- (rw+v): KOLOR Raw\n  ROI_PAC -raster- (rw+v): ROI_PAC raster\n  RRASTER -raster- (rov): R Raster\n  ENVI -raster- (rw+v): ENVI .hdr Labelled\n  EHdr -raster- (rw+v): ESRI .hdr Labelled\n  ISCE -raster- (rw+v): ISCE raster\n  ARG -raster- (rwv): Azavea Raster Grid format\n  RIK -raster- (rov): Swedish Grid RIK (.rik)\n  USGSDEM -raster- (rwv): USGS Optional ASCII DEM (and CDED)\n  GXF -raster- (ro): GeoSoft Grid Exchange Format\n  DODS -raster- (ro): DAP 3.x servers\n  KEA -raster- (rw+): KEA Image Format (.kea)\n  BAG -raster- (ro): Bathymetry Attributed Grid\n  HDF5 -raster- (ros): Hierarchical Data Format Release 5\n  HDF5Image -raster- (ro): HDF5 Dataset\n  NWT_GRD -raster- (rw+v): Northwood Numeric Grid Format .grd/.tab\n  NWT_GRC -raster- (rov): Northwood Classified Grid Format .grc/.tab\n  ADRG -raster- (rw+vs): ARC Digitized Raster Graphics\n  SRP -raster- (rovs): Standard Raster Product (ASRP/USRP)\n  BLX -raster- (rwv): Magellan topo (.blx)\n  PostGISRaster -raster- (rws): PostGIS Raster driver\n  SAGA -raster- (rw+v): SAGA GIS Binary Grid (.sdat)\n  XYZ -raster- (rwv): ASCII Gridded XYZ\n  HF2 -raster- (rwv): HF2/HFZ heightfield raster\n  OZI -raster- (rov): OziExplorer Image File\n  CTG -raster- (rov): USGS LULC Composite Theme Grid\n  E00GRID -raster- (rov): Arc/Info Export E00 GRID\n  ZMap -raster- (rwv): ZMap Plus Grid\n  NGSGEOID -raster- (rov): NOAA NGS Geoid Height Grids\n  IRIS -raster- (rov): IRIS data (.PPI, .CAPPi etc)\n  PRF -raster- (rov): Racurs PHOTOMOD PRF\n  GPKG -raster,vector- (rw+vs): GeoPackage\n  CAD -raster,vector- (rovs): AutoCAD Driver\n  PLSCENES -raster,vector- (ro): Planet Labs Scenes API\n  HTTP -raster,vector- (ro): HTTP Fetching Wrapper\n```\n\n\n\n## 使用命令行工具进行栅格格式转换\n\nGDAL库不但提供了C/C++编程语言的API接口，还提供了很多实用的命令行工具，帮助我们完成一些日常的数据处理工作。\n\n我们以将GeoTIFF格式转为IMAGE格式为例，说明如何使用命令行工具进行栅格格式转换：\n\n`gdal_translate -of HFA example.tif example.img`\n\n其中，`of`选项指示了输出数据格式，`HFA`代表的是Erdas Imagine Images ，`example.tif`是输入数据路径，`example.img`是输出数据路径\n\n详细参数参考：[gdal_translate](http://www.gdal.org/gdal_translate.html)\n\n## 使用Python代码进行栅格格式转换\n\n下面介绍两种进行转换的方式：\n\n1. GDAL 2.1版本之后提供了，从代码中调用命令行的API接口，所以我们可以在Python代码中直接调用`Translate()`函数进行转换\n\n   ```python\n   from osgeo import gdal\n   \n   ds = gdal.Open('example.tif')\n   ds = gdal.Translate('example.img', ds, format='HFA')\n   ds = None\n   ```\n\n   `Translate()`函数的第一个参数是输出数据路径，第二参数是输入数据路径或者输入数据的Dataset对象，后面都是可选参数，具体`Translate()`函数的参数可以参见：[GDAL/OGR Python API](http://gdal.org/python/)\n\n2. 使用`CreateCopy()`方法进行数据的复制及格式转换\n\n   ```python\n   from osgeo import gdal\n   \n   # 打开数据文件\n   src_ds = gdal.Open('example.tif')\n   \n   # 获得数据驱动对象\n   driver = gdal.GetDriverByName('HFA')\n   \n   # 进行数据格式转换\n   dst_ds = driver.CreateCopy('example.img', src_ds)\n   \n   # 关闭数据集对象\n   dst_ds = None\n   src_ds = None\n   ```\n\n   ","tags":["空间数据处理","Python","栅格数据","格式转换"],"categories":["空间数据处理"]},{"title":"打开栅格数据的正确方式","url":"/geos/打开栅格数据的正确方式/","content":"\n# 打开栅格数据的正确方式\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-16\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 以一个简单例子说明如何打开栅格影像\n\n下面的例子打开一副GeoTIFF影像，输出了影像的一些信息，然后遍历了所有波段，输出波段的一些信息\n\n```python\nimport gdal\n\n# 打开栅格数据集\nds = gdal.Open('example.tif')\n\n# 获得栅格数据的一些重要信息\nprint(f'投影信息：{ds.GetProjection()}')\nprint(f'栅格波段数：{ds.RasterCount}')\nprint(f'栅格列数（宽度）：{ds.RasterXSize}')\nprint(f'栅格行数（高度）：{ds.RasterYSize}')\n\n# 获取数据集的元数据信息\nmetadata = ds.GetMetadata_Dict()\nfor key, value in metadata.items():\n    print(f'{key} -> {value}')\n\n\nfor b in range(ds.RasterCount):\n    # 注意GDAL中的band计数是从1开始的\n    band = ds.GetRasterBand(b + 1)\n    # 波段数据的一些信息\n    print(f'数据类型：{gdal.GetDataTypeName(band.DataType)}')  # DataType属性返回的是数字\n    print(f'NoData值：{band.GetNoDataValue()}')  # 很多影像都是NoData，我们在做数据处理时要特别对待\n    print(f'统计值（最大值最小值）：{band.ComputeRasterMinMax()}')  # 有些数据本身就存储了统计信息，有些数据没有需要计算\n\n# 关闭数据集\nds = None\n```\n\n输出如下：\n\n```\n投影信息：PROJCS[\"WGS 84 / UTM zone 49N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",111],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32649\"]]\n栅格波段数：3\n栅格列数（宽度）：4800\n栅格行数（高度）：4800\nAREA_OR_POINT -> Area\n数据类型：Int16\nNoData值：-28672.0\n统计值（最大值最小值）：(-435.0, 6134.0)\n数据类型：Int16\nNoData值：-28672.0\n统计值（最大值最小值）：(-468.0, 6265.0)\n数据类型：Int16\nNoData值：-28672.0\n统计值（最大值最小值）：(21.0, 7267.0)\n```\n\n\n\n## 如何将Dataset转为Numpy的ndarray\n\n当我们得到`Band`对象以后，如果按照GDAL的C/C++接口惯例，我们可以使用`WriteRaster()`方法进行数据写入（C/C++接口是`WriteBlock()`），但是在Python中我们有很强大的`ndarray`对象，所以我们一般是将`Band`对象中存储的数据转为`ndarray`进行处理以后，然后再写回去。\n\n下面介绍几种转换的方法：\n\n1. 在`Dataset`级别进行转换，转换结果是一个三维数组，第一个维度是波段数\n2. 在`Band`级别进行转换，转换的结果是一个二维数据\n3. 使用`gdal_array`模块中的`LoadFile()`函数直接进行（相当于第一种转换）\n\n```python\nimport gdal\n\n# 打开栅格数据集\nds = gdal.Open('example.tif')\n# 在数据集层面转换\nimage = ds.ReadAsArray()\n\nprint(f'数据的尺寸：{image.shape}')\n# 输出结果为：数据的尺寸：(3, 4800, 4800)\n# 这说明ReadAsArray方法将每个波段都转换为了一个二维数组\n\n# 获得第一个波段的数据\nband1 = image[0]\n\n# 在波段层面的转换\nfor b in range(ds.RasterCount):\n    # 注意GDAL中的band计数是从1开始的\n    band = ds.GetRasterBand(b + 1)\n    band = band.ReadAsArray()\n    print(f'波段大小：{band.shape}')\n\n# 关闭数据集\nds = None\n```\n\n输出结果：\n\n```\n数据的尺寸：(3, 4800, 4800)\n波段大小：(4800, 4800)\n波段大小：(4800, 4800)\n波段大小：(4800, 4800)\n```\n\n使用`gdal_array`模块\n\n```python\nfrom osgeo import gdal_array\n# gdal_array模块\nimage = gdal_array.LoadFile('example.tif')\nprint(f'数据的尺寸：{image.shape}')\n```\n\n\n\n## 在GDAL中使用Python的异常对象\n\n```python\nimport gdal\nimport sys\n\n# 允许GDAL跑出Python异常\ngdal.UseExceptions()\n\ntry:\n    ds = gdal.Open('example.tif')\nexcept (FileNotFoundError, RuntimeError) as e:\n    print('文件打开失败！')\n    print(e)\n    sys.exit(1)\n```\n\n","tags":["空间数据处理","Python","栅格数据"],"categories":["空间数据处理"]},{"title":"GDAL简介","url":"/geos/GDAL简介/","content":"\n# GDAL简介\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-05-13\n\n声明：本文为博主原创文章，转载请注明原文出处\n\n------\n\nGeospatial Data Abstraction Library （[GDAL](http://www.gdal.org/)）是使用C/C++语言编写的用于读写空间数据的一套跨平台开源库。现有的大部分GIS或者遥感平台，不论是商业软件ArcGIS，ENVI还是开源软件GRASS，QGIS，都使用了GDAL作为底层构建库。\n\nGDAL库由OGR和GDAL项目合并而来，OGR主要用于空间要素矢量矢量数据的解析，GDAL主要用于空间栅格数据的读写。此外，空间参考及其投影转换使用开源库 [PROJ.4](https://proj4.org)进行。\n\n目前，GDAL主要提供了三大类数据的支持：栅格数据，矢量数据以及空间网络数据（Geographic Network Model）。\n\nGDAL提供了C/C++借口，并且通过[SWIG](http://www.swig.org/)提供了Python，Java，C#等的调用借口。当我们在Python中调用GDAL的API函数时，其实底层执行的是C/C++编译的二进制文件。\n\nGDAL不但提供了API借口方便开发人员自定义自己的功能，而且还提供了一系列实用工具（Command Line Tools）可以实现方便快速的空间数据处理。我们可以使用这些实用工具，结合Linux Shell脚本或者Windows批处理脚本进行大批量空间数据的批量处理。\n\nGDAL 1.x版本以前，对于栅格和矢量数据的读写API借口设计是相对分离的，从2.x版本开始，栅格和矢量数据的API进行了集成，对开发者更加友好。我们这里的示例都是以2.x版本为例。\n\n\n\n## 栅格数据组织\n\nGDAL中使用dataset表示一个栅格数据（使用抽象类[GDALDataset](http://www.gdal.org/classGDALDataset.html)表示），一个dataset包含了对于栅格数据的波段，空间参考以及元数据等信息。一张GeoTIFF遥感影像，一张DEM影像，或者一张土地利用图，在GDAL中都是一个GDALDataset。\n\n- 坐标系统（使用OGC WKT格式表示的空间坐标系统或者投影系统）\n- 地理放射变换（使用放射变换表示图上坐标和地理坐标的关系）\n- GCPs（大地控制点记录了图上点及其大地坐标的关系，通过多个大地控制点可以重建图上坐标和地理坐标的关系）\n- 元数据（键值对的集合，用于记录和影像相关的元数据信息）\n- 栅格波段（使用[GDALRasterBand](http://www.gdal.org/classGDALRasterBand.html)类表示，真正用于存储影像栅格值，一个栅格数据可以有多个波段）\n- 颜色表（Color Table用于图像显示）\n\n### 地理放射变换\n\n放射变换使用如下的公式表示栅格图上坐标和地理坐标的关系：\n$$\n\\begin{matrix}\nX_{geo} = GT(0) + X_{pixel} * GT(1) + Y_{line} * GT(2) \\\\\nY_{geo} = GT(3) + X_{pixel} * GT(4) + Y_{line} * GT(5)\n\\end{matrix}\n$$\n（$X_{ge0}$, $Y_{ge0}$）表示对应于图上坐标（$X_{pixel}$, $Y_{line}$）的实际地理坐标。对一个上北下南的图像，GT(2)和GT(4)等于0， GT(1)是像元的宽度, GT(5)是像元的高度。（GT(0),GT(3)）坐标对表示左上角像元的左上角坐标。\n\n 通过这个放射变换，我们可以得到图上所有像元对应的地理坐标。\n\n参考资料：[GDAL Data Model](http://www.gdal.org/gdal_datamodel.html)\n\n## 矢量数据组织\n\nGDAL的矢量数据模型是建立在[OGC Simple Features](http://www.opengeospatial.org/standards/sfa)规范的基础之上的，OGC Simple Features规范规定了常用的点线面几何体类型，及其作用在这些空间要素上的操作。\n\nOGR矢量数据模型中比较重要的几个概念：\n\n- Geometry（[OGRGeometry](http://www.gdal.org/classOGRGeometry.html)类表示了一个空间几何体，包含几何体定义，空间参考，以及作用在几何体之上的空间操作，几何体和OGC WKB，WKT格式直接的导入导出）\n- Spatial Reference（[OGRSpatialReference](http://www.gdal.org/classOGRSpatialReference.html)类表示了空间参考信息，各种格式的空间参考的导入导出）\n- Feature（OGRFeature类表示空间要素，一个空间要素是一个空间几何体及其属性的集合）\n- Layer（OGRLayer表示一个图层，一个图层中可以包含很多个空间要素）\n- Dataset（GDALDataset抽象类表示一个矢量数据，一个Dataset可以包含多个图层）\n\n总结一下：一个数据集（Dataset）可以包含多个图层（Layer），一个图层中可以包含多个空间要素（Feature），一个Feature由一个空间几何体（Geometry）及其属性构成\n\n参考资料：[OGR Architecture](http://www.gdal.org/ogr_arch.html)","tags":["空间数据处理","Python","GDAL"],"categories":["空间数据处理"]},{"title":"空间数据","url":"/geos/空间数据/","content":"\n# 空间数据\n\n作者：阿振 \n\n邮箱：tanzhenyugis@163.com \n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375> \n\n修改时间：2018-05-06 \n\n声明： 本文为博主原创文章，转载请注明原文出处\n\n------\n\n## 矢量数据（Vector）\n\n矢量数据基于对象模型（object-based）的空间数据描述模型。矢量数据使用对象（点，线，面）及其对象之间的关系描述空间实体。\n\n常见的矢量数据格式：\n\n- ESRI Shapefile （Shapefile是一种基于文件方式存储GIS数据的被广泛使用的数据格式。一个Shapefile文件至少由shp，dbf，shx三个文件作成，分别存储空间对象的几何信息，属性信息和两者索引信息。\n- ESRI Personal Geodatabase（基于微软的Access数据库进行空间数据存储的数据格式，即可以存储矢量数据又可以存储栅格数据）\n- ESRI File Geodatabase（使用Geodatabase数据模型基于文件进行存储，跨平台）\n- OGC GML（Geography Markup Language，基于XML文件格式，国际标准）\n- KML （Keyhole Markup Language，基于XML文件格式，Google Earth专用空间数据格式）\n- GeoJSON（基于JSON数据格式用于表示空间实体的标记语言）\n\n## 栅格数据（Raster）\n\n栅格数据基于场模型（field-based）把空间事物和现象作为连续的变量或体来看待。\n\n常见的栅格数据格式：\n\n- GeoTIFF （Geographic Tagged Image File Format，GIS和卫星遥感应用的行业图像标准文件（.tif或者.tiff））\n- ERDAS Imagine （ERDAS软件的专用数据格式（.img））\n- ENVI格式（ENVI使用一个二进制文件（.dat或者.bin），一个文本头文件存储栅格数据（.hdr））\n- HDF （Hierarchical Data Format，美国国家高级计算机应用中心研发，MODIS数据就采用该格式，当前版本HDF5）\n- NetCDF（Network Common Data Form，由美国大学大气研究协会研发，广泛应用于大气环境领域（.nc）NetCDF4基于HDF5）\n\n## 空间数据库\n\n- Oracle Spatial and Graph （GeoSpatial，GeoRaster）\n- PostGIS（支持栅格矢量数据）\n- OGC GeoPackage （基于SQLLite数据的数据存储规范）","tags":["空间数据处理","Python","空间数据"],"categories":["空间数据处理"]},{"title":"地图投影","url":"/geos/地图投影/","content":"\n# 地图投影\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-04-29\n\n声明：\n\n- 本文为博主原创文章，转载请注明原文出处\n\n- 图片来源于网络，如有侵权请私信删除\n\n------\n\n## 什么是地图投影\n\n我们的地球是圆的，而我们的纸张是平面。为了将地球绘制在平面纸张上，我们需要将地球表面投影到平面上。地图投影的实质是建立空间地理坐标和平面直角坐标关系的过程。\n\n进过投影，我们的地球在平面上可能是这个样子，也可能是那个样子，但是地图投影要尽可能减少实际地物在平面上的变形，尽可能真实地表现地物的位置及地物之间的相对位置。\n\n![地球投影](/images/geopy/地球投影.jpg)\n\n## 地图投影分类\n\n### 根据投影面和地球球面的位置关系\n\n1. 投影面和地轴的关系\n\n- 正轴投影（投影面的中心线与地轴一直）\n- 斜轴投影（投影面的中心线与地轴斜交）\n- 横轴投影（投影面的中心线与地轴垂直）\n\n1. 投影面和地球面的关系\n\n- 切投影 (投影面和地球球面相切)\n- 割投影 (投影面和地球球面相割)\n\n![地图投影分类](/images/geopy/地图投影分类.jpeg)\n\n### 根据正轴投影时经纬网的形状\n\n- 圆锥投影 （投影中纬线为同心圆圆弧，经线为圆的半经）\n\n- 圆柱投影 （投影中纬线为一组平行直线，经线为垂直于纬线的另一组平行直线，且两相邻经线之间的距离相等）\n\n- 方位投影 （投影中纬线为同心圆，经线为圆的半径，且经线间的夹角等于地球面上相应的经差）\n\n  此外，还有伪圆锥投影，伪圆柱投影，伪方位投影，多圆锥投影等\n\n  ![正轴地图投影分类](/images/geopy/正轴地图投影分类.jpg)\n\n### 根据投影的变形\n\n- 等角投影 （地球表面无穷小图形投影后保持相似）\n- 等面积投影 （地球表面图形在投影后面积保持不变）\n- 任意投影\n\n## 常用地图投影\n\n1. 我国基本比例尺地形图（1:100万，1:50万，1:25万，1:10万，1:5万，1:2.5万，1:1万，1:5000）除1:100万以外均采用高斯-克吕格Gauss-Kruger投影（横轴等角切圆柱投影）为地理基础。\n\n   ![高斯克吕格投影](/images/geopy/高斯克吕格投影.jpg)\n\n   通常其按经差6°或3°分为六度带或三度带。根据带号及其带内的平面直角坐标，即可确定在地球上的位置。\n\n   ![高斯克吕格投影分带](/images/geopy/高斯克吕格投影分带.gif)\n\n2. 1:100万地形图采用兰伯特Lambert投影（正轴等角割圆锥投影），其分幅原则与国际地理学会规定的全球统一使用的国际百万分之一地图投影保持一致。\n\n3. 我国大部分省区图以及大多数这一比例尺的地图也多采用Lambert投影和同属于这一投影系统的Albers投影（正轴等面积割圆锥投影）。\n\n4. UTM投影（Universal Transverse Mercator Projection）全称为“通用横轴墨卡托投影”，是横轴等角割圆柱投影（高斯-克吕格为横轴等角切圆柱投影）。UTM投影与高斯投影的主要区别在南北格网线的比例系数上。Landsat卫星影像使用该投影。\n\n5. Google地图和百度地图使用的墨卡托投影（正轴等角圆柱投影），但是这种网络地图使用的墨卡托投影和常规的墨卡托投影稍微有一些区别：在网络地图中将地球抽象为球体而不是椭球体，这样的墨卡托投影称为Web Mercator投影。此外，墨卡托投影广泛用于航海图。","tags":["空间数据处理","Python","地图投影"],"categories":["空间数据处理"]},{"title":"空间参考系统","url":"/geos/空间参考系统/","content":"\n# 空间参考系统\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-04-28\n\n声明：\n\n- 本文为博主原创文章，转载请注明原文出处\n\n- 图片来源于网络，如有侵权请私信删除\n\n------\n\n## 空间参考系统（分类）\n\n在三维的地球上，我们为了描述一个物体的空间位置，定义了：\n\n- 坐标参考系统 (X, Y)\n- 高程参考系统 (Z)\n\n不考虑高程，对于空间位置的描述我们有：\n\n- 地理坐标系 （使用经纬度坐标表示）\n- 投影坐标系  （使用平面直角坐标表示）\n\n## 地球椭球体和大地基准面（如何建立空间参考系统）\n\n地球像一个倒放着的大鸭梨，两极略扁，中间略大的不规则球体。\n\n![地球](/images/geopy/真实地球.jpg)\n\n假想将静止的平均海水面，延伸到大陆内部，形成一个连续不断的，与地球比较接近的形状，其表面称之为大地水准面，由它包围的球体叫“大地体”。\n\n大地水准面的特性：其表面处处和铅垂线正交\n\n为了测量成果的计算和制图工作的需要，通常用地球椭球体来代替大地体。我们使用一个形状同地球相近，并能用数学方法来表达的旋转椭球体描述我们的地球。\n\n![地球椭球体](/images/geopy/地球椭球体.png)\n\n![地球表面和地球椭球体](/images/geopy/地球与参考椭球.png)\n\n建立了地球椭球体，即确定了地球的形状和大小。为了观测和制图的准确和方便，我们还需要确定椭球体与大地水准面的相对位置。\n\n![椭球体定位](/images/geopy/椭球体定位.png)\n\n- 地心坐标系 （如果参考椭球体和大地体球心重合，例如WGS84，2000国家大地坐标系）\n- 参心坐标系 （如果参考椭球体和大地体球心不重合，例如北京54坐标系，西安80坐标系）\n\n总结一下：\n\n确定一个空间参考系统需要：\n\n- 确定地球椭球体参数\n- 确定高程系统 （一个国家一般只有一个高程系统）\n- 确定参考椭球和大地水准面的位置关系\n\n## 我国常用的空间参考\n\n我国家常用的空间参考系统：\n\n- 1954年北京坐标系（北京54）：我国过去采用的大地坐标系，其原点在苏联西部的普尔科夫，采用克拉索夫斯基椭球模型\n- 1980年国家坐标系（西安80）：1980年新测定位于陕西省的坐标原点，采用1975年国家椭球模型\n\n高程系统\n\n我国曾使用过黄海平均海水面，称“1956黄海高程系”。 现采用“1985年国家高程基准”，该系统是采用青岛验潮站1952年—1979年潮汐观测资料计算的平均海水面作为高程基准面。","tags":["空间数据处理","Python","空间参考"],"categories":["空间数据处理"]},{"title":"空间数据处理环境搭建","url":"/geos/空间数据处理环境搭建/","content":"\n# Python空间数据处理环境搭建\n\n作者：阿振\n\n邮箱：tanzhenyugis@163.com\n\n博客：<https://blog.csdn.net/theonegis/article/details/80089375>\n\n修改时间：2018-04-26\n\n------\n\n## Conda的下载和安装\n\n什么是Conda? 官方定义：Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN\n\nConda就是一个虚拟环境和包（库）依赖管理工具\n\n下载地址：[Downloading conda](https://conda.io/docs/user-guide/install/download.html)\n\n对于Windows版本的，确定Python版本和系统类型直接下载安装包进行安装\n\n对于Linux和macOS系统，在Terminal中运行bash脚本进行安装即可。\n\n## Conda的使用\n\n### 新建虚拟环境（[Managing environments](https://conda.io/docs/user-guide/tasks/manage-environments.html)）\n\n`conda create -n osgeo`\n\n### 切换到新建的虚拟环境\n\n`source activate osgeo`  （Linux和macOS）\n\n`activate osgeo`  （Windows）\n\n### 退出虚拟环境\n\n`source deactivate`  （Linux和macOS）\n\n`deactivate`  （Windows）\n\n实用命令：\n\n- 查看虚拟环境列表  `conda env list` 或者 `conda info --envs`\n- 删除虚拟环境  `conda remove --name <environment> --all`\n- 查看虚拟环境中的包列表  `conda list`\n- 更新conda或者某个包  `conda update conda` 或者 `conda update <package>`\n- 更新虚拟环境下的所有包  `conda update --all`\n- 查看过时的包  `conda search --outdated`\n- 搜索指定包  `conda search <package>`\n- 删除某个包 `conda remove <package>`\n- 添加channel到conda配置文件  `conda config --add channels <channel>` 或者 `conda config --append channels <channel>`\n\n## 空间数据处理Python库的安装\n\n### 常用的空间数据处理Python库\n\n- [GDAL](http://www.gdal.org) 全能型的基础空间数据处理库\n- [fiona](https://github.com/Toblerity/Fiona) 基于GDAL的空间矢量数据处理库\n- [rasterio](https://github.com/mapbox/rasterio) 基于GDAL的空间栅格处理库\n- [basemap](https://matplotlib.org/basemap/) 基于matplotlib的空间制图库\n- [GeoPandas](http://geopandas.org) 基于pandas的空间数据分析库\n- [RSGISlib](https://www.rsgislib.org) 针对遥感数据及GIS分析的高级库\n\n### 使用conda进行库的安装\n\n打开命令行工具（Terminal），输入命令，进入虚拟环境\n\n1. 安装GDAL库\n\n   `conda install -c conda-forge gdal`\n\n1. 安装fiona库\n\n   `conda install -c conda-forge fiona`\n\n2. 安装rasterio库\n\n   `conda install -c conda-forge rasterio `\n\n   ​\n\n### 使用pip进行库的安装\n\n什么是pip呢？pip是Python默认和推荐实用的包管理工具，可以利用pip从[PyPI](https://pypi.org)网络仓库自动下载Python包进行安装和管理。\n\n对于Windows下的二进制库的预编译包，提供给大家一个网站：[Unofficial Windows Binaries for Python Extension Packages](https://www.lfd.uci.edu/~gohlke/pythonlibs/)\n\n使用pip安装的时候，如果该Python包底层依赖一些C++库，则需要手动进行编译，或者安装指定平台下预编译好的库。\n\n1. 安装GDAL库\n\n   `pip install GDAL‑2.2.4‑cp37‑cp37m‑win_amd64.whl`\n\n2. 安装fiona库\n\n   `pip install Fiona‑1.7.11.post1‑cp37‑cp37m‑win_amd64.whl`\n\n3. 安装rasterio库\n\n   `pip install rasterio‑1.0a12‑cp37‑cp37m‑win_amd64.whl`\n\n4. 安装Jupyter\n\n   `pip install jupyter`\n\n   使用Jupyter Notebook进行编程\n\n   `jupyter notebook`","tags":["空间数据处理","Python","Conda"],"categories":["空间数据处理"]},{"title":"Hexo+GitHub Pages创建个人博客","url":"/tools/Hexo-GitHub-Pages创建个人博客/","content":"\n# Hexo+GitHub Pages创建个人博客\n\n全球最大的同性交友平台[GitHub](https://github.com/)为我们提供了免费的[GitHub Pages](https://pages.github.com/)服务，个人或者组织可以用于托管静态的网页资源。使用GitHub Pages可以创建产品介绍类的网站，或者个人博客等。\n\n用于创建或者生成静态网页资源的常见工具，或者叫静态网站生成器，是一种将文本文档通过一些处理生成一个有机整体的（HTML+CSS+JS文件组成）静态网站（不和后台服务器有交互）。常用的静态网站生成器有：[Jekyll](https://jekyllrb.com/)，[Hugo](https://gohugo.io/)，[Hexo](https://hexo.io/)等。Jekyll使用Ruby语言构建，是GitHub Pages默认支持的生成器；Hugo使用Go语言构建，Hexo使用NodeJS技术构建。因为我对Ruby和Go语言都没有接触过，所以自然而然地选择了Hexo。\n\n使用Hexo和GitHub Pages创建自己的免费个人博客主要包括以下步骤：\n\n1. 在GitHub上创建一个新的仓库（repository）\n2. 安装NodeJS和Hexo工具环境\n3. 选择自己喜欢的Hexo主题并做简单配置\n4. 将Hexo生成的静态网站资源发布到GitHub仓库。\n\n好的，注意力集中，下面我们一步一步介绍如何进行实战。\n\n## 创建GitHub仓库\n\n- 首先，你必须有一个GitHub账户。如果没有的话，注册一个，免费的。\n- GitHub Pages提供了两种类型的网站托管：用户或者组织网站（User or organization site）和项目网站（Project site）。前者只能创建一个，后者可以创建多个。这里我们创建用户或者组织网站。\n- 创建用户或者组织网站，我们直接登录自己的GitHub账户，创建一个新的私有仓库，仓库的名称必须是*username.github.io*这样的格式，username可以自定义，比如我的是theonegis.github.io。这样即可！\n\nGitHub Pages的[官网](https://pages.github.com/)提供了详细的步骤说明，有兴趣的童鞋可以看看。\n\n## 安装Hexo环境\n\n- 首先，需要确保你正确安装了NodeJS（可在命令行终端使用`node -v`进行验证），如果没有请移步到[NodeJS官网](https://nodejs.org/en/)下载对应的安装包进行安装。\n- 打开命令行终端，使用`npm install -g hexo-cli`命令进行Hexo的安装。使用`hexo -v`命令查看是否安装成功。\n\n## 配置Hexo\n\n- 在自己的工作目录下面新建一个文件夹用于存储自己的网站资源，比如我使用新建了一个名为blog的文件夹。\n\n- 使用Hexo的命令行工具执行初始化工作，具体命令如下：\n\n  ```Shell\n  hexo init blog\n  cd blog\n  npm install\n  ```\n\n- 下面我来说一些，初始化工作完成以后，生成的一些文件或者文件夹的用途：\n\n  ```\n  .\n  ├── _config.yml\n  ├── package.json\n  ├── scaffolds\n  ├── source\n  |   ├── _drafts\n  |   └── _posts\n  └── themes\n  ```\n\n  - **_config.yml** 文件用于对Hexo整体的配置（具体的配置说明参见：https://hexo.io/docs/configuration.html）\n  - **source** 文件夹用于存储自己的网站内容，**_posts**文件夹下面主要存储博客文本文件\n  - **themes** 文件夹用于存储下载的主题\n\n- 接下来下载自己喜欢的主题，当然也可以使用[默认主题](https://github.com/hexojs/hexo-theme-landscape)。[Hexo官方](https://hexo.io/themes/index.html)提供了很多可选的主题，众多网友推荐使用[NexT](https://github.com/theme-next/hexo-theme-next)主题，简洁大方，功能丰富。但是由于使用人比较多，我自己选择了一款比较小众的主题，这个主要看个人喜好吧！一般使用`git`工具将GitHub上开源的主题克隆到自己的themes文件夹下面，主题的配置文件在**themes/_config.yml**文件中进行，具体的配置需要参考特定主题的说明。要将自己下载并且配置好的主题应用到Hexo，则需要在根目录下的**_config.yml**中进行，修改**theme**的名称为你自己想要应用的主题名称。\n\n- 比较重要的是对于网站菜单的配置，这个是在**themes/_config.yml**文件中进行的，比如我的配置如下（每个主题可能稍微有一些不一样，注意查看主题的说明文档）：\n\n  ```\n  # Header\n  menu:\n    主页: /\n    归档: /archives\n    分类: /categories\n    标签: /tags\n    关于: /about\n  ```\n\n  为了生成分类和标签页面，我们需要执行如下步骤：\n\n  在命令行执行如下命令生成分类页面：\n\n  ```\n  # 新建一个页面，命名为 categories。编辑刚新建的页面，将页面的类型设置为categories，主题将自动为这个页面显示所有分类\n  hexo new page categories\n  ```\n\n  我的分类页内容如下：\n\n  ```\n  ---\n  title: \"分类\"\n  date: 2018-07-10 23:34:06\n  type: \"categories\"\n  ---\n  ```\n\n  同理，标签页也是类似的，命令如下：\n\n  ```\n  # 新建一个页面，命名为 tags。编辑刚新建的页面，将页面的类型设置为tags，主题将自动为这个页面显示标签云\n  hexo new page tags\n  ```\n\n  我的标签页内容如下：\n\n  ```\n  ---\n  title: \"标签\"\n  date: 2018-07-10 23:35:16\n  type: \"tags\"\n  ---\n  ```\n\n  关于`hexo new`命令的使用，参见：https://hexo.io/docs/writing.html\n\n## 撰写博文\n\n- 使用`hexo new post \"Hexo-GitHub-Pages创建个人博客\"`创建自己的第一篇博文，默认会在**source/_posts**文件夹下面生成一个同名的Markdown文件，然后我们打开该文件使用Markdown语法进行博文撰写即可。\n\n  这是我写完博文以后source文件夹的目录列表：\n\n  ```\n  source\n  ├── _posts\n  │   └── Hexo-GitHub-Pages创建个人博客.md\n  ├── about\n  │   └── index.md\n  ├── categories\n  │   └── index.md\n  └── tags\n      └── index.md\n  ```\n\n  注：_posts文件夹下面默认会有一个`Hello Word.md`的文件，是默认生成的示例性质的一篇博文，删除即可。\n\n- 写完以后，可以使用`hexo server`启动一个本机的服务器，使用http://localhost:4000/地址进行访问，预览一下自己的博客。\n\n## 发布\n\n- 要将博文发布出去，需要在根目录下的**_config.yml**文件中配置一下：\n\n  ```\n  # Deployment\n  ## Docs: https://hexo.io/docs/deployment.html\n  deploy:\n    type: git\n    repo: https://github.com/theonegis/theonegis.github.io.git  # 这里是你自己的github仓库地址\n    branch: master\n  ```\n\n- 然后使用如下命令进行发布：\n\n  ```\n  hexo clean  # 清理生成的静态文件(非必须)\n  hexo generate  # 重新生成静态网页文件，生成文件位于根目录下public文件夹内\n  hexo deploy  # 将本机的静态文件发布到GitHub上去\n  ```\n\n  Hexo的常用命令：https://hexo.io/docs/commands.html\n\n## 关于撰写博文时分类和标签的说明\n\n- 我们需要在博文中通过元数据的形式告诉Hexo如何对我们的博文进行分类和添加标签，比如我的这篇博文的开头由如下的原信息：\n\n    ```\n        ---\n        title: Hexo+GitHub Pages创建个人博客\n        date: 2018-07-11 11:36:56\n        tags: [Hexo, GitHub, 个人博客]\n        categories: \n        - 工具\n        - git\n        ---\n    ```\n    这里的tags用于给文章添加标签，多个标签需要用方括号括起来，并用逗号分隔。\n\n    categories指定文章分类，如果有多个分类可以以列表的形式指定。\n\n- 第二个问题是，Hexo默认的链接是以日期指定的，即使`:year/:month/:day/:title/`这种形式，如果我想以分类来表示博文的永久链接，可以修改为`:category/:title/`。但是，如果我的文章分类中有文中怎么办？\n\n    我们可以通过在根目录下的配置文件中设置一个映射关系解决，示例如下：\n\n    ```\n    # Category & Tag\n    default_category: uncategorized\n    category_map:\n      Python入门: python\n      空间数据处理: geos\n      机器学习: ml\n      深度学习: dl\n      算法: algorithm\n    tag_map:\n    ```\n\n","tags":["Hexo","GitHub","个人博客"],"categories":["工具"]},{"title":"关于我的故事","url":"/about/index.html","content":"# 个人简介\n\n- 姓名：谭振宇\n- 学历：工学博士\n- 职称：讲师\n- 邮箱：tanzhenyu@nwu.edu.cn\n- 研究方向：多源数据融合、湖泊水色遥感、空间大数据与人工智能、GIS系统设计与开发\n\n# 教育/工作经历\n\n- 2020-至今 西北大学城市与环境学院 遥感与地理信息系 讲师\n- 2017-2019 美国乔治梅森大学（George Mason University）国家公派联合培养博士研究生\n- 2013-2019 武汉大学 测绘遥感信息工程国家重点实验室 硕博连读 摄影测量与遥感\n- 2009-2013 长安大学 地球科学与资源学院 理学学士 地理信息系统\n\n# 主持/参与项目（近五年）\n\n1. 自然科学基金青年项目，高鲁棒遥感影像深度时空融合模型研究，2022-2024，主持；\n2. 自然资源部地理国情监测重点实验室开放基金，多源遥感卫星与无人机联合的中小型湖泊水质监测，2023-2024，主持；\n3. 陜西省教育厅专项基金，基于深度卷积网络的湖泊蓝藻水华信息智能检测方法，2021-2022，主持；\n4. 科技部第三次新疆综合科学考察项目，新疆水环境关键要素遥感动态监测，2021-2024，参与。\n\n# 发表论文（近五年，第一作者）\n\n1. Tan Z, Yang C, Qiu Y, et al. A three-step machine learning approach for algal bloom detection using stationary RGB camera images[J]. International Journal of Applied Earth Observation and Geoinformation, 2023, 122: 103421.\n2. Tan Z, Cao Z, Shen M, et al. Remote estimation of water clarity and suspended particulate matter in qinghai lake from 2001 to 2020 using MODIS images[J]. Remote Sensing, 2022, 14(13): 3094.\n3. Tan Z, Gao M, Yuan J, et al. A Robust Model for MODIS and Landsat Image Fusion Considering Input Noise[J]. IEEE Transactions on Geoscience and Remote Sensing, 2022, 60: 1-17.\n4. Tan Z, Gao M, Li X, et al. A flexible reference-insensitive spatiotemporal fusion model for remote sensing images using conditional generative adversarial network[J]. IEEE Transactions on Geoscience and Remote Sensing, 2021, 60: 1-13.\n5. Tan Z, Li X, Gao M, et al. The environmental story during the COVID-19 lockdown: how human activities affect PM2. 5 concentration in China?[J]. IEEE Geoscience and Remote Sensing Letters, 2020, 19: 1-5.\n6. Tan Z, Di L, Zhang M, et al. An enhanced deep convolutional model for spatiotemporal image fusion[J]. Remote Sensing, 2019, 11(24): 2898.\n\n# 承担课程\n\n- 本科生课程（空间大数据与人工智能、地理信息服务、三维GIS与可视化、移动GIS）\n- 研究生课程（GIS设计与开发、地理计算方法）\n\n# 学生指导（指导学生发表论文，通讯作者）\n\n1. Tan Z, Tan Z*, Luo J, et al. Mapping 30-m cotton areas based on an automatic sample selection and machine learning method using Landsat and MODIS images[J]. Geo-spatial Information Science, 2023: 1-18.\n2. Yang C, Tan Z*, Li Y, et al. A Comparative Analysis of Machine Learning Methods for Algal Bloom Detection Using Remote Sensing Images[J]. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023.\n3. 李一民,谭振宇*,杨辰等.基于多源卫星的滇池藻华提取机器学习算法研究[J].地球科学进展,2022,37(11):1141-1156.\n\n**现为硕士生导师一枚，专硕学硕均可招收，团队其他老师有招收博士生资格。热忱欢迎对遥感大数据处理、智能环境监测与应用、GIS设计与开发方面感兴趣的同学咨询报考！**\n\n**热心指导学生，从来不抢学生一作！想混学历的非诚勿扰，不要彼此耽误！**\n"},{"title":"分类","url":"/categories/index.html"},{"title":"标签","url":"/tags/index.html"},{"title":"搜索","url":"/search/index.html"},{"title":"时间轴","url":"/timeline/index.html"}]